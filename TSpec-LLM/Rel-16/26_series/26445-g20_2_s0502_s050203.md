5.2 LP-based Coding
-------------------

In general terms, speech dominated content is encoded using
Analysis-by-Synthesis Linear Prediction (LP) paradigm. At some low
bitrates configurations, the LP-based coding is used also for generic
audio. On the other hand, LP prediction is not used above 64 kb/s. The
LP-based coding consists in encoding the LP excitation signal and the
speech spectral envelope, represented by the LP filter coefficients.
Depending on the particular characteristics of a speech frames,
different flavours of the excitation coding are used to encode voiced or
unvoiced speech frames, audio frames, inactive frames etc.

The internal sampling rate of the LP-based coding is rather independent
of the input signal sampling rate. Instead, it depends on the encoded
bitrate to optimize coding efficiency. In the EVS, there are two
different internal sampling rates used -- 12.8 kHz is used up to 13.2
kb/s inclusively, and 16 kHz sampling rate is used for higher bitrates.
It means that up to 13.2 kb/s, the LP-based encoding covers first 6.4
kHz of the input signal while from 16.4 kb/s and up the LP-based
encoding covers 8 kHz of the input. For NB signals, the sampling rate is
always 12.8 kHz.

For other than NB signals, the upper bandwidth (not covered with the
LP-based coding) is then encoded using bandwidth extension (BWE)
technologies, ranging from blind BWE at the lowest bitrates, parametric
BWEs optimized to different content at higher bitrates, up to full
encoding of the upper bandwidth spectrum at the highest bitrate (64
kb/s).

The basic block for the LP excitation coding is a subframe. The size of
the subframe in samples is independent of the internal sampling rate. It
equals to 64 samples. It means that at 12.8 kHz internal sampling rate,
EVS uses 4 subframes of 5 ms while at 16 kHz internal sampling rate, EVS
uses 5 subframes of 4 ms.

### 5.2.1 Perceptual weighting

The encoding parameters, such as adaptive codebook delay and gain,
algebraic codebook index and gain are searched by minimizing the error
between the input signal and the synthesized signal in a perceptually
weighted domain. Perceptual weighting is performed by filtering the
signal through a perceptual weighting filter, derived from the LP filter
coefficients. The perceptually is similar to the weighting also used in
open-loop pitch analysis. However, an adaptive perceptual weighting is
used in case of LP-based excitation coding.

The traditional perceptual weighting filter
$W\left( z \right) = A\left( z/\gamma_{1} \right)/A\left( z/\gamma_{2} \right)$
has inherent limitations in modelling the formant structure and the
required spectral tilt concurrently. The spectral tilt is more
pronounced in wideband signals due to the wide dynamic range between low
and high frequencies. A solution to this problem is to introduce a
pre-emphasis filter at the input and enhance the high frequency content
in case of wideband signals. The LP filter coefficients are then found
by means of LP analysis on the pre-emphasized signal. Subsequently, they
are used to form a perceptual weighting filter. Its transfer function is
the same as the LP filter transfer function but with the denominator
having fixed coefficients (similar to the pre-emphasis filter). In this
way, the weighting in formant regions is decoupled from the spectral
tilt as shown below. Finally, the pre emphasized signal is filtered
through the perceptual filter to obtain a perceptually weighted signal,
which is used further.

The perceptual weighting filter has the following form

$W\left( z \right) = A\left( z/\gamma_{1} \right)H_{\text{de} - \text{emph}}\left( z \right) = A\left( z/\gamma_{1} \right)/\left( 1 - \beta_{1}z^{- 1} \right)$
(470)

where

$H_{\text{de} - \text{emph}}\ (z) = \frac{1}{(1 - \beta_{1}z^{- 1})}$
(471)

and $\beta_{1}$is equal to 0.68.

Because $A\left( z \right)$ is computed based on the pre-emphasized
signal $s_{\text{pre}}\left( n \right)$, the tilt of the filter
$1/A\left( z/\gamma_{1} \right)$is less pronounced compared to the case
when $A\left( z \right)$ is computed based on the original signal (as
the pre-emphasized signal itself exhibits less spectral tilt than the
original wideband signal). Since de-emphasis is performed in the
decoder, it can be shown that the quantization error spectrum is shaped
by a filter having a transfer function
$1/W\left( z \right)H_{\text{de} - \text{emph}}\left( z \right) = 1/A\left( z/\gamma_{1} \right)$.
Thus, the spectrum of the quantization error is shaped by a filter whose
transfer function is $1/A\left( z/\gamma_{1} \right)$, with
$A\left( z \right)$ computed based on the pre-emphasized signal. The
perceptual weighting is performed on a frame basis while the LP filter
coefficients are calculated on a subframe basis using the principle of
LSP interpolation, described in subclause 5.1.9.6. For a subframe of
size $L$ = 64, the weighted speech is given by

$s_{h}(n) = s_{\text{pre}}\left( n \right) + \sum_{i = 1}^{\text{16}}{a_{i}\ }\gamma_{1}^{i}\ s_{\text{pre}}\left( n - i \right) + \beta_{1}\ s_{h}\left( n - 1 \right),\ n = 0,\ldots,\ L - 1$
(472)

### 5.2.2 LP filter coding and interpolation

#### 5.2.2.1 LSF quantization

##### 5.2.2.1.1 LSF weighting function

For frame-end LSF quantization, the weighting given by equation (473) is
defined by combining the magnitude weighting, frequency weighting, IHM
and squared IHM.

As shown in figure 1, since the spectral analysis and LP analysis use
similar temporal sections, the FFT spectrum of the second analysis
window can be reused to find the best weighting function for the
frame-end LSF quantizer.

![](media/image1.wmf){width="4.365972222222222in"
height="3.1034722222222224in"}

Figure 19: LSF weighting computation with FFT spectrum

> Figure 2 is a block diagram of a spectral analysis module that
> determines a weighting function. The spectral analysis computation is
> performed by a pre-processing module and the output is a linear scale
> spectrum magnitude which is obtained by FFT.

![](media/image2.wmf){width="6.404166666666667in"
height="3.1944444444444446in"}

Figure 20: Block diagram of LSF weighting computation

In the Normalization block, the LSFs are normalized to a range of 0 to
$K$-1. The LSFs generally span the range of 0 to $\pi$. For a 12.8 kHz
internal sampling frequency, $K$ is 128 and for a 16 kHz internal
sampling frequency, $K$ is 160.

The Find magnitude weighting for each normalized LSF block determines
the magnitude weighting function $W_{1}(n)$ using the spectrum analysis
information and the normalized LSF.

The magnitude weighting function is determined using the magnitude of
the spectral bins corresponding to the frequency of the normalized LSFs
and the additional two magnitudes of the neighbouring spectral bins (+1
and -1 of the spectral bin corresponding to the frequency of the
normalized LSFs) around the spectral bin.

The spectral magnitude is obtained by a 128-point FFT and its bandwidth
corresponds to the range of 0 to 6400 Hz. If the internal sampling
frequency is16 kHz, the number of spectral magnitudes is extended to
160. Because the spectrum magnitude for the range of 6400 to 8000 Hz is
missing, the spectrum magnitude for this range will be generated by the
input spectrum. More specifically, the average value of the last 32
spectrum magnitudes which correspond to the bandwidth of 4800 to 6400 Hz
are repeated to fill in the missing spectrum.

The final magnitude function determines the weighting function of each
magnitude associated with a spectral envelope by extracting the maximum
magnitude among the three spectral bins.

$W_{1}(n) = \left( \sqrt{w_{f}(n) - \text{Min}} \right) + 2$,for
*n=0,...,M-1* (473)

where Min is the minimum value of $w_{f}(n)$ and

$w_{f}(n) = \text{10}\text{log}\left( E_{\text{max}}(n) \right)$, for
*n=0,...,M-1* (474)

where $M$=16 and the $E_{\text{max}}(n)$ is the maximum magnitude among
the three spectral bins for each LSF.

In the Find frequency weighting for each normalized LSF block, the
frequency weighting function $W_{2}(n)$ is determined by using frequency
information from the normalized LSF.

The function determines the weighting function of each frequency using
the predetermined weighting graph which is selected by using the input
bandwidth and coding mode. There are two predetermined weighting graphs,
as shown in figure 3, which are determined by perceptual characteristics
such as Bark scale and a formant distribution of the input signal.

The function corresponding to graph (a) in figure 4 is as follows.

$W_{2}(n) = \left\{ \begin{matrix}
0\text{.}5 + \frac{\text{sin}\left( \frac{\pi \cdot f_{n}(n)}{\text{12}} \right)}{2},\ \text{for}\ f_{n}(n) = \lbrack 0,5\rbrack,\  \\
\frac{1}{\left( \frac{\left( f_{n}(n) - 6 \right)}{\text{121}} + 1 \right)},\ \text{for}\ f_{n}(n) = \lbrack 6,\text{127}\rbrack\ \text{for}\ \text{WB}\ \text{and}\ \lbrack 6,\text{159}\rbrack\ \text{for}\ \text{WB}\text{16}\text{kHz}\text{.} \\
\end{matrix} \right.\ $ (475)

The function corresponding to graph (b) in figure 21 is as follows.

$W_{2}(n) = \left\{ \begin{matrix}
0\text{.}5 + \frac{\text{sin}\left( \frac{\pi \cdot f_{n}(n)}{\text{12}} \right)}{2},\ \text{for}\ f_{n}(n) = \lbrack 0,5\rbrack,\  \\
1\text{.}0,\ \text{for}\ f_{n}(n) = \lbrack 6,\text{20}\rbrack,\  \\
\frac{1}{\left( \frac{3\left( f_{n}(n) - \text{20} \right)}{\text{107}} + 1 \right)},\ \text{for}\ f_{n}(n) = \lbrack\text{21},\text{127}\rbrack\ \text{for}\ \text{WB}\ \text{and}\ \lbrack\text{21},\text{159}\rbrack\ \text{for}\ \text{WB}\text{16}\text{kHz}\text{.} \\
\end{matrix} \right.\ $ (474)

![](media/image3.wmf){width="6.686805555555556in"
height="2.838888888888889in"}

Figure 21: **Frequency weighting functions**

Next, the FFT weighting function $W_{f}(n)$ is determined by combining
the magnitude weighting function and the frequency weighting function.
Computing the FFT weighting function $W_{f}(n)$ for frame-end LSF
quantization is performed as follows:

$W_{f}(n) = W_{1}(n) \cdot W_{2}(n)$, *n=0,...,M-1* (477)

The FFT weighting function uses different types of frequency and
magnitude weighting functions depending on frequency bandwidth (NB, WB
or WB16 kHz) and coding modes (UC or others such as VC, GC, AC, IC and
TC).

Along with the FFT weightings $W_{f}$, another weighting function called
the inverse harmonic mean (IHM) is computed and defined as:

$W_{\text{IHM}}(n) = \frac{1}{\text{lsf}_{n} - \text{lsf}_{n - 1}} + \frac{1}{\text{lsf}_{n + 1} - \text{lsf}_{n}}$,
*n=0,...,M-1* (478)

The LSFs $\text{lsf}_{n}$are normalized between 0 and $\pi$, where the
first and the last weighting coefficients are calculated with this
pseudo LSFs $\text{lsf}_{0} = 0$and $\text{lsf}_{M} = \pi$. M is the
order 16 of the LP model.

IHM approximates the spectral sensitivity of LSFs by measuring how close
adjacent LSFs come. If two LSF parameters are close together the signal
spectrum has a peak near that frequency. Hence a LSF that is close to
one of its neighbours has a high scalar sensitivity and should be given
a high weight. The sensitivity of close neighbours LSF is even enhanced
by computing the squared of IHM:

$W_{\text{IHM}^{2}}(n) = W_{\text{IHM}}(n) \cdot W_{\text{IHM}}(n)$,
*n=0,...,M-1* (479)

The three set of weightings, $W_{f}$, $W_{\text{IHM}}$, and
$W_{\text{IHM}^{2}}$ are gathered into an *M* by 4 matrix as follows:

$E = \begin{bmatrix}
1 & W_{\text{IHM}}(0) & W_{\text{IHM}^{2}}(0) & W_{f}(0) \\
1 & W_{\text{IHM}}(1) & W_{\text{IHM}^{2}}(1) & W_{f}(1) \\
 \vdots & \vdots & \vdots & \vdots \\
1 & W_{\text{IHM}}(M - 1) & W_{\text{IHM}^{2}}(M - 1) & W_{f}(M - 1) \\
\end{bmatrix}$ (480)

The set of weightings are combined linearly by multiplying the matrix
*E* by a constant column vector *P* of dimension M:

$W = E \cdot P$ (481)

The vector *P* is different for NB, WB/SWB at internal sampling rate
12.8 kHz and WB/SWB at internal sampling rate 16 kHz. The vectors *P*
are derived off-line over a training data by minimizing the distance of
the linear combination *W* and the weightings derived mathematically
based on Gardner and Rao method, weightings near-optimal but too complex
for being computed on-line compared to an heuristic approach.

##### 5.2.2.1.2 Bit allocation

The frame-end LSF quantization codebooks and bit allocations depend on
the selected coding mode. In addition, different codebooks are used for
NB, WB and WB 16kHz modes. This means there is a separate, optimized
codebook for each coding mode and for each input bandwidth. In NB mode
the LSF vectors are in the range of 0-6400Hz although the input signal
has content only up to 4kHz. The WB mode corresponds to the mode where
the LSF parameters are estimated in the 0-6400Hz range. The WB2 mode
corresponds to the mode where the LSF parameters are estimated in the
0-8000Hz range and it is used in general for the higher bitrates.

Table 1 shows the bit allocation for frame-end LSF quantization for each
coding mode.

Table 23: Bit allocation for LSF vectors

+----------------+----------+----------+--------+---------+------------+-------+---+
| ACELP core     | Inactive | Unvoiced | Voiced | Generic | Transition | Audio |   |
|                |          |          |        |         |            |       |   |
| bitrate (kbps) |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 3.6            | 0        | 27       | 16     | 22      | 0          | 0     |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 7.2            | 22       | 37       | 31     | 29      | 31         | 22    |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 8.0            | 22       | 40       | 36     | 33      | 34         | 22    |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 9.6            | 31       | 31       | 31     | 31      | 0          | 0     |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 13.20          | 31       | 0        | 38     | 38      | 40         | 31    |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 16.40          | 31       | 0        | 31     | 31      | 0          | 31    |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 24.40          | 31       | 0        | 31     | 31      | 0          | 31    |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 32.00          | 41       | 0        | 0      | 41      | 41         | 0     |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
|                |          |          |        |         |            |       |   |
+----------------+----------+----------+--------+---------+------------+-------+---+
| 64.00          | 41       | 0        | 0      | 41      | 41         | 0     |   |
+----------------+----------+----------+--------+---------+------------+-------+---+

##### 5.2.2.1.3 Predictor allocation

There are three possible cases. In safety net only the mean removed LSF
vectors are quantized with the multi stage quantizer. In MA predictive
quantization the MA prediction error is quantized with the MSVQ. In
switched safety net /AR predictive there is a selection between
quantizing the mean removed LSF vector and the AR prediction error.
Table 24 specifies for each coding type and each bandwidth which
quantization scheme is used. The values in the table indicate safety net
(0), MA prediction (1), and AR prediction combined with safety net (2).
The value "-1" indicates that the corresponding mode is not used. The
coding modes that employ switched safety net/ AR prediction use one bit
to signal which one of the two variants is used.

Table 25: Predictive mode type for LSF quantizer

  --------------------- ---------- ---------- -------- --------- ------------ -------
                        Inactive   Unvoiced   Voiced   Generic   Transition   Audio
  Narrowband            1          1          2        2         0            2
  Wideband \<9.6kbps    1          1          2        2         0            2
  Wideband 16kHz        1          -1         2        1         0            1
  Wideband \>=9.6kbps   1          1          2        1         0            1
  --------------------- ---------- ---------- -------- --------- ------------ -------

The predictor values are optimized for all quantizer modes. For a given
coding mode and bandwidth, all bitrates use the same predictor values.
In general LSF values for voiced speech are considered quite stable over
several consecutive frames. Consequently the corresponding AR predictor
has the highest coefficient values. Other AR predictor coefficients are
slightly lower. For the MA predictor the same value of 1/3 is used
everywhere. The value is significantly lower than for AR coefficients
since the quantization error starts oscillating over time if the MA
coefficient is too large. The value is experimentally chosen to provide
reasonable prediction efficiency, stability and good error recovery.

##### 5.2.2.1.4 LSF quantizer structure

A safety net, predictive or switched safety-net predictive multi-stage
vector quantizer (MSVQ) is used to quantize the full length frame-end
LSF vector for all modes except voiced mode at 16 kHz internal sampling
frequency. The last stage of the MSVQ is a multiple scale lattice vector
quantizer (MSLVQ) \[22\]. For each coding mode number of 1 to 4
unstructured VQ stages are used followed by a MSLVQ stage. The number of
stages, number of bits per each stage and the codebook names for each
coding mode are detailed in table 26. The codebook names are mentioned
to illustrate how some of the codebooks are reused between modes.

Table 27: Optimized codebooks and their bit allocation for LSF
quantizers

+---------+---------+---------+---------+---------+---------+---------+
| Coding  | Bits VQ | Bits in | Co      | Bits VQ | Bits in | Co      |
| mode    | safety  | VQ      | debooks | pre     | VQ      | debooks |
|         | net     | stages  |         | dictive | stages  |         |
|         |         | --      |         | mode    | pre     |         |
|         |         | safety  |         |         | dictive |         |
|         |         | net     |         |         | mode    |         |
+---------+---------+---------+---------+---------+---------+---------+
| I       | \-      | \-      | \-      | 5       | 5       | I       |
| nactive |         |         |         |         |         | AA\_MA1 |
| NB      |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| U       | \-      | \-      | \-      | 8       | 4+4     | U       |
| nvoiced |         |         |         |         |         | VD\_MA1 |
| NB      |         |         |         |         |         |         |
|         |         |         |         |         |         | U       |
|         |         |         |         |         |         | VD\_MA2 |
+---------+---------+---------+---------+---------+---------+---------+
| Voiced  | 8       | 4+4     | SV      | 6       | 3+3     | GESV    |
| NB      |         |         | NB\_SN1 |         |         | NB\_AR1 |
|         |         |         |         |         |         |         |
|         |         |         | SV      |         |         | GESV    |
|         |         |         | NB\_SN2 |         |         | NB\_AR2 |
+---------+---------+---------+---------+---------+---------+---------+
| Generic | 9       | 5+4     | GETR    | 6       | 3+3     | GESV    |
| NB      |         |         | NB\_SN1 |         |         | NB\_AR1 |
|         |         |         |         |         |         |         |
|         |         |         | GETR    |         |         | GESV    |
|         |         |         | NB\_SN2 |         |         | NB\_AR2 |
+---------+---------+---------+---------+---------+---------+---------+
| Tra     | 9       | 5+4     | GETR    | \-      | \-      | \-      |
| nsition |         |         | NB\_SN1 |         |         |         |
| NB      |         |         |         |         |         |         |
|         |         |         | GETR    |         |         |         |
|         |         |         | NB\_SN2 |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| Audio   | 4       | 4       | AU      | 0       | 0       | \-      |
| NB      |         |         | NB\_SN1 |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| I       | \-      | \-      | \-      | 5       | 5       | I       |
| nactive |         |         |         |         |         | AA\_MA1 |
| WB      |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| U       | \-      | \-      | \-      | 12      | 4+4+4   | U       |
| nvoiced |         |         |         |         |         | VD\_MA1 |
| WB      |         |         |         |         |         |         |
|         |         |         |         |         |         | U       |
|         |         |         |         |         |         | VD\_MA2 |
|         |         |         |         |         |         |         |
|         |         |         |         |         |         | UV      |
|         |         |         |         |         |         | WB\_MA3 |
+---------+---------+---------+---------+---------+---------+---------+
| Voiced  | 8       | 4+4     | SV      | 6       | 3+3     | GESV    |
| WB      |         |         | WB\_SN1 |         |         | WB\_AR1 |
|         |         |         |         |         |         |         |
|         |         |         | SV      |         |         | GESV    |
|         |         |         | WB\_SN2 |         |         | WB\_AR2 |
+---------+---------+---------+---------+---------+---------+---------+
| Generic | 9       | 5+4     | GETR    | 6       | 3+3     | GESV    |
| WB      |         |         | WB\_SN1 |         |         | WB\_AR1 |
|         |         |         |         |         |         |         |
|         |         |         | GETR    |         |         | GESV    |
|         |         |         | WB\_SN2 |         |         | WB\_AR2 |
+---------+---------+---------+---------+---------+---------+---------+
| Tra     | 9       | 5+4     | GETR    | \-      | \-      | \-      |
| nsition |         |         | WB\_SN1 |         |         |         |
| WB      |         |         |         |         |         |         |
|         |         |         | GETR    |         |         |         |
|         |         |         | WB\_SN2 |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| Audio   | 4       | 4       | AU      | 0       | 0       | \-      |
| WB      |         |         | WB\_SN1 |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| I       | \-      | \-      | \-      | 5       | 5       | I       |
| nactive |         |         |         |         |         | AA\_MA1 |
| WB2     |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| U       | \-      | \-      | \-      | \-      | \-      | \-      |
| nvoiced |         |         |         |         |         |         |
| WB2     |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| Voiced  | \-      | \-      | BC-TCVQ | \-      | \-      | BC-TCVQ |
| WB2     |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| Generic | \-      | \-      | \-      | 5       | 5       | GEW     |
| WB2     |         |         |         |         |         | B2\_MA1 |
+---------+---------+---------+---------+---------+---------+---------+
| Tra     | 8       | 4+4     | TRW     | \-      | \-      | \-      |
| nsition |         |         | B2\_SN1 |         |         |         |
| WB2     |         |         |         |         |         |         |
|         |         |         | TRW     |         |         |         |
|         |         |         | B2\_SN2 |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| Audio   | \-      | \-      | \-      | 5       | 5       | AUW     |
| WB2     |         |         |         |         |         | B2\_MA1 |
+---------+---------+---------+---------+---------+---------+---------+
| CNG     | 4       | 4       | C       | \-      | \-      | \-      |
|         |         |         | NG\_SN1 |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| Generic | \-      | \-      | \-      | 5       | 5       | GE      |
| WB \>=  |         |         |         |         |         | WB\_MA1 |
| 9.6kbps |         |         |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+

The WB2 voiced mode is using BC-TCVQ technology detailed in subclause
5.2.2.1.5.

Overall the optimized VQ codebooks use 14,368 kBytes and the MSLVQ
parameters use 9.304 kBytes, including CNG mode.

The remaining LSF quantizer bits are used for the MSLVQ stage. The
quantization in all the stages is done such that it minimizes a weighted
Euclidean distortion. The calculation of the weights is detailed in
subclause 5.2.2.2.1. The search in the multi-stage quantizer is done
such that at most 2 candidates are kept per stage. For each candidate
obtained in the search in the unstructured optimized VQ, a residual LSF
vector is formed by subtracting from the LSF vector the codevectors
obtained in each unstructured VQ stage. If there is one optimized VQ
stage two residual LSF vectors are obtained, if there are two optimized
VQ stages, 4 candidates are obtained and so on.

Each residual LSF vector is split into two 8-dimensional sub vectors.
Each sub vector is coded as follows. The lattice codebook obtained
through the reunion of three D~8~^+^ lattice truncations differently
scaled. Each lattice truncation has a different number of leader
classes. The leader classes contained in the lattice truncations are
given in table 28.

Table 29: Lattice leader class vectors

  -------------------- ----------------------------------------- -------------------- ----------------------------------------
  Leader class index   Leader class vector                       Leader class index   Leader class vector
  0                    1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0    25                   3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0
  1                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5    26                   3.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0
  2                    1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0,   27                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 0.5, 0.5
  3                    2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0    28                   2.5, 1.5, 1.5, 1.5, 0.5, 0.5, 0.5, 0.5
  4                    1.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5    29                   2.5, 2.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5
  5                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0    30                   3.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5
  6                    2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0    31                   2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0
  7                    1.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5    32                   2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0
  8                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0    33                   3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
  9                    2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0    34                   3.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0
  10                   2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0    35                   4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
  11                   1.5, 1.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5    36                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 0.5
  12                   2.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5    37                   2.5, 1.5, 1.5, 1.5, 1.5, 0.5, 0.5, 0.5
  13                   2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0    38                   2.5, 2.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5
  14                   2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0    39                   3.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5
  15                   3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0    40                   2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0
  16                   1.5, 1.5, 1.5, 1.5, 0.5, 0.5, 0.5, 0.5    41                   3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0
  17                   2.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5    42                   3.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0
  18                   2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0    43                   3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
  19                   2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0    44                   4.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0
  20                   3.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0    45                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5
  21                   1.5, 1.5, 1.5, 1.5, 1.5, 0.5, 0.5, 0.5    46                   2.5, 1.5, 1.5, 1.5, 1.5, 1.5, 0.5, 0.5
  22                   2.5, 1.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5    47                   2.5, 2.5, 1.5, 1.5, 0.5, 0.5, 0.5, 0.5
  23                   2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0    48                   3.5, 1.5, 1.5, 0.5, 0.5, 0.5, 0.5, 0.5
  24                   2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0    49                   
  -------------------- ----------------------------------------- -------------------- ----------------------------------------

Given the bitrate available for the lattice codebook, the codebook is
thus defined by a set of three integers representing the number of
leader vectors for each truncation and three positive real number
representing the scale for each lattice truncation. For instance a
multiple scale lattice structure is defined by the number of leaders
(20, 14, 5, 16, 10, 0) and the scales (1.057, 1.794, 2.896, 1.154,
1.860, 0.0). It means that the first subvector is quantized with a
structure having three lattice truncations having 20, 14, and 5 leader
classes respectively, which are scaled with the scales 1.057, 1.794,
2.896, respectively. The second subvector has only two truncations
having 16 and 10 leader classes respectively. The truncations are
ordered such that, for each subvector, their number of leader classes is
descendingly ordered.

The difference in number of bits between the total number of bits for
LSF end encoding, the prediction bit if needed and the number of bits
used for unstructured VQ is used for the MSLVQ stage. The quantization
in all the stages is done such that it minimizes a weighted Euclidean
distortion. The calculation of the weights is detailed in subclause
5.2.2.1.1.

Suppose $x$ is the current LSF 8-dimensional sub vector and w its
corresponding weight vector. The vector $x$ is normalized, i.e.
component wise multiplied with the inverse of the off line estimated
standard deviation. The resulting vector $x$ is further sorted in
descending order based on the absolute value of its components and the
weights vector is arranged following the same order. Let $x'$ be the
vector of descendingly sorted absolute values of $x$ and $w'$ the
correspondingly sorted weights vector. The weighted distance to the best
codevector of each leader class corresponds to:

$\left\| x' - s_{j}l_{k} \right\|_{w'}^{2} = \sum_{i = 1}^{8}{x'^{2}(i)w'(i) - 2s_{j}\sum_{i = 1}^{8}{w'(i)x'(i)l_{k}(i) + s_{j}^{2}}}\sum_{i = 1}^{8}{w'(i)l_{k}^{2}(i)}$
(475)

where $l_{k}$is the leader vector corresponding to class $k$ and $s_{j}$
is the scale of the truncation $j$. Each lattice codebook has at most 3
truncations with their corresponding scales. Each truncation has a given
number of leader vector classes. The sum of cardinalities of the classes
for the truncations forming the codebook for the first LSF subvector and
for the second subvector are within the number of bits for the
considered operating point given by the overall bitrate and bandwidth.
Computing in the transformed input space the second and the third terms
from equation (476) directly gives a relative measure of goodness for
the best codevector from the leader class $k$ and truncation $j$ which
may be considered as a potential codevector for the truncation $j$ and
the leader class $k$.

$d_{\text{kj}} = - 2s_{j}\sum_{i = 1}^{8}{w'(i)x'(i)l_{k}(i) + s_{j}^{2}}\sum_{i = 1}^{8}{w'(i)l_{k}^{2}(i)}$
(477)

The part of equation (478) that is independent of the scale is
calculated only once for all the leader classes from the first
truncation, which is the one having the highest number of leader
classes. When adding the last term to the first sum of equation (483)
the product $x'(8)l_{k}(8)$is considered with negative sign if the
parity constraint of the leader $k$ is not obeyed by the signs of the
vector $x$. The contribution of the scale values is considered only
afterwards in order to obtain the value $d_{\text{kj}}$. The leader
class vector $k$ and the truncation *j* having the smallest
$d_{\text{kj}}$ correspond to the codevector of the current input
vector. The inverse permutation of the sorting operation on the input
vector applied on the winning leader vector $l_{k}$ gives the lattice
codevector after applying also the corresponding signs. If the parity of
leader vector $l_{k}$ is 0 the signs are identical to the signs of the
input vector. If the parity is 1 the signs are similar to the signs of
the input vector with the constraint that the number of negative
components is even. If the parity is -1 the signs are similar with signs
of the input vector with the constraint that the number of negative
components is odd. The final codevector is obtained after multiplication
with the scale $s_{j}$ and with the inverse of the component-wise
off-line computed standard deviation. The standard deviations are
individually estimated for each coding mode and bandwidth.

The candidate quantized LSF vectors are obtained by adding each lattice
quantized residual to the corresponding candidates from the upper
stages. The obtained candidates are increasingly sorted. For each sorted
candidate the weighted Euclidean distortion with respect to the original
LSF vector is calculated. The candidate that minimizes this distortion
is selected as codevector to be encoded. The indexes corresponding to
the first unstructured optimized VQ codebooks together with the index in
the lattice codebook are written in the bitstream. The index for the
lattice codebook is obtained as described in subclause 5.2.2.1.4.2.

For the CNG mode, using a total of 29 bits for the LSF quantization, the
multiple scale lattice codebook structure is specific to each of the 16
codevectors obtained in the first stage. In addition based on the value
of the last component of the 16 dimensional LSF vector only part of the
first stage codebook is searched. If the last component of $x$ is larger
than 6350 then the search is done only for the first 6 codevectors of
the first stage and the LSF vector corresponds to internal sampling
frequency of 16kHz, otherwise the search is performed within the last 10
codevectors of the first stage.

##### 5.2.2.1.4.1 Selection between safety net and predictive mode {#selection-between-safety-net-and-predictive-mode .H6}

For the modes where switched safety-net prediction is allowed the
selection between the two is done as follows. For frame error
concealment reasons safety net is imposed, and variable
$\text{force}_{\text{sf}}$ set to 1, under the following conditions:

\- first three ACELP frames after an HQ frame

\- in voiced class signals, if the frame erasure mode LSF estimate of
the next frame based on the current frame is at a distance from the
current frame LSF vector larger than 0.25. The distance, or stability
factor, is calculated as:

$\text{sf} = 1\text{.}\text{25} - \frac{\text{256}}{\text{400000}}\frac{D}{\text{frame}_{\text{len}}}$
(479)

where frame\_len is the frame length of the current frame and *D* is the
Euclidean distance between the current frame LSF vector and the FER
estimate for the next frame. In this case
$\text{force}_{\text{sf}}$calculated at the current frame is stored in
memory for use at the subsequent frame, thereby forcing the safety net
decision for the subsequent frame when $\text{force}_{\text{sf}}$is
equal to 1.

\- some cases of rate switching

Safety net usage is decided by the following code line:

if ( force\_sf \|\| Err\[0\] \< abs\_threshold \|\|
Err\[0\]\*(\*streaklimit) \< 1.05 \* Err\[1\])

Thus the safety net mode is selected if force\_sf is enabled or if for
the quantized safety net codevector the quantization distortion
(weighted Euclidean distance) is smaller than abs\_threshold of 41000
for NB or 45000 for WB frames. For these relatively low error values the
quantization is already transparent to original LSF values and it makes
sense from the error recovery point of view to use safety-net as often
as possible. Finally the safety net quantized error is compared to the
predictively quantized error, with scaling of 1.05 to prefer safety net
usage as well using \*streaklimit multiplying factor that is adaptive to
the number of consecutive predictive frames. The \*streaklimit factor
gets smaller, when the streak of continuous predictive frames gets
longer. This is done in order to restrict the very long usage streaks of
predictive frames for frame-erasure concealment reasons. For voiced
speech longer predictive streaks are allowed than for other speech
types. In voiced mode streak limiting starts after 6 frames, in other
modes after 3 frames.

##### 5.2.2.1.4.2 Indexing of the lattice codevector {#indexing-of-the-lattice-codevector .H6}

The indexes of each one of the two multiple scale lattice codevectors is
composed of the following entities:

\- scale indexes $j_{1},j_{2}$ for the two 8-dimensional subvectors

\- leader class index, $k_{1},k_{2}$for the two 8-dimensional subvectors

\- leader permutation index, $I_{l1},I_{l2}$ unsigned permutation index

\- sign index with parity constraint, $I_{s1},I_{s2}$

\- scale offset $O_{s}(j_{i}),i = 1,2$ the number of codevectors
corresponding to the truncations with smaller scale indexes

\- leader offset $O_{l}(k_{i}),i = 1,2$ the number of codevectors
corresponding to leader classes with smaller leader indexes

\- $\pi_{0}(k_{i})$, i=1,2 cardinality of unsigned leader class, i.e.
number of unsigned permutations in the class, shown in table 30.

\- $N_{2}$ is the number of codevectors for the second subvector

The index for each subvector is calculated using

$I_{i} = O_{s}(j_{i}) + O_{l}(k_{i}) + (I_{\text{si}}\pi_{0}(k_{i}) + I_{\text{li}})\ \text{for}\ i = 1,2$
(480)

The indexes I~li~ and I~si~ are obtained using the position encoding
based on counting the binomial coefficients and the sign encoding
described in \[26\].

Table 31: Cardinality of unsigned leader vector permutations

  --------------------- ----- --------------------- ----- --------------------- ------
  Leader vector index         Leader vector index         Leader vector index   
  0                     28    17                    56    34                    1120
  1                     1     18                    420   35                    8
  2                     70    19                    56    36                    8
  3                     8     20                    280   37                    280
  4                     8     21                    56    38                    168
  5                     28    22                    168   39                    56
  6                     168   23                    28    40                    420
  7                     28    24                    560   41                    336
  8                     1     25                    168   42                    840
  9                     280   26                    336   43                    28
  10                    28    27                    28    44                    168
  11                    56    28                    280   45                    1
  12                    8     29                    28    46                    168
  13                    56    30                    8     47                    420
  14                    420   31                    280   48                    168
  15                    56    32                    70                          
  16                    70    33                    8                           
  --------------------- ----- --------------------- ----- --------------------- ------

The binomial encoding used for calculating *I~l~*~1~ and *I~l~*~2~ uses
the fact that the cardinality of an unsigned leader class with distinct
values *v*~0~,...,*v*~n-1~, each having the number of occurrences
*k*~0~,...,*k*~n-1~ is given by:

$\begin{pmatrix}
S \\
k_{0}\text{.}\text{.}\text{.}k_{n - 1} \\
\end{pmatrix} = \begin{pmatrix}
S \\
k_{0} \\
\end{pmatrix}\begin{pmatrix}
S - k_{0} \\
k_{1} \\
\end{pmatrix}\text{.}\text{.}\text{.}\begin{pmatrix}
S - \sum_{}^{}k_{i} \\
k_{n - 1} \\
\end{pmatrix}$. (481)

The distinct values for each leader class vector and the number of each
value in each leader class vector are given in the following table:

Table 32: Leader vector distinct values, their number of occurrences,
and leader vector parities

  -------------------- ----------------- ----------------------- -------- -------------------- -------------------- ----------------------- --------
  Leader class index   Distinct values   Number of occurrences   Parity   Leader class index   Distinct values      Number of occurrences   Parity
  0                    1, 0,             2,6                     0        25                   3.0, 1.0, 0.0        1,5,2                   0
  1                    0.5               8                       1        26                   3.0, 2.0, 1.0, 0.0   1,1,1,5                 0
  2                    1, 0              4,4                     0        27                   1.5, 0.5             6,2                     1
  3                    2, 0              1,7                     0        28                   2.5, 1.5, 0.5,       1,3,4                   -1
  4                    1.5, 0.5          1,7                     -1       29                   2.5, 0.5             2,6                     1
  5                    1.0, 0.0          6,2                     0        30                   3.5, 0.5,            1,7                     -1
  6                    2.0, 1.0, 0.0,    1,2,5                   0        31                   2.0, 1.0, 0.0        3,4,1                   0
  7                    1.5, 0.5          2,6                     1        32                   2.0, 0.0             4,4                     0
  8                    1.0               8                       0        33                   3.0, 1.0             1,7                     0
  9                    2.0, 1.0, 0.0     1,4,3                   0        34                   3.0, 2.0, 1.0, 0.0   1,1,3,3                 0
  10                   2.0, 0.0          2,6                     0        35                   4.0, 0.0,            1,7                     0
  11                   1.5, 0.5          3,5                     -1       36                   1.5, 0.5             7,1                     -1
  12                   2.5, 0.5          1,7                     1        37                   2.5, 1.5, 0.5        1,4,3                   1
  13                   2.0, 1.0, 0.0     1,6,1                   0        38                   2.5, 1.5, 0.5        2,1,5                   -1
  14                   2.0, 1.0, 0.0     2,2,4                   0        39                   3.5, 1.5, 0.5        1,1,6                   1
  15                   3.0, 1.0, 0.0     1,1,6                   0        40                   2.0, 1.0, 0.0        4,2,2                   0
  16                   1.5, 0.5          4,4                     1        41                   3.0, 2.0, 1.0, 0.0   1,1,5,1                 0
  17                   2.5, 1.5, 0.5     1,1,6                   -1       42                   3.0, 2.0, 1.0, 0.0   1,2,1,4                 0
  18                   2.0, 1.0, 0       2,4,2                   0        43                   3.0, 0.0             2,6                     0
  19                   2.0, 0.0          3,5                     0        44                   4.0, 1.0, 0.0        1,2,5                   0
  20                   3.0, 1.0, 0.0,    1,3,4                   0        45                   1.5                  8                       1
  21                   1.5, 0.5          5,3                     -1       46                   2.5, 1.5, 0.5        1,5,2                   -1
  22                   2.5, 1.5, 0.5     1,2,5                   1        47                   2.5, 1.5, 0.5        2,2,4                   1
  23                   2.0, 1.0          2,6                     0        48                   3.5, 1.5, 0.5        1,2,5                   -1
  24                   2.0, 1.0, 0.0     3,2,3                   0        49                                                                
  -------------------- ----------------- ----------------------- -------- -------------------- -------------------- ----------------------- --------

The index for the two multiple scale lattice codevectors corresponding
to the two residual LSF subvectors are combined in a single index, I,
which is written in the bitstream.

$I = N_{2}I_{1} + I_{2}$. (482)

##### 5.2.2.1.5 LSFQ for voiced coding mode at 16 kHz internal sampling frequency : BC-TCVQ

##### 5.2.2.1.5.1 Block-constrained trellis coded vector quantization (BC-TCVQ) {#block-constrained-trellis-coded-vector-quantization-bc-tcvq .H6}

The VC mode operating at 16 kHz internal sampling frequency has two
decoding rates: 31 bits per frame and 40 bits per frame. The VC mode is
quantized by a 16-state and 8 stage block-constrained trellis coded
vector quantization (BC-TCVQ) scheme.

Trellis coded vector quantization (TCVQ) \[42\] generalizes trellis
coded quantization (TCQ) to allow vector codebooks and branch labels.
The main feature of TCVQ is the partitioning of an expanded set of VQ
symbols into subsets and the labelling of the trellis branches with
these subsets. TCVQ is based on a rate-1/2 convolutional code, which has
$N = 2^{v}$ trellis states and two branches entering/leaving each
trellis state. Given a block of m source vectors, the Viterbi algorithm
(VA) is used to find the minimum distortion path. This encoding
procedure allows the best trellis path to begin in any of N initial
states and end in any of N terminal states. In TCVQ, the codebook has
$2^{(R + \overset{\sim}{R})L}$ vector codewords. $\overset{\sim}{R}$ is
referred to as "codebook expansion factor" (in bits per dimension) since
the codebook has $2^{\overset{\sim}{R}L}$times as many codewords as a
nominal rate-$R$ VQ. The encoding is accomplished in the following two
steps.

Step 1. For each input vector, find the closest codeword and
corresponding distortion in each subset.

Step 2. Let the branch metric for a branch labelled with subset S be the
distortion found in step 1 and use the VA to find the minimum distortion
path through the trellis.

BC-TCVQ is a low-complexity approach that requires exactly one bit per
source sample to specify the trellis path. Figure 5 shows the concept of
'block constrained' and illustrates the search process of the Viterbi
algorithm with a 4-state and 8 stages trellis structure, which selects
'00' and '10' as initial states. When the initial state is '00', the
terminal state is selected to be one of '00' or '01' and when the
initial state is '10', the terminal state is selected to be one of '10'
or '11'. As an example, the survival path from the initial stage with
state '00' to the stages $(\text{NS} - \text{log}_{2}4)$ with state '00'
is shown by a dotted line. In this case, the only two possible trellis
paths for the last two stages are toward states '00' and '01'. This
example uses one bit for the initial state and one bit for the terminal
state. If the terminal state is decided, the path information for the
last two stages is not needed.

![](media/image4.wmf){width="5.145833333333333in" height="2.3125in"}

Figure 22: Block constrained concept in 4-state and 8 stages trellis
structure for BC-TCVQ encoding

For any $0 \leq k \leq \upsilon$, consider a BC-TCVQ structure that
allows $2^{k}$ initial trellis states and exactly $2^{v - k}$ terminal
trellis states for each allowed initial trellis state. A single VA
encoding, starting from the allowed initial trellis states, proceeds in
the normal way up to the vector stage $m - k$. It takes k bits to
specify the initial state, and $m - k$ bits to specify the path to
vector stage $m - k$. A unique terminating path, possibly dependent on
the initial trellis state, is pre-specified for each trellis state at
vector stage $m - k$ through vector stage $m$. Regardless of the value
of $k$, the encoding complexity is only a single VA search of the
trellis, and exactly m bits are required to specify an initial trellis
state and a path through the trellis.

The BC-TCVQ for VC mode at a 16kHz internal sampling frequency utilizes
16-state ($N$=16) and 8-stage ($\text{NS}$=8) TCVQ with 2-dimensional
($L$=2) vector. LSF subvectors with two elements are allocated to each
stage. Table 33 shows the initial states and terminal states for
16-state BC-TCVQ. In this case the parameters $k$ and $v$ are 2 and 4,
respectively. Four bits are used for both the initial state and terminal
state.

Table 34: Initial state and terminal state for 16-state BC-TCVQ

  --------------- ----------------
  Initial state   Terminal state
  0               0, 1, 2, 3
  4               4, 5, 6, 7
  8               8, 9, 10, 11
  12              12, 13, 14, 15
  --------------- ----------------

##### 5.2.2.1.5.2 Bit Allocations and codebook size for BC-TCVQ {#bit-allocations-and-codebook-size-for-bc-tcvq .H6}

The bit allocations for the LSF quantizer at 31 and 40 bits/frame are
summarized in tables 35 and 36.

Table 37: Bit allocation for the LSF quantizer at 31 bits/frame

+------------------+------------------------+------------------------+
| Parameters       | Bit allocation         |                        |
+------------------+------------------------+------------------------+
| BC-TCVQ          | Path information       | 2+4+2                  |
|                  |                        |                        |
|                  | (Initial states + path |                        |
|                  | + final states)        |                        |
+------------------+------------------------+------------------------+
|                  | Subset codewords       | 4 bits $$2 (Stages 1   |
|                  |                        | to 2)                  |
|                  |                        |                        |
|                  |                        | 3 bits $$2 (Stages 3   |
|                  |                        | to 4)                  |
|                  |                        |                        |
|                  |                        | 2 bits $$4 (Stages 5   |
|                  |                        | to 8)                  |
+------------------+------------------------+------------------------+
| Scheme selection | 1                      |                        |
+------------------+------------------------+------------------------+
| Total            | 31                     |                        |
+------------------+------------------------+------------------------+

Table 38: Bit allocation for the LSF quantizer at 40 bits/frame

+------------------+------------------------+------------------------+
| Parameters       | Bit allocations        |                        |
+------------------+------------------------+------------------------+
| BC-TCVQ          | Path information       | 2+4+2                  |
|                  |                        |                        |
|                  | (Initial states + path |                        |
|                  | + final states)        |                        |
+------------------+------------------------+------------------------+
|                  | Subset codewords       | 4 bits $$2 (Stages 1   |
|                  |                        | to 2)                  |
|                  |                        |                        |
|                  |                        | 3 bits $$2 (Stages 3   |
|                  |                        | to 4)                  |
|                  |                        |                        |
|                  |                        | 2 bits $$4 (Stages 5   |
|                  |                        | to 8)                  |
+------------------+------------------------+------------------------+
| SVQ              | Subset codewords       | 5 (1^st^ vector with   |
|                  |                        | dim.=8)                |
|                  |                        |                        |
|                  |                        | 4 (2^nd^ vector with   |
|                  |                        | dim.=8)                |
+------------------+------------------------+------------------------+
| Scheme selection | 1                      |                        |
+------------------+------------------------+------------------------+
| Total            | 40                     |                        |
+------------------+------------------------+------------------------+

Figures 6 and 7 show the LSF quantizer at 31 and 40 bits/frame,
respectively. The 1st and 2nd BC-TCVQ use the same bit allocation but
different codebook entries. The 3rd and 4th SVQ use the same bit
allocation and codebooks. The 31 bit LSF quantizer uses BC-TCVQ and the
40 bit LSF quantizer uses both BC-TCVQ and SVQ.

The following table summarizes the codebook size for BC-TCVQ and SVQ.
The overall codebook size is 2,432 words. In addition, there are several
tables for BC-TCVQ such as intra-prediction coefficients (56 words),
scale information (32 words) and branch information (192 words). The
total codebook size is 2,712 words.

Table 39: Codebook size for BC-TCVQ and SVQ

  ------------------------------ ------------- ------------- ------------- ------------- ------------- ------------- ------------- ------------- -----------------
                                 1^st^ stage   2^nd^ stage   3^rd^ stage   4^th^ stage   5^th^ stage   6^th^ stage   7^th^ stage   8^th^ stage   Total per frame
  Bits for BC-TCVQ subcodebook   4             4             3             3             2             2             2             2             
  Scalars for Predictive         256           256           128           128           64            64            64            64            1,024
  Scalars for Safety-net         256           256           128           128           64            64            64            64            1,024
  Bits for SVQ subcodebook       5             4                                                                                                 
  Scalars                        256           128           384                                                                                 
  Total                                        2,432                                                                                             
  ------------------------------ ------------- ------------- ------------- ------------- ------------- ------------- ------------- ------------- -----------------

##### 5.2.2.1.5.3 Quantization scheme selection {#quantization-scheme-selection .H6}

The quantization scheme for the VC mode consists of Safety-net and
Predictive schemes. The quantization scheme is selected in an open-loop
manner as shown in the figures 8 and 9. The scheme selection is done by
calculating the prediction error of unquantized LSFs.

The prediction error ($E_{k}$) of the $k$th frame is obtained from the
inter-frame prediction contribution$p_{k}\left( i \right)$, the
weighting function$w_{\text{end}}\left( i \right)$, and a mean-removed
unquantized LSF $z_{k}\left( i \right)$ as

$E_{k} = \sum_{i = 0}^{M - 1}{w_{\text{end}}(i)\left( z_{k}(i) - p_{k}(i) \right)^{2}}$
(483)

where

$p_{k}(i) = \rho(i){\hat{z}}_{k - 1}(i)$, for i=0,...,M (484)

and $\rho\left( i \right)$is the selected AR prediction coefficients for
VC mode and ${\hat{z}}_{k - 1}\left( i \right)$ is the mean-removed
quantized LSF of the previous frame and $M$ is the LPC order.

When $E_{k}$ is bigger than a threshold, it implies the tendency of the
current frame to be non-stationary. Then the safety-net scheme is a
better choice. Otherwise the predictive scheme is selected. In addition,
the streak limit (streaklimit) prevents the consecutive selection of the
predictive scheme.

The quantization scheme selection is shown by the following pseudo-code.

If $E_{k}$ \> streaklimit \* op\_loop\_thr

safety\_net = 1;

else

safety\_net = 0;

where $E_{k}$ is the prediction error of the kth frame and the open-loop
threshold (op\_loop\_thr) is 3,784,536.3.

If the safety-net flag (safety\_net) is set to 1, the safety-net scheme
is selected, and if the safety-net flag (safety\_net) is set to 0, the
predictive scheme is selected. The scheme selection is encoded using a
single bit.

##### 5.2.2.1.5.4 31 bit LSF quantization by the predictive BC-TCVQ with safety-net {#bit-lsf-quantization-by-the-predictive-bc-tcvq-with-safety-net .H6}

Figure 10 shows the predictive BC-TCVQ with safety-net for an encoding
rate of 31 bits.

![](media/image5.wmf){width="6.409027777777778in"
height="3.984722222222222in"}

Figure 23: Block diagram of the predictive BC-TCVQ with safety-net for
an encoding rate of 31bits/frame

The operation of the 31 bit LSF quantizer is described as follows. If
the safety-net scheme is selected, the mean-removed LSF vector,
$z_{k}(i)$, is quantized by the 1st BC-TCVQ and 1st intra-frame
prediction with 30 bits. If the predictive scheme is selected, the
prediction error, $r_{k}(i)$, which is the difference between the
mean-removed LSF vector $z_{k}(i)$ and the prediction vector $p_{k}(i)$
is quantized by the 2nd BC-TCVQ and 2nd intra-frame prediction with 30
bits.

An optimal index for each stage of BC-TCVQ is obtained by searching for
an index which minimizes $E_{\text{werr}}(p)$ of equation (485).

$E_{\text{werr}}(p) = \sum_{i = 0}^{1}{w_{\text{end}}(2(j - 1) + i)\left( t_{k}^{'}(2(j - 1) + i) - c_{j}^{p}(i) \right)^{2}}$,
for $p$=1,..., $P_{j}$ and $j$=1,..., $M$/2 (486)

where $P_{j}$ is the number of codevectors in the $j$th sub-codebook,
$c_{j}^{p}$ is the pth codevector of $j$th the subcodebook,
$w_{\text{end}}(i)$ is a weighting function, and
$t_{k}^{'} = \lbrack t_{k}^{t}(0),t_{k}^{t}(1),\text{.}\text{.}\text{.},t_{k}^{t}(\frac{M}{2} - 1)\rbrack$.

Intra-frame correlation typically remains in the inter-frame AR
prediction error vectors. The presence of significant intra-frame
correlation motivates the introduction of an intra-predictive coding
scheme for the AR prediction error vector, as shown in figure 11, in
order to increase the coding gain. The intra-frame prediction uses the
quantized elements of the previous stage. The difference between
$z_{k}(i)$ and its prediction is then quantized. The prediction is
formed for each trellis node using the output codevectors specified by
the survivor path associated with the particular node.

The prediction coefficients used for the intra-frame prediction is
predefined by the codebook training process. The prediction coefficients
are two-by-two matrices for the 2-dimensional vector. The intra-frame
prediction process of BC-TCVQ is as follows. The prediction residual
vector, $t_{k}(i)$, which is the input of the 1st BC-TCVQ, is computed
as

$t_{k}(0) = z_{k}(0)$

$t_{k}(i) = z_{k}(i) - {\overset{\sim}{z}}_{k}(i)$, for $i$=1,...,
$M$/2-1 (487)

where

${\overset{\sim}{z}}_{k}(i) = A_{i}{\hat{z}}_{k}(\text{i-1})$, for
$i$=1,..., $M$/2-1 (488)

where ${\overset{\sim}{z}}_{k}(i)$is the estimation of $z_{k}(i)$,
${\hat{z}}_{k}(i - 1)$ is the quantized vector of $z_{k}(i - 1)$, and
$A_{i}$ is the prediction matrix with 2$$2 which is computed as

$A_{i} = R_{\text{01}}^{i}\lbrack R_{\text{11}}^{i}\rbrack^{- 1}$, for
$i$=1,..., $M$/2-1, (489)

where

![](media/image6.wmf){width="1.148611111111111in"
height="0.2423611111111111in"}and
![](media/image7.wmf){width="1.2770833333333333in"
height="0.2423611111111111in"} (494)

and $M$ is the LPC order.

Then

${\hat{z}}_{k}(i) = {\hat{t}}_{k}(i) + {\overset{\sim}{z}}_{k}(i)$, for
$i$=0,..., $M$/2-1. (490)

The prediction residual, $t_{k}(i)$, is quantized by the 1st BC-TCVQ.
The 1st BC-TCVQ and the 1st intra-frame prediction are repeated to
quantize $z_{k}(i)$. Table 40 represents the designed prediction
coefficients $A_{i}$ for the BC-TCVQ in the safety-net scheme.

Table 41: Intra-frame prediction coefficients for the BC-TCVQ in the
safety-net scheme

  -------------------- -------------------
  Coefficient Number   Coefficient Value
                       
                       
                       
                       
                       
                       
                       
  -------------------- -------------------

For the predictive scheme, $r_{k}(i)$ is quantized by the 2nd BC-TCVQ
and the 2nd intra-frame prediction. An optimal index for each stage of
BC-TCVQ is obtained by searching for an index which minimizes
$E_{\text{werr}}(p)$ in equation (491).

The intra-frame prediction uses the same process with different
prediction coefficients as that of the safety-net scheme. Then

${\hat{r}}_{k}(i) = {\hat{t}}_{k}(i) + {\overset{\sim}{r}}_{k}(i)$, for
i=0,...,M/2-1. (492)

The prediction residual, $t_{k}(i)$, is quantized by the 2nd BC-TCVQ.
The 2nd BC-TCVQ and the 2nd intra-frame prediction are repeated to
quantize. Table 42 represents the designed prediction coefficients
$A_{i}$ for the BC-TCVQ in the predictive scheme.

Table 43: Intra-frame prediction coefficients for the BC-TCVQ in the
predictive scheme

  -------------------- -------------------
  Coefficient Number   Coefficient Value
                       
                       
                       
                       
                       
                       
                       
  -------------------- -------------------

##### 5.2.2.1.5.5 40 bit LSF quantization using the predictive BC-TCVQ/SVQ with safety-net {#bit-lsf-quantization-using-the-predictive-bc-tcvqsvq-with-safety-net .H6}

Figure 12 shows the predictive BC-TCVQ/split-VQ(SVQ) with safety-net for
an encoding rate of 40 bits. Both 31 bit LSF quantizer and 40 bit LSF
quantizer use the same codebook for BC-TCVQ.

![](media/image8.wmf){width="6.389583333333333in"
height="3.9381944444444446in"}

Figure 13: Block diagram of the predictive BC-TCVQ/SVQ with safety-net
for an encoding rate of 40 bits/frame

In the LSF quantization for an encoding rate of 40 bit/frame, the
difference between the mean-removed LSF and its BC-TCVQ output is
quantized by the 3rd and 4th SVQ, as shown in figure 14. The scheme
selection, 1st and 2nd BC-TCVQ, and 1st and 2nd intra-frame prediction
blocks of the 40 bit LSF quantizer are exactly same as those of the 31
bit LSF quantizer. Both LSF quantizers use same codebooks for the
BC-TCVQ.

If the current coding mode in the scheme selection block is selected as
the predictive scheme, the prediction error
$\text{pos}_{\text{en}\text{\_min}} = \underset{j = 0,\text{.}\text{.}\text{.},3}{\text{argmin}({\hat{f}}_{\text{env}_{\text{band}}}(j))}$
is derived by subtracting $p$ from the mean-removed LSF $z$. It is
quantized by the 2nd BC-TCVQ and the 2nd intra-frame prediction. The
residual signal $r_{2}$ is obtained by subtracting ${\hat{r}}_{1}$from
$r$. The residual signal $r_{2}$ is then split into two sub-vectors of
dimensions 8 and 8, and is quantized using the 4th SVQ. Since the low
band is perceptually more important than the high band, five bits are
allocated to the 1st 8-dimensional VQ and four bits are allocated to the
2nd 8-dimensional VQ. $r_{2}$ is quantized by the 4th SVQ to produce
${\hat{r}}_{2}$. $\hat{r}$ is then obtained by adding ${\hat{r}}_{2}$ to
${\hat{r}}_{1}$. Finally the predictive scheme output $\hat{z}$ is
derived by adding $p$ to $\hat{r}(n)$.

If the current coding mode is selected as the safety-net scheme, the
mean-removed LSF $z$ is quantized by the 1st BC-TCVQ and the 1st
intra-frame prediction. The residual signal $z_{2}$ is extracted by
subtracting ${\hat{z}}_{1}$from $z$, and it is quantized by the 3rd SVQ
to produce ${\hat{z}}_{2}$. The 3rd SVQ is exactly same as the 4th SVQ.
That is, both SVQ quantizers use same codebooks. Because the input
distribution of the 3rd SVQ is different from that of the 4th SVQ,
scaling factors are used to compensate the difference. Scaling factors
are computed by considering the distribution of both residual signals
$z_{2}$ and $r_{2}$. To minimize the computational complexity in in an
actual implementation, the input signal $z_{2}$ of the 3rd SVQ is
divided by the scaling factor, and the resulting signal is quantized by
the 3rd SVQ. The quantized signal ${\hat{z}}_{2}$ of the 3rd SVQ is
obtained by multiplying the quantized output with the scaling factor.
Table 44 shows the scaling factors for the quantization and
de-quantization. Finally, the quantized mean-removed LSF $\hat{z}$ is
derived by adding ${\hat{z}}_{2}$ to ${\hat{z}}_{1}$.

Table 45: Scaling factor for the SVQ

  --------------------------------------- -------- -------- -------- -------- -------- -------- -------- --------
  Dimension                               0        1        2        3        4        5        6        7
  Inverse scale factor for quantization   0.5462   0.5434   0.5553   0.5742   0.5800   0.5725   0.6209   0.6062
  Scale factor for de-quantization        1.8307   1.8404   1.8009   1.7416   1.7240   1.7467   1.6106   1.6497
  Dimension                               8        9        10       11       12       13       14       15
  Inverse scale factor for quantization   0.6369   0.6432   0.6351   0.6173   0.6397   0.6562   0.6331   0.6404
  Scale factor for de-quantization        1.5702   1.5548   1.5745   1.6199   1.5633   1.5239   1.5796   1.5615
  --------------------------------------- -------- -------- -------- -------- -------- -------- -------- --------

##### 5.2.2.1.6 Mid-frame LSF quantizer

For a more accurate representation of the spectral envelope during
signal transitions, the encoder quantizes mid-frame LSF coefficients. In
contrast to the frame-end LSF vector, the mid-frame LSF vector is not
quantized directly. Instead, a weighting factor is searched in a
codebook to calculate a weighted average between the quantized LSF
vectors of the current and the previous frames. Only 2-6 bits are
required depending on the bitrate and the coding mode (see Table 35a).

Table 35a: Bit allocation in mid-frame LSF quantization

  ----------------- ---- ---- ---- ---- ---- ----
  Bitrate \[bps\]   IC   UC   VC   GC   TC   AC
  7200              2    5    4    5    5    2
  8000              2    5    4    5    5    2
  9600              2    5    4    5    0    0
  13200             2    0    5    5    5    2
  16400             4    0    5    5    0    0
  24400             5    0    5    5    0    0
  32000             5    0    0    5    5    5
  64000             5    0    0    5    5    5
  ----------------- ---- ---- ---- ---- ---- ----

Before searching the codebook, the unquantized mid-frame LSF vector is
weighted with the LSF weighting function defined in Equation (481). For
simplicity, the following description will be provided by using LSP
vectors instead of LSF vectors. These two vectors are related by the
following simple relation $q(k) = \text{cos}(\omega(k))$ where *q*(*k*)
is the *k*th LSP coefficient and *ω*(*k*) is *k*th LSF coefficient. The
mid-frame LSP weighting can be expressed using the following formula

$q_{\text{wmid}}(k) = W(k)q_{\text{mid}}(k)$, for *k*=0,...,*M*-1.
(496a)

where $q_{\text{mid}}(k)$ is the *k*th unquantized LSP coefficient and
$W(k)$ is *k*th weighting factor of the function defined in Equation
(481). Note, that this is not the weighting factor which is quantized.
This weighting is based on the FFT spectrum where more weight is put on
perceptually important part of the spectrum and less weight elsewhere.

The weighting factor to be quantized is a vector of size *M* that is
searched in a closed-loop fashion such that the error between the
quantized mid-frame LSP coefficients and this weighted representation is
minimized in a mean-square sense. That is

$\begin{matrix}
E_{\text{mid}} = \sum_{k = 0}^{M - 1}\left\lbrack q_{\text{wmid}}(k) - \left\lbrack (1 - f_{\text{mid}}(k))q_{\text{wend}}^{\lbrack - 1\rbrack}(k) + f_{\text{mid}}(k)q_{\text{wend}}(k) \right\rbrack \right\rbrack^{2} \\
 \\
\end{matrix}$ (496b)

where $q_{\text{wend}}(k)$ is kth quantized weighted end-frame LSP
coefficient and $f_{\text{mid}}$ is the mid-frame weighting vector taken
from the codebook. To save computation complexity, both operations are
combined. That is

$\begin{matrix}
E_{\text{mid}} = \sum_{k = 0}^{M - 1}{w_{f}(k)\left\lbrack q_{\text{mid}}(k) - \left\lbrack (1 - f_{\text{mid}}(k))q_{\text{end}}^{\lbrack - 1\rbrack}(k) + f_{\text{mid}}(k)q_{\text{end}}(k) \right\rbrack \right\rbrack^{2}} \\
 \\
\end{matrix}$ (496c)

Once the winning weighting factor is found, the quantized LSP vector is
reordered to maintain a stable LP filter. After the quantization, the
end-frame and the mid-frame LSF vectors are used to determine the
quantized LP parameters in each subframe. This is done in the same way
as for unquantized LP parameters (see Equation (58) in Clause 5.1.96).

### 5.2.3 Excitation coding

The excitation signal coding depends on the coding mode. In general it
can be stated that in the absence of DTX/CNG operation, the excitation
signal is coded per subframes of 64 samples. This means that it is
encoded four times per frame in case of 12.8 kHz internal sampling rate
and five times per frame in case of 16 kHz internal sampling rate. The
exception is the GSC coding where longer subframes can be used to encode
some components of the excitation signal, especially at lower bitrates.

The excitation coding will be described in the following subclauses,
separately for each coding mode. The description of excitation coding
starts with the GC and VC modes. For the UC, TC, and GSC modes, it will
be described in subsequent subclauses with references to this subclause.

#### 5.2.3.1 Excitation coding in the GC, VC and high rate IC/UC modes

The GC,VC and high rate IC/UC modes are very similar and are described
together. The VC mode is used in stable voiced segments where the pitch
is evolving smoothly within an allowed range as described in subclause
5.1.13.2. Thus, the major difference between the VC and GC modes is that
more bits are assigned to the algebraic codebook and less to the
adaptive codebook in case of the VC mode as the pitch is not allowed to
evolve rapidly in the VC mode. The high-rate IC and UC modes are similar
and are used for signalling inactive frames where only a background
noise is detected, and unvoiced frames, respectively. The two modes
differ from GC mode mainly by their specific gain coding codebook. The
GC mode is then used in frames not assigned to a specific coding mode
during the signal classification procedure and is aimed at coding
generic speech and audio frames. The principle of excitation coding is
shown in a schematic diagram in figure 15. The individual blocks and
operations are described in detail in the following subclauses.

![](media/image9.wmf){width="5.449305555555555in"
height="4.240277777777778in"}

Figure 25: Schematic diagram of the excitation coding in GC and VC mode

##### 5.2.3.1.1 Computation of the LP residual signal

To keep the processing flow similar for all coding modes, the LP
residual signal is computed for the whole frame in the first processed
subframe of each frame, as this is needed in the TC mode. For each
subframe, the LP residual is given by

$r\left( n \right) = s_{\text{pre}}\left( n \right) + \sum_{i = 1}^{\text{16}}{{\hat{a}}_{i}s_{\text{pre}}\left( n - i \right)},\ n = 0,\ldots,\text{63}$
(497)

where $s_{\text{pre}}\left( n \right)$ is the pre-emphasized input
signal, defined in subclause 5.1.4 and ${\hat{a}}_{i}$are the quantized
LP filter coefficients, described in subclause 5.2.2.1.

In DTX operation the computed LP residual signal
![](media/image10.wmf){width="0.2916666666666667in"
height="0.20833333333333334in"} is attenuated by multiplying an
attenuation factor ![](media/image11.wmf){width="0.20833333333333334in"
height="0.15625in"} for all input bandwidths except NB. The attenuation
factor is calculated as

![](media/image12.wmf){width="1.4166666666666667in"
height="0.5416666666666666in"} (497a)

where ![](media/image13.wmf){width="0.625in"
height="0.20833333333333334in"} as determined in subclause 5.6.2.1.1 is
upper limited by ![](media/image14.wmf){width="0.9722222222222222in"
height="0.19375in"}, ![](media/image15.wmf){width="0.5694444444444444in"
height="0.19375in"} if the bandwidth is not WB or the latest bitrate
used for actively encoded frames
![](media/image16.wmf){width="0.7638888888888888in"
height="0.2361111111111111in"} is larger than 16.4 kbps. Otherwise
![](media/image17.wmf){width="0.2222222222222222in" height="0.19375in"}
is determined from a hangover attenuation table as defined in Table 35b.
![](media/image17.wmf){width="0.2222222222222222in"
height="0.19375in"}is only updated in the first SID frame after an
active signal period if two criteria are both fulfilled. The first
criterion is satisfied if AMR-WB IO mode is used or the bandwidth=WB.
The second criterion is met if the number of consecutive active frames
in the latest active signal segment was at least
![](media/image18.wmf){width="1.4416666666666667in"
height="0.1673611111111111in"} number of frames or if the current SID is
the very first encoded SID frame. The attenuation factor
![](media/image19.wmf){width="0.20833333333333334in"
height="0.1527777777777778in"} is finally lower limited
to![](media/image20.wmf){width="0.2222222222222222in"
height="0.20833333333333334in"}.

Table 35b: Attenuation floor

  ------------------------------------------------------------------------------------ ---------------------------------------------------------------------------------------
  **\                                                                                  **\
  Latest active bitrate \[kbps\]**                                                     **![](media/image21.wmf){width="0.2222222222222222in" height="0.20833333333333334in"}

  ![](media/image22.wmf){width="1.0944444444444446in" height="0.2263888888888889in"}   0.5370318

  ![](media/image23.wmf){width="1.4277777777777778in" height="0.2263888888888889in"}   0.6165950

  ![](media/image24.wmf){width="1.4277777777777778in" height="0.2263888888888889in"}   0.6839116

  ![](media/image25.wmf){width="1.4944444444444445in" height="0.2263888888888889in"}   0.7079458

  ![](media/image26.wmf){width="1.5479166666666666in" height="0.2263888888888889in"}   0.7079458
  ------------------------------------------------------------------------------------ ---------------------------------------------------------------------------------------

##### 5.2.3.1.2 Target signal computation

The target signal for adaptive codebook search is usually computed by
subtracting a zero-input response of the weighted synthesis filter
$W\left( z \right)H\left( z \right) = A\left( z/\gamma_{1} \right)H_{\text{de} - \text{emph}}\left( z \right)/\hat{A}\left( z \right)$from
the weighted pre-emphasized input signal. This is performed on a
subframe basis. An equivalent procedure for computing the target signal,
which is used in this codec, is filtering of the residual signal,
$r\left( n \right)$, through the combination of the synthesis filter
$H\left( z \right) = 1/\hat{A}\left( z \right)$and the weighting
filter$W\left( z \right) = A\left( z/\gamma_{1} \right)H_{\text{de} - \text{emph}}\left( z \right)$.
After determining the excitation signal for a given subframe, the
initial states of these filters are updated by filtering the difference
between the LP residual signal and the excitation signal. The memory
update of these filters is explained in subclause 5.2.3.1.8. The
residual signal, $r\left( n \right)$, which is needed for finding the
target vector, is also used in the adaptive codebook search to extend
the past excitation buffer. This simplifies the adaptive codebook search
procedure for delays less than the subframe size of 64 as will be
explained in the next subclause. The target signal in a given subframe
is denoted as $x\left( n \right)$.

##### 5.2.3.1.3 Impulse response computation

The impulse response, $h\left( n \right)$, of the weighted synthesis
filter

$W\left( z \right)H\left( z \right) = A\left( z/\gamma_{1} \right)H_{\text{de} - \text{emph}}\left( z \right)/\hat{A}\left( z \right)$
(498)

is computed for each subframe. Note that $h\left( n \right)$is not the
impulse response of the filter$H\left( z \right)$, but of the filter
$W\left( z \right)H\left( z \right)$. In the equation
above,$\hat{A}\left( z \right)$, is the quantized LP filter, the
coefficients of which are ${\hat{a}}_{i}$ (see subclause 5.2.2.1). This
impulse response is needed for the search of adaptive and algebraic
codebooks. The impulse response$h\left( n \right)$is computed by
filtering the vector of coefficients of the
filter$A\left( z/\gamma_{1} \right)$, extended by zeros, through the two
filters:
$1/\hat{A}\left( z \right)$and$H_{\text{de} - \text{emph}}\left( z \right)$.

##### 5.2.3.1.4 Adaptive codebook

##### 5.2.3.1.4.1 Adaptive codebook search {#adaptive-codebook-search .H6}

The adaptive codebook search consists of performing a closed-loop pitch
search, and then computing the adaptive codevector, $v\left( n \right)$,
by interpolating the past excitation at the selected fractional pitch
lag. The adaptive codebook parameters (or pitch parameters) are the
closed-loop pitch, $T_{\text{CL}}$, and the pitch gain, $g_{p}$(adaptive
codebook gain), calculated for each subframe. In the search stage, the
excitation signal is extended by the LP residual signal to simplify the
closed-loop search. The adaptive codebook search is performed on a
subframe basis. The bit allocation is different for the different modes.

In the first and third subframes of a GC, UC or IC frame, the fractional
pitch lag is searched with a resolution in the range \[34, 91½\], and
with integer sample resolution in the range \[92, 231\]depending on the
bit-rate and coding mode. Closed-loop pitch analysis is performed around
the open-loop pitch estimates. Always bounded by the minimum and maximum
pitch period limits, the range \[$d^{\left\lbrack 0 \right\rbrack}$--8,
$d^{\left\lbrack 0 \right\rbrack}$+7\] is searched in the first
subframe, while the range \[$d^{\left\lbrack 1 \right\rbrack}$--8,
$d^{\left\lbrack 1 \right\rbrack}$+7\] is searched in the third
subframe. The pitch period quantization limits are summarized in table
46.

Table 36: Pitch period quantization limits

  -------------- ----------------------------------- ------------- ------------- -------------
  Rates (kbps)   Sampling rate of the limits (kHz)   IC/UC         VC            GC
  7.2            12.8                                n.a.          \[17; 231\]   \[34; 231\]
  8.0            12.8                                n.a.          \[17; 231\]   \[20; 231\]
  9.6            12.8                                n.a.          \[29; 231\]   \[29; 231\]
  13.2           12.8                                n.a.          \[17; 231\]   \[20; 231\]
  16.4           16                                  n.a.          \[36; 289\]   \[36; 289\]
  24.4           16                                  n.a.          \[36; 289\]   \[36; 289\]
  32             16                                  \[21; 289\]   n.a.          \[21; 289\]
  64             16                                  \[21; 289\]   n.a.          \[21; 289\]
  -------------- ----------------------------------- ------------- ------------- -------------

For the second and fourth subframes, a pitch resolution depending on the
bit-rate and coding mode is used and the closed-loop pitch analysis is
performed around the closed-loop pitch estimates, selected in the
preceding (first or third) subframe. If the closed-loop pitch fraction
in the preceding subframe is 0, the pitch is searched in the range
\[$T_{I}$--8, $T_{I}$+7½\],
where$T_{I} = \left\lfloor T_{\text{CL}}^{\left\lbrack p \right\rbrack} \right\rfloor$is
the integer part of the fractional pitch lag of the preceding subframe
(*p* is either 0, to denote the first subframe, or 3 to denote the third
subframe). If the fraction of the pitch in the previous subframe is
$1/2$, the pitch is searched in the range \[$T_{I}$--7, $T_{I}$+8½\].
The pitch delay is encoded as follows. In the first and third subframe,
absolute values of the closed-loop pitch lags are encoded. In the third
and fourth subframe, only relative values with respect to the absolute
ones are encoded.

In the VC mode, the closed-loop pitch lag is encoded absolutely in the
first subframe and relatively in the following 3 subframes. If the
fraction of the closed-loop pitch of the preceding subframe is 0, the
pitch is searched in the interval \[$T_{I}$--4, $T_{I}$+3½\]. If the
fraction of the closed-loop pitch lag in the preceding subframe is
$1/2$, the pitch is searched in the range \[$T_{I}$--3, $T_{I}$+4½\].

The closed-loop pitch search is performed by minimizing a mean-squared
weighted error between the target signal and the past filtered
excitation (past excitation, convolved with$h\left( n \right)$). This is
achieved by maximizing the following correlation

$C_{\text{CL}} = \frac{\sum_{}^{}{x\left( n \right)y_{k}\left( n \right)}}{\sqrt{\sum_{}^{}{y_{k}\left( n \right)y_{k}\left( n \right)}}}$
(499)

where $x\left( n \right)$ is the target signal
and$y_{k}\left( n \right)$is the past filtered excitation at delay *k*.
Note that negative indices refer to the past signal. Note also that the
search range is limited around the open loop pitch lags, as explained
earlier. The convolution of the past excitation signal
with$h\left( n \right)$is computed only for the first delay in the
searched range. For other delays, it is updated using the recursive
relation

$y_{k}\left( n \right) = y_{k - 1}\left( n - 1 \right) + u\left( - k \right)h\left( n \right),\ n = 0,\ldots,\text{63}$
(500)

where $u\left( n \right)$,
$n = - \left( \text{231} + \text{17} \right),\ldots,\text{63}$, is the
excitation buffer. Note that in the search stage, the
samples$u\left( n \right)$, $n = 0,\ldots,\text{63}$, are unknown and
they are needed for pitch delays less than 64. To simplify the search,
the LP residual signal, $r\left( n \right)$, is copied
to$u\left( n \right)$for $n = 0,\ldots,\text{63}$, in order to make the
relation in equation (501) valid for all delays. If the optimum integer
pitch lag is in the range \[34, 91\], the fractions around that integer
value are tested. The fractional pitch search is performed by
interpolating the normalized correlation of equation (502) and searching
for its maximum. The interpolation is performed using an FIR filter for
interpolating the term in equation (503) using a Hamming windowed sinc
function truncated at$\pm \text{17}$. The filter has its cut off
frequency (--3 dB) at 5050 Hz and --6 dB at 5760 Hz in the down-sampled
domain, which means that the interpolation filter exhibits low-pass
frequency response. Note that the fraction is not searched if the
selected best integer pitch coincides with the lower end of the searched
interval.

Once the fraction is determined, the initial adaptive codevector,
$v^{'}\left( n \right)$, is computed by interpolating the past
excitation signal $u_{k}\left( n \right)$at the given phase (fraction).
In the following text, the fractional pitch lags (not the fractions) in
all subframes will be denoted
as$d_{\text{fr}}^{\left\lbrack i \right\rbrack}$, where the index
$i = 0,1,2,3$denotes the subframe.

In order to enhance the coding performance, a low-pass filter can be
applied to the adaptive codevector. This is important since the
periodicity doesn't necessarily extend over the whole spectrum. The low
pass filter is of the form
$b_{\text{LPF}}\left( z \right) = a + b \cdot z^{- 1} + a \cdot z^{- 2}$.
Thus, the adaptive codevector is given by

$v\left( n \right) = \sum_{i = - 1}^{1}{b_{\text{LPF}}\left( i + 1 \right)v^{'}\left( n + i \right),}\ n = 0,\ldots,\text{63}$
(504)

where
$b_{\text{LPF}} = \left\{ 0\text{.}\text{18},\ 0\text{.}\text{64},\ 0\text{.}\text{18} \right\}$
for $\text{sr}_{\text{celp}} = \text{12800}$ for rates at and above
32kbps and
$b_{\text{LPF}} = \left\{ 0\text{.}\text{21},\ 0\text{.}\text{48},\ 0\text{.}\text{21} \right\}$
otherwise.

An adaptive selection is possible by sending 1 bit per sub-frame. There
are then two possibilities to generate the excitation, the adaptive
codebook $v(n)$, $v(n) = v'(n)$ in the first path, or its low
pass-filtered version as described above in the second path. The path
which results in minimum energy of the target signal $x(n)$ is selected
for the filtered adaptive codebook vector.

Alternatively, the first or the second path can be used without any
adaptive selection. Table 37 summarizes the strategy for the different
combinations.

Table 38: Adaptive codebook filtering configuration

  -------------- ------------- -------------------- --------------------
  Rates (kbps)   IC/UC         VC                   GC
  7.2            n.a.          Non-filtered         LP filtered
  8.0            n.a.          Non-filtered         LP filtered
  9.6            n.a.          Non-filtered         LP filtered
  13.2           n.a.          Adaptive selection   Adaptive selection
  16.4           LP-filtered   Adaptive selection   LP filtered
  24.4           LP-filtered   Adaptive selection   LP filtered
  32             n.a.          n.a.                 Adaptive selection
  64             n.a.          n.a.                 Adaptive selection
  -------------- ------------- -------------------- --------------------

##### 5.2.3.1.4.2 Computation of adaptive codevector gain {#computation-of-adaptive-codevector-gain .H6}

The adaptive codevector gain (pitch gain) is then found by

$g_{p} = \frac{\sum_{n = 0}^{\text{63}}{x\left( n \right)y\left( n \right)}}{\sum_{n = 0}^{\text{63}}{y\left( n \right)y\left( n \right)}},\ \text{constrained}\ \text{by}\ 0 \leq g_{p} \leq 1\text{.}2$
(505)

where $y\left( n \right) = v\left( n \right) \ast h\left( n \right)$ is
the filtered adaptive codevector (zero-state response of
$W\left( z \right)H\left( z \right)$to$v\left( n \right)$).

To avoid instability in case of channel errors, $g_{p}$ is limited by
0.95, if the pitch gains of the previous subframes have been close to 1
and the LP filters of the previous subframes have been close to being
unstable (highly resonant).

The instability elimination method tests two conditions: resonance
condition using the LP spectral parameters (minimum distance between
adjacent LSFs), and gain condition by testing for high valued pitch
gains in the previous frames. The method works as follows. First, a
minimum distance between adjacent LSFs is computed as...

At 9.6, 16.4 and 24.4 kbps, the gain is further constrained. It is done
for helping the recovery after the loss of a previous frame.

$g_{p} = \text{min}(g_{p},0\text{.}8\sqrt{\frac{\sum_{n = 0}^{\text{63}}{x\left( n \right)y\left( n \right)}}{\sum_{n = 0}^{\text{63}}{y\left( n \right)y\left( n \right)}}})$
(506)

##### 5.2.3.1.5 Algebraic codebook

##### 5.2.3.1.5.1 Adaptive pre-filter {#adaptive-pre-filter .H6}

An important feature of this codebook is that it is a dynamic codebook,
whereby the algebraic codevectors are filtered through an adaptive
pre-filter$F\left( z \right)$. The transfer function of the adaptive
pre-filter varies in time in relation to parameters representative of
spectral characteristics of the signal. The pre-filter is used to shape
the frequency characteristics of the excitation signal to damp
frequencies perceptually annoying to the human ear. Here, a pre-filter
relevant to WB signals is used which consists of two parts: a
periodicity enhancement part
$1/\left( 1 - 0\text{.}\text{85}z^{- T} \right)$and a tilt part
$\left( 1 - \beta_{1}z^{- 1} \right)$. That is,

$F^{\left( 0 \right)}\left( z \right) = \frac{1 - \beta_{1}z^{- 1}}{1 - 0\text{.}\text{85}z^{- T}}$
(507)

The periodicity enhancement part of the filter colours the spectrum by
damping inter-harmonic frequencies, which are annoying to the human ear
in case of voiced signals. *T* is the integer part of the closed-loop
pitch lag in a given subframe (representing the fine spectral structure
of the speech signal) rounded to the ceiling,
i.e.,$\left\lceil d_{\text{fr}}^{\left\lbrack i \right\rbrack} \right\rceil$,
where *i* denotes the subframe.

The factor$\beta_{1}$of the tilt part of the pre-filter is related to
the voicing of the previous subframe. At 16.4 and 24.4 kbps it is
bounded by \[0.28, 0.56\] and it computed as

$\beta_{1} = 0\text{.}\text{28} + \frac{0\text{.}\text{28}E_{v}^{\left\lbrack - 1 \right\rbrack}}{E_{v}^{\left\lbrack - 1 \right\rbrack} + E_{c}^{\left\lbrack - 1 \right\rbrack}}$
(508)

Otherwise it is bounded by \[0.0, 0.5\] and is given by

$\beta_{1} = \frac{0\text{.}5E_{v}^{\left\lbrack - 1 \right\rbrack}}{E_{v}^{\left\lbrack - 1 \right\rbrack} + E_{c}^{\left\lbrack - 1 \right\rbrack}}$
(509)

where$E_{v}^{\left\lbrack - 1 \right\rbrack}$and
$E_{c}^{\left\lbrack - 1 \right\rbrack}$are the energies of the scaled
pitch codevector and the scaled algebraic codevector of the previous
subframe, respectively. The role of the tilt part is to reduce the
excitation energy at low frequencies in case of voiced frames.

Depending on bitrates, coding mode and the estimated level of background
noise, the adaptive pre-filter also includes a filter based on the
spectral envelope, which colours the spectrum by damping frequencies
between the formant regions. The final form of the adaptive pre filter
$F\left( z \right)$is given by

$F\left( z \right) = F^{\left( 0 \right)}\left( z \right)\frac{\hat{A}\left( z/\eta_{1} \right)}{\hat{A}\left( z/\eta_{2} \right)}$
(510)

where $\eta_{1} = 0\text{.}\text{75}$and $\eta_{2} = 0\text{.}9$if
$\text{sr}_{\text{celp}} = \text{12800}$Hz and
$\eta_{1} = 0\text{.}8$and $\eta_{2} = 0\text{.}\text{92}$if
$\text{sr}_{\text{celp}} = \text{16000}$Hz.

The codebook search is performed in the algebraic domain by combining
the pre-filter, $F\left( z \right)$, with the weighted synthesis filter
prior to the codebook search. Thus, the impulse response
$h\left( n \right)$of the weighted synthesis filter must be modified to
include the pre-filter $F\left( z \right)$. That is,
$h\left( n \right) \leftarrow h\left( n \right) \ast f\left( n \right)$,
where $f\left( n \right)$is the impulse response of the pre-filter.

##### 5.2.3.1.5.2 Overview of Algebraic codebooks used in EVS {#overview-of-algebraic-codebooks-used-in-evs .H6}

Depending on the bitrate and rendered bandwidth, algebraic codebooks of
different sizes are used in the EVS codec. The following tables
summarize the codebooks used in each subframe at different bitrates of
the EVS codec

Table 38: NB Algebraic codebook configurations (bits/subframe)

  ------------- ------------- ------------- ------------- -------------
  Rate (kbps)   IC            UC            VC            GC
  7.2           n.a.          n.a.          12/12/12/20   12/12/12/20
  8.0           n.a.          n.a.          12/20/12/20   12/20/12/20
  9.6           30/32/32/32   30/32/32/32   28/28/28/28   24/26/24/26
  13.2          n.a.          n.a.          36/43/36/43   36/36/36/43
  16.4          56/58/56/58   56/58/56/58   56/56/56/58   55/56/55/56
  24.4          96/98/96/98   96/98/96/98   96/96/96/98   94/96/96/96
  ------------- ------------- ------------- ------------- -------------

Table 39: WB Algebraic codebook configurations (bits/subframe)

+-------+-------+-------+-------+-------+-------+-------+-------+
| *     | *     | *     | *     | *     | **VC- | **GC- | **    |
| *Rate | *IC** | *UC** | *VC** | *GC** | FEC** | FEC** | GSC** |
| (kb   |       |       |       |       |       |       |       |
| ps)** |       |       |       |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 7.2   | n.a.  | n.a.  | 1     | 1     | n.a.  | n.a.  | n.a.  |
|       |       |       | 2/12/ | 2/12/ |       |       |       |
|       |       |       | 12/20 | 12/20 |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 8.0   | n.a.  | n.a.  | 1     | 1     | n.a.  | n.a.  | n.a.  |
|       |       |       | 2/20/ | 2/20/ |       |       |       |
|       |       |       | 12/20 | 12/20 |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 9.6   | 2     | 2     | 2     | 2     | n.a.  | n.a.  | n.a.  |
|       | 8/28/ | 8/28/ | 6/26/ | 0/26/ |       |       |       |
|       | 28/28 | 28/28 | 26/28 | 24/24 |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 13.2  | n.a.  | n.a.  | 2     | 2     | n.a.  | n.a.  | n.a.  |
|       |       |       | 8/36/ | 8/36/ |       |       |       |
|       |       |       | 36/36 | 28/36 |       |       |       |
|       |       |       |       |       |       |       |       |
|       |       |       | (TD   | (TD   |       |       |       |
|       |       |       | BWE)  | BWE)  |       |       |       |
|       |       |       |       |       |       |       |       |
|       |       |       | 3     | 3     |       |       |       |
|       |       |       | 6/36/ | 6/36/ |       |       |       |
|       |       |       | 36/43 | 36/36 |       |       |       |
|       |       |       |       |       |       |       |       |
|       |       |       | (FD   | (FD   |       |       |       |
|       |       |       | BWE)  | BWE)  |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 16.4  | 43/4  | 43/4  | 40/4  | 40/4  | n.a.  | n.a.  | n.a.  |
|       | 3/43/ | 3/43/ | 3/43/ | 3/40/ |       |       |       |
|       | 43/43 | 43/43 | 43/43 | 43/43 |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 24.4  | 75/7  | 75/7  | 73/7  | 73/7  | 73/7  | 70/7  | n.a.  |
|       | 5/75/ | 5/75/ | 5/73/ | 3/73/ | 3/73/ | 5/73/ |       |
|       | 75/75 | 75/75 | 75/75 | 75/73 | 73/75 | 73/73 |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 32    | 12/1  | n.a.  | n.a.  | 36/3  | n.a.  | n.a.  | n.a.  |
|       | 2/12/ |       |       | 6/36/ |       |       |       |
|       | 12/12 |       |       | 36/36 |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 64    | 12/1  | n.a.  | n.a.  | 36/3  | n.a.  | n.a.  | n.a.  |
|       | 2/12/ |       |       | 6/36/ |       |       |       |
|       | 12/12 |       |       | 36/36 |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+

Table 40: SWB Algebraic codebook configurations (bits/subframe)

  ----------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------
  **Rate (kbps)**   **IC**           **UC**           **VC**           **GC**           **VC-FEC**       **GC-FEC**       **GSC**
  9.6               24/26/24/26      24/26/24/26      20/26/24/24      20/20/20/20      n.a.             n.a.             n.a.
  13.2              n.a.             n.a.             28/36/28/36      28/28/28/36      n.a.             n.a.             n.a.
  16.4              36/36/36/36/36   36/36/36/36/36   34/36/36/36/36   34/36/34/36/36   n.a.             n.a.             n.a.
  24.4              62/65/62/65/62   62/65/62/65/62   62/62/62/65/62   62/62/62/62/62   62/62/62/62/62   61/61/62/61/62   n.a.
  32                12/12/12/12/12   n.a.             n.a.             36/28/28/36/36   n.a.             n.a.             n.a.
  64                12/12/12/12/12   n.a.             n.a.             36/36/36/36/36   n.a.             n.a.             n.a.
  ----------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------

Table 41: FB Algebraic codebook configurations (bits/subframe)

  ------------- ---------------- ---------------- ---------------- ---------------- ---------------- ----------------
  Rate (kbps)   IC               UC               VC               GC               VC-FEC           GC-FEC
  16.4          36/36/36/36/36   36/36/36/36/36   34/36/36/34/36   34/34/36/34/36   n.a.             n.a.
  24.4          62/62/65/62/65   62/62/65/62/65   62/62/62/62/62   62/62/62/62/62   61/62/61/62/62   61/61/61/61/61
  32            12/12/12/12/12   n.a.             n.a.             36/28/28/36/36   n.a.             n.a.
  64            12/12/12/12/12   n.a.             n.a.             36/36/36/36/36   n.a.             n.a.
  ------------- ---------------- ---------------- ---------------- ---------------- ---------------- ----------------

VC-FEC and GC-FEC are specific configurations for which 4 bits are
reserved to transmit LPC-based information exploited by the decoder in
case of error of the previous frame.

##### 5.2.3.1.5.3 Codebook structure and pulse indexing of the 7-bit codebook {#codebook-structure-and-pulse-indexing-of-the-7-bit-codebook .H6}

In the 7-bit codebook, the algebraic vector contains only 1 non-zero
pulse at one of 64 positions. The pulse position is encoded with 6 bits
and the sign of the pulse is encoded with 1 bit. This gives a total of 7
bits for the algebraic code. The sign index here is set to 1 for
positive signs and 0 for negative signs.

##### 5.2.3.1.5.4 Codebook structure and pulse indexing of the 12-bit codebook {#codebook-structure-and-pulse-indexing-of-the-12-bit-codebook .H6}

In the 12-bit codebook, the algebraic vector contains only 2 non-zero
pulses. The 64 positions in a subframe are divided into 2 tracks, where
each track contains one pulse, as shown in table 39.

Table 40: Potential positions of individual pulses in the 12-bit
algebraic codebook

  ----------- ----------- ---------------------------------------------------------------------------------------------------------------------------
  **Track**   **Pulse**   **Positions**
  1           0           0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62
  2           1           1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63
  ----------- ----------- ---------------------------------------------------------------------------------------------------------------------------

Each pulse position in one track is encoded with 5 bits and the sign of
the pulse in the track is encoded with 1 bit. This gives a total of 12
bits for the algebraic code. The sign index here is set to 0 for
positive signs and 1 for negative signs.

The index of the signed pulse is given by

$I = p + s \cdot 2^{M}$ (511)

where $p$ is the position index, $s$ is the sign index, and $M = 5$ is
the number of bits per track. For example, a pulse at position 31 has a
position index of 31/2 = 15 and it belongs to the track with index 1
(second track).

##### 5.2.3.1.5.5 Codebook structure and pulse indexing of the 20-bit and larger codebooks {#codebook-structure-and-pulse-indexing-of-the-20-bit-and-larger-codebooks .H6}

In the 20-bit or larger codebooks, the codevector contains 4 non-zero
pulses. All pulses can have the amplitudes +1 or --1. The 64 positions
in a subframe are divided into 4 tracks, where each track contains one
pulse, as shown in table 41.

Table 43: Potential positions of individual pulses in the 20-bit
algebraic codebook

  ----------- ----------- --------------------------------------------------------------
  **Track**   **Pulse**   **Positions**
  1           0           0, 4, 8, 12, 16, 20, 24, 28, 32 36, 40, 44, 48, 52, 56, 60
  2           1           1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61
  3           2           2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62
  4           3           3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63
  ----------- ----------- --------------------------------------------------------------

Each pulse position in one track is encoded with 4 bits and the sign of
the pulse in the track is encoded with 1 bit. This gives a total of 20
bits for the algebraic code.

##### 5.2.3.1.5.6 Pulse indexing of the algebraic codebook {#pulse-indexing-of-the-algebraic-codebook .H6}

The objective is to enumerate all possible constellations of pulses in a
vector $c$ which corresponds to one track of length $L$ within a
sub-frame. That is, vector $c$ has signed integer values such that its
norm-1 is $|x|_{1} = p$, whereby we say that $c$ contains $p$ pulses.

We can then partition the vector $c$ into two parts,
$c = \left\lbrack c_{1}\text{,~}c_{2} \right\rbrack$ such that the
partitions are of length $L_{1}$ and $L_{2} = L - L_{1}$ and contain
$p_{1}$ and $p_{2} = p - p_{1}$ pulses respectively. The number of
different constellations for the original vector $c$ can then be
determined by the recursive formulae:

$\left\{ \begin{matrix}
f\left( p,L \right) = \underset{p_{1} = 0}{\overset{p}{\sum_{}^{}}}f\left( p_{1},L_{1} \right)f\left( p - p_{1},L_{2} \right),p > 0,L > 1 \\
 \\
\end{matrix}p > 1 \middle| \middle| f\left( 0,L \right) = 1 \middle| \middle| L > 0\text{.} \middle| \right.\ $
(512)

For computational efficiency, the values of this function can be
pre-calculated and placed in a table.

Above equation gives the number of possible states for given $p$ and
$L$. We can then enumerate a specific state, where $c_{1}$ and $c_{2}$
have $\pi_{1}$ and $\pi_{2} = p - \pi_{1}$ pulses respectively. The
number of states that have less pulses than $\pi_{1}$ in partition
$c_{1}$ is

$s\left( c_{1},c_{2} \right) = \underset{p_{1} = 0}{\overset{\pi_{1} - 1}{\sum_{}^{}}}f\left( p_{1},L_{1} \right)f(p - p_{1},L - L_{1})\text{.}$
(513)

We can then define that overall state has
$s\left( c \right) \geq s\left( c_{1},c_{2} \right)$, whereby the
overall state can be encoded with the recursion

$s\left( c \right) = s\left( c_{1},c_{2} \right) + s\left( c_{1} \right) + f\left( p_{1},L_{1} \right)s\left( c_{2} \right),$
(514)

where the boundary conditions are

$s\left( c \right) = \left\{ \begin{matrix}
0, & L = 1\text{,~}c\left( 1 \right) \geq 0 \\
1, & L = 1,c\left( 1 \right) < 0\text{.} \\
\end{matrix} \right.\ $ (515)

The state can be decoded by the algorithm

1.  Set $p_{1}: = 1$ and choose partitioning length $L_{1} \geq 1$ and
    $L_{2} \geq 1$.

2.  Calculate $s\left( c_{1},c_{2} \right)$ with $p_{1}$.

3.  If $s\left( c \right) < s\left( c_{1},c_{2} \right)$ then
    $\pi_{1} = p_{1} - 1$. Otherwise, set $p_{1}: = p_{1} + 1$ and go
    to 2.

The states of the partitions $s\left( c_{2} \right)$ and
$s\left( c_{1} \right)$ can then be calculated from the integer and
reminder parts of the fraction
$\frac{s\left( c \right) - s\left( c_{1},c_{2} \right)}{f\left( p_{1},L_{1} \right)}\text{.}$
We can then recursively determine the state of each position in the
vector $c$ until a partition has $L_{k} = 1$, whereby

$c_{k}\left( 1 \right) = \left\{ \begin{matrix}
 + \pi_{k}, & \text{for~}s\left( c_{k} \right) = 0 \\
 - \pi_{k}, & \text{for~}s\left( c_{k} \right) = 1\text{.} \\
\end{matrix} \right.\ $ (516)

Observe that both the number of states $f\left( p,L \right)$ as well as
the state $s\left( c \right)$ are integer numbers which can become
larger than 32 bits. We must therefore employ arithmetic operations
which support long integers throughout the algorithm.

##### 5.2.3.1.5.7 Pulse indexing of the 43-bit codebook {#pulse-indexing-of-the-43-bit-codebook .H6}

The joint indexing encoding procedure of three pulses on two tracks is
described as follows:

For 3 pulses on a track, the occurrence probability of 3 different pulse
positions on a track is the highest, and the occurrence probability of 2
different pulse positions on a track is the second highest, and even the
pulses have a higher occurrence probability on the left position of the
track than on the right position of the track because the algebraic
codebooks need to compensate for the boundary leap of adaptive codebooks
between two [neighbour](http://www.iciba.com/neighbour) sub-frame. So
the case of the first pulse with lower position order will be encoded
with a smaller index value and the case of more different pulse
positions with higher occurrence probability will also be encoded with a
smaller index value. The rule is same in case of more than 3 pulses on a
track. This rule can be used to save bit in the multi-track joint
indexing encoding.

1.  Firstly, the pulse information for each track is indexed as follows:
    (here we suppose that $Q(Q = 3)$ pulses are assigned for each track,
    and the total quantity of positions on the track is $M$)

```{=html}
<!-- -->
```
1)  Analyse the statistics about the positions of the $Q$ pulses to be
    > encoded on a track and obtain pulse distribution on the track, it
    > includes: quantity (namely $\text{pos}_{\text{num}} = N$) of pulse
    > positions with pulses in it, the pulse position distribution which
    > includes pulse position vector:
    > $P(N) = \left\{ p(0),p(1),\text{.}\text{.}\text{.},p(N - 1) \right\}$,
    > $N$ is the quantity of pulse positions , $p(i)$ is the *i^th^*
    > pulse positions with pulse in it on the track, and quantity of
    > pulses in each pulse position with pulse in it which includes
    > pulse number vector
    > $\text{SU}(N) = \left\{ \text{su}(0),\text{su}(1),\text{.}\text{.}\text{.},\text{su}(N - 1) \right\}$,
    > $\text{su}(0) + \text{su}(1) + \cdots + \text{su}(N - 1) = Q$,
    > where $Q$ is the number of pulses per track, $\text{su}(i)$ is the
    > number of pulses in position $p(i)$, and pulse sign vector
    > $S(N) = \left\{ s(0),s(1),\text{.}\text{.}\text{.},s(N - 1) \right\}$,
    > $s(i)$ is the *i^th^* sign in position $p(i)$. If there are pulses
    > having the same positions (pulses with the same positions have the
    > same signs), they are merged into one pulse and the number of
    > pulses for each pulse position as well as the pulse sign is saved.
    > Pulse position are sorted in ascend order, the pulse sign is also
    > adjusted based on the order of pulse position.

2)  Compute the offset index $I_{1}(N)$ according to the quantity of
    > pulse positions, the offset index is saved in a table and used in
    > both encoder and decoder sides. Each offset index in the table
    > indicate a unique number of pulse positions in the track, in case
    > $Q = N$, the offset index only indicate a pulse distribution of
    > pulse positions on the track $P(N)$, in case $Q > N$, the offset
    > index indicate many $\text{SU}(N)$ which have a same pulse
    > distribution of pulse positions on the track $P(N)$.

3)  Compute the pulse- position index $I_{2}(N)$ according to the pulse
    > distribution of pulse positions on the track
    > ($0 \leq I_{2}(N) < C_{M}^{N}$). The $I_{2}(N)$only indicate a
    > pulse distribution of pulse positions on the track $P(N)$ among
    > all the pulse distribution of $I_{1}(N)$. Permuting serial numbers
    > of the positions
    > $P(N) = \left\{ p(0),p(1),\text{.}\text{.}\text{.},p(N - 1) \right\}$
    > and all possible values of $P(N)$ are ordered from a smaller value
    > to a greater value $p(0) < p(1) < \cdots < p(N - 1)$, $N$ refers
    > to the quantity of positions with pulses in it, $M$ is the total
    > quantity of positions on the track. Compute $I_{2}(N)$ by using
    > the permutation method as follows:

$I_{2}(N) = C_{M}^{N} - C_{M - p(0)}^{N} + \sum_{n = 1}^{N - 1}{\lbrack C_{M - p(n - 1) - 1}^{N - n} - C_{M - p(n)}^{N - n}\rbrack},\ 0 \leq I_{2}(N) < C_{M}^{N},\ 0 \leq p(0)\text{\ <\ }p(1)\text{\ <}\cdots\text{<\ }p(N\text{\ -\ 1}) < \text{15}$
(514)

> wherein $p(n)$ represents a position serial number of an *n^th^*
> position that has pulses on it, $n \in \lbrack 0,N - 1\rbrack$,
> $p(0) \in \lbrack 0,M - N\rbrack$,
> $p(n) \in \lbrack p(n - 1) + 1,M - N + n\rbrack$,
> $p(0) < p(1) < \cdots < p(N - 1)$. For 43bit mode, 3 pulses on a
> track, $M = \text{16}$, $0 < N \leq 3$.
>
> $I_{2}(N) = C_{\text{16}}^{3} - C_{\text{16} - p(0)}^{3} + C_{\text{15} - p(0)}^{2} - C_{\text{16} - p(1)}^{2} + C_{\text{15} - p(1)}^{1} - C_{\text{16} - p(2)}^{1}$
> (515)
>
> Compute the pulse-number index $I_{3}(N)$ according to the quantity of
> pulses in each pulse position as follows:
>
> $I_{3}(N)$ is determined according to $\text{SU}(N)$ which represents
> the quantity of pulses in each position with pulses. In order to
> determine correspondence between $\text{SU}(N)$ and $I_{3}(N)$ through
> algebraic calculation, a calculation method of the third index
> $I_{3}(N)$ is provided below:
>
> For a track, situations that a track with $N$ pulse positions and $Q$
> pulses are mapped to situations that a $N$ positions track have
> $Q - N$ pulses, where $Q$ represents the total number of pulses that
> are required to be encoded and on the track. For example, in the
> condition of 6-pulse 4-position ($Q$ =6, $N$=4) situations,
> $\text{SU}(N)$ is {1, 2, 1, 2}, 1 is subtracted from the number of
> pulses in each position (because each position has at least one pulse)
> to obtain {0, 1, 0, 1}, that is, information of $\text{SU}(N)$ is
> mapped to a 2-pulse 4-position ($Q$ =2, $M$=4) encoding situation.
> Figure 16 gives an example of the mapping for $I_{3}(N)$.

![](media/image27.png){width="4.583333333333333in"
height="1.9673611111111111in"}

Figure 17: Example of mapping for $I_{3}(N)$

> According to set order, all possible distribution situations of
> $Q - N$ pulses on $N$ positions are arrayed, and an arrayed serial
> number is used as the index$I_{3}(N)$ indicating the number of pulses
> on a position that has pulse.
>
> A calculation formula reflecting the foregoing calculation method is:

$I3(N) = C_{\text{PPT}}^{\text{ΔN}} - C_{\text{PPT} - q(0)}^{\text{ΔN}} + \sum_{h = 1}^{\text{ΔN} - 1}{\lbrack C_{\text{PPT} - h - q(h - 1)}^{\text{ΔN} - h} - C_{\text{PPT} - h - q(h)}^{\text{ΔN} - h}\rbrack}$
(517)

> wherein $\text{ΔN} = Q - N,\text{PPT} = Q - 1$, $q(h)$ represents a
> position serial number of an (h + 1)^th^ pulse,
> $h \in \lbrack 0,\text{ΔN} - 1\rbrack$,
> $q(h) \in \lbrack 0,N - 1\rbrack$,
> $q(0) \leq q(1) \leq \text{.}\text{.}\text{.} \leq q(\text{ΔN} - 1)$,
> and ∑ indicates summation.
>
> Compute the pulse-sign index$I_{4}(N)$ based on the $N$ pulse sign
> information.
>
> The pulse sign represented by $s(i)$ may be a positive value or a
> negative value. A simple coding mode is generally applied, $s(i) = 0$
> represents a positive pulse and $s(i) = 1$ represents a negative
> pulse.
>
> Generate the global index $I$. Combine the indices $I_{1}(N)$,
> $I_{2}(N)$, $I_{3}(N)$ and $I_{4}(N)$ to get the global index $I$ as
> follows :
>
> $I = I_{1}(N) + I_{\text{23}} \times 2^{N} + I_{4}(N),1 \leq N \leq M,I \in \lbrack 0,W - 1\rbrack$
> (518)
>
> $I_{\text{23}} = I_{3}(N) \times C_{M}^{N} + I_{2}(N)$ (519)
>
> Here $W$ is the upper range of $I$ which is also the number of total
> permutations of $Q$ pulses.

2\. Combine the index of the two 3-pulse tracks together which is
encoded as in step 1, suppose the indexes of the two tracks are
$\text{Ind}_{1}$ and $\text{Ind}_{2}$ respectively,
$\text{Ind}_{1} \in \lbrack 0,W_{1} - 1\rbrack$ and
$\text{Ind}_{2} \in \lbrack 0,W_{2} - 1\rbrack$, then the
$\text{Joint\_index}$ is as below:

> $\text{Joint\_index} = \text{Ind}_{1} \ast W_{2} + \text{Ind}_{2}$
> (520)

3\. Encode the joint index $\text{Joint\_index}$. (Suppose encode with
25 bits). In order to reduce the number of bits used for pulse indexing,
a threshold $\text{THR}$ is set at 3611648 for 3-pulses, according to
the the pulse number, combination of the occurrence probability and the
number of bits that may be saved. If the joint index
$\text{Joint\_index}$ is smaller than $\text{THR}$, 24 bits will be used
to encode the joint index $\text{Joint\_index}$. If the joint index
$\text{Joint\_index}$ is bigger than or equal to $\text{THR}$,
$\text{THR}$ will be added into the joint index $\text{Joint\_index}$
and 25 bits will be used to encode the joint index
$\text{Joint\_index}$. This procedure is described as below:

If ($\text{Joint\_index}$\<$\text{THR}$)

{

$\text{Joint\_index}$ is encoded with 24 bits.

}

Else

{

$\text{Joint\_index} = \text{Joint\_index} + \text{THR}$

$\text{Joint\_index}$ is encoded with 25 bits.

}

For two pulses on the other track, the index for each track is encoded
just as pulse indexing of the 20-bit codebook, but there is no joint
indexing procedure, then the index for each track is transmitted one by
one.

5.2.3.1.5.8 Multi-track joint coding of pulse indexing

The codebook for more than three pulses on a track have idle space in
difference ratio, joint encoding for more than two tracks may enable
idle codebook spaces in single-track encoding to be combined, and once
combined idle spaces are sufficient, one actual encoding bit may be
reduced. If several encoding indexes are directly combined, the final
encoding length may be very large, or even may exceed the bit width
(such as 64 bits) generally used for operating, so a general solution is
to split each encoding index into two part and only all the high part is
combined together in order to avoid directly combining.

The method is described as follows: the value range of the original
index $\text{Ind}_{t}$ is divided into several intervals by a factor
$\text{SLF}_{t}$, correspondingly the original index $\text{Ind}_{t}$ is
split into two indexes $\text{Ind}_{t0}$ and $\text{Ind}_{t1}$ by the
factor $\text{SLF}_{t}$, the length of each interval is not greater than
$\text{SLF}_{t}$, $\text{SLF}_{t}$ is a positive integer,
$\text{Ind}_{t0}$ denotes a serial number of an interval to which
$\text{Ind}_{t}$ belongs, and $\text{Ind}_{t1}$ denotes a serial number
of $\text{Ind}_{t}$ in the interval to which $\text{Ind}_{t}$belongs
(apparently,$\text{Ind}_{t1} \leq \text{SLF}_{t}$), and:
$\text{Ind}_{t} \leq \text{Ind}_{t0} \times \text{SLF}_{t} + \text{Ind}_{t1}$;

The most economical case of splitting is performed as following:

$\text{Ind}_{t0} = \text{Int}(\frac{\text{Ind}_{t}}{\text{SLF}_{t}})$,
where $\text{Int}()$ denotes rounding down to an integer, and

$\text{Ind}_{t1} = \text{Ind}_{t}\text{\%}\text{SLF}_{t}$, where
$\text{\%}$ denotes taking a remainder.

If a combined index needs to achieve better effect of saving encoding
bits, it is needed to select a split index that retains the space
characteristics of $\text{Ind}_{t}$ as much as possible, and therefore,
for the track t providing a split index to participate in combination,

if $\text{SLF}_{t} = 2^{K_{t}}$, it is appropriate to select *Ind~t0~*
to participate in combination, and

if
$\text{SLF}_{t} = \text{Int}(\frac{\text{Ind}_{t\text{max}}}{2^{K_{t}}})$,
it is appropriate to select Ind~t1~ to participate in combination.

![](media/image28.png){width="4.195833333333334in"
height="1.4784722222222222in"}

Figure 18: The split factor selection and the corresponding codebook
space section

Each track may adopt different $\text{SLF}_{t}$, according to the pulse
number on it

Table 42: the parameters for multi-track joint coding

  ------- ------ ---------------- --------- ----------------- -------------- ------- ------------ ------ ------- -------
  pulse   bits   Codebook space   Hi Bit    effective ratio   re-back bits                                       
                 Dec              Hex       value             bits           range                8bit   16bit   24bit
  1       5      32                                                                                              
  2       9      512              200       1                 1              2       1.00         0              
  3       13     5472             1560      A8                8              172     0.6875       3              
  4       16     44032            AC00      AC                9              345     0.67578125   1      9       
  5       19     285088           459A0     8B                8              140     0.546875            5       
  6       21     1549824          17A600    BD                8              190     0.7421875           3       
  7       23     7288544          6F36E0    DE                8              223     0.875               1       9
  8       25     30316544         1CE9800   1CE               9              463     0.904297                    8
  9       27     113461024        6C34720   6C3               11             1732    0.845704                    8
  ------- ------ ---------------- --------- ----------------- -------------- ------- ------------ ------ ------- -------

Multi-track joint coding processing is described as following:

Calculate an encoding index $\text{Ind}_{t}$ of each track, (subscript t
denotes the *t^th^* track), split $\text{Ind}_{t}$ into two split
indexes $\text{hi}_{t}$and $\text{track}_{}$ according to a set factor
$\text{SLF}_{t}$ combine a split index $\text{hi}_{t}$of each track to
generate a combined index $\text{hi}_{\text{SLPt}}$. The combined index
$\text{hi}_{\text{SLPt}}$ is split into recombined indexes $h_{t}$
according to the re-back bits length, and each recombined index $h_{t}$
and an un-combined split index $\text{track}_{}$ of a corresponding
track are respectively combined, then obtain the final recombined
index$\text{final}_{\text{index}_{t}}$ with fixed length 8,16 or 24
bits.

For 4 track in a sub-frame, the algebraic codebook
94bit(8777)\~108bit(9999) use 24 bits mode joint en/decoding，the
algebraic codebook 62bit(4444)\~92bit(7777) use 16 bits mode joint
en/decoding，the algebraic codebook 40bit(3222)\~61bit(4443) use 8 bits
mode joint en/decoding.

All the encoding steps are described as following:

1.  Get the parameter from table 43 according to the pulse number of
    > each track, include the index $\text{bits}_{t}$,
    > $\text{Hi}_{\text{Bit}_{\text{bits}_{t}}}$,
    > $\text{Hi}_{\text{Bit}_{\text{range}_{t}}}$,
    > $\text{re} - \text{back}_{\text{bits}_{t}}$. And get the 8/16/24
    > mode also according to the pulse number of all track.

2.  The index $\text{Ind}_{t}$ of $\text{track}_{t}$ is split into
    > $\text{hi}_{t}$ and $\text{track}_{}$, the $\text{SLF}_{t}$ is
    > $2^{(\text{bits}_{t} - \text{Hi}_{\text{Bit}_{\text{bits}_{t}}})}$,
    > and length of $\text{hi}_{t}$ is
    > $\text{Hi}_{\text{Bit}_{\text{bits}_{t}}}$, length of
    > $\text{track}_{}$ is
    > $(\text{bits}_{t} - \text{Hi}_{\text{Bit}_{\text{bits}_{t}}})$,

3.  Combine the$\text{hi}_{0}$ and $\text{hi}_{1}$ into
    > $\text{hi}_{\text{SLP}0}$ as following:

> $\text{hi}_{\text{SLP}0} = \text{hi}_{0} \times \text{Hi}_{\text{Bit}_{\text{range}_{1}}} + \text{hi}_{1}$
> (521)

4.  Split the low part of $\text{hi}_{\text{SLP}0}$and get the $h_{0}$,
    > and the length of $h_{0}$ is
    > $\text{re} - \text{back}_{\text{bits}_{0}}$,which get from table
    > 44 in step 1, the $h_{0}$ and $\text{track}_{}$ are combine into a
    > $\text{final}_{\text{index}_{0}}$with the length of 8,16 or 24
    > bits.

5.  The high part $\text{hi}_{\text{SLP}0H}$ of
    > $\text{hi}_{\text{SLP}0}$continue combining with the next
    > $\text{hi}_{2}$as following:

> $\text{hi}_{\text{SLP}1} = \text{hi}_{\text{SLP}0H} \times \text{Hi}_{\text{Bit}_{\text{range}_{2}}} + \text{hi}_{2}$
> (522)

6.  Split the low part of $\text{hi}_{\text{SLP}1}$and get the $h_{1}$,
    > and the length of $h_{1}$ is
    > $\text{re} - \text{back}_{\text{bits}_{1}}$,which get from table
    > 45 in step 1, the $h_{1}$ and $\text{track}_{}$ are combine into a
    > $\text{final}_{\text{index}_{1}}$with the length of 8,16 or 24
    > bits.

7.  The high part $\text{hi}_{\text{SLP}1H}$ of
    > $\text{hi}_{\text{SLP}1}$continue combining with the next
    > $\text{hi}_{3}$as following:

> $\text{hi}_{\text{SLP}2} = \text{hi}_{\text{SLP}1H} \times \text{Hi}_{\text{Bit}_{\text{range}_{3}}} + \text{hi}_{3}$
> (523)

8.  Split the low part of $\text{hi}_{\text{SLP}2}$and get the $h_{2}$,
    > and the length of $h_{2}$ is
    > $\text{re} - \text{back}_{\text{bits}_{2}}$,which get from table
    > 46 in step 1, the $h_{2}$ and $\text{track}_{}$ are combine into a
    > $\text{final}_{\text{index}_{2}}$with the length of 8,16 or 24
    > bits.

9.  The high part $\text{hi}_{\text{SLP}2H}$ of
    > $\text{hi}_{\text{SLP}1}$is split into two parts. The low part of
    > $\text{hi}_{\text{SLP}2H}$is used as the $h_{3}$, and the length
    > of $h_{3}$ is $\text{re} - \text{back}_{\text{bits}_{3}}$ which is
    > obtained from table 47 in step 1, the $h_{3}$ and
    > $\text{track}_{}$ are combined into a
    > $\text{final}_{\text{index}_{3}}$with the length of 8,16 or 24
    > bits.

10. Finally, the high part $\text{track}_{\text{hi}}$of
    > $\text{hi}_{\text{SLP}2H}$together
    > with$\text{final}_{\text{index}_{0}}$,
    > $\text{final}_{\text{index}_{1}}$,
    > $\text{final}_{\text{index}_{2}}$ and
    > $\text{final}_{\text{index}_{3}}$ are the outputs of multi-track
    > joint coding and stored into the stream in 16 bits unit.

![](media/image29.wmf){width="6.539583333333334in"
height="4.304166666666666in"}

Figure 19: Schematic diagram of 4-track joint coding

##### 5.2.3.1.5.9 The search criterion at lower bitrates {#the-search-criterion-at-lower-bitrates .H6}

The algebraic codebook is searched by minimizing the error between an
updated target signal and a scaled filtered algebraic codevector. The
updated target signal is given by

$x_{\text{11}}\left( n \right) = x\left( n \right) - g_{p}y\left( n \right),\ n = 0,\ldots,\text{63}$
(524)

where $y\left( n \right) = v\left( n \right) \ast h\left( n \right)$is
the filtered adaptive codevector and $g_{p}$ is the unquantized adaptive
codebook gain. Thus, the updated target signal is obtained by
subtracting the adaptive contribution from the initial target signal,
$x\left( n \right)$.

Let a matrix $H$be defined as a lower triangular Toeplitz convolution
matrix with the main diagonal $h\left( 0 \right)$and lower diagonals
$h\left( 1 \right),\ldots,h\left( \text{63} \right)$, and
$d = H^{T}x_{\text{11}}$ (also known as the backward filtered target
vector) be the correlation between the updated signal
$x_{\text{11}}\left( n \right)$and the impulse response
$h\left( n \right)$. Furthermore, let $\Phi = H^{T}H$ be the matrix of
correlations of $h\left( n \right)$. Here, $h\left( n \right)$is the
impulse response of the combination of the synthesis filter, the
weighting filter and the pre-filter $F\left( z \right)$which includes a
long-term filter.

The elements of the vector $d\left( n \right)$are computed by

$d\left( n \right) = \sum_{i = n}^{\text{63}}x_{\text{11}}\left( i \right)h\left( i - n \right),\ n = 0,\ldots,\text{63}$
(525)

and the elements of the symmetric matrix $\Phi$ are computed by

$\phi\left( i,j \right) = \sum_{n = j}^{\text{63}}h\left( n - i \right)h\left( n - j \right),\ i = 0,\ldots,\text{63},\ j = i,\ldots,\text{63}$
(526)

Let $c_{k}$ the *k*-th algebraic codevector. The algebraic codebook is
searched by maximizing the following criterion:

$Q_{k} = \frac{\left( x_{\text{11}}^{T}\text{Hc}_{k} \right)^{2}}{c_{k}^{T}H^{T}\text{Hc}_{k}} = \frac{\left( d^{T}c_{k} \right)^{2}}{c_{k}^{T}\text{Φc}_{k}} = \frac{\left( R_{k} \right)^{2}}{E_{k}}$
(527)

The vector $d\left( n \right)$and the matrix $\Phi$are usually computed
prior to the codebook search.

The algebraic structure of the codebooks allows for very fast search
procedures since the algebraic codevector, The algebraic structure of
the codebooks allows for very fast search procedures since the algebraic
codevector, $c_{k}\left( n \right)$, contains only a few non-zero
pulses. The correlation in the numerator of equation (528) is given by

$R = \sum_{i = 0}^{N_{p} - 1}s_{i}d\left( m_{i} \right)$ (529)

where $m_{i}$is the position of the *i*-th pulse, $s_{i}$is its
amplitude (sign), and $N_{p}$is the number of pulses. The energy in the
denominator of equation (530) is given by

$E = \sum_{i = 0}^{N_{p} - 1}{\phi\left( m_{i},m_{i} \right)} + 2\sum_{i = 0}^{N_{p} - 1}{\sum_{j = i + 1}^{N_{p} - 1}s_{i}}s_{j}\phi\left( m_{i},m_{j} \right)$
(531)

For saving the search load along with a better search result in the
12-bit codebook, the pulse amplitudes are predetermined based on a
high-pass filtered $d\left( n \right)$. The high-pass filter is a
three-tap MA (moving-average)-type filter, and its filter coefficients
are { -0.35, 1.0, -0.35 }. The sign of a pulse in a position $n$ is set
to negative when the high-pass filtered $d\left( n \right)$ is negative,
otherwise the sign is set to positive. To simplify the search,
$d\left( n \right)$ and $\Phi\left( k,h \right)$ are modified to
incorporate the predetermined signs.

##### 5.2.3.1.5.10 The search criterion at higher bitrates {#the-search-criterion-at-higher-bitrates .H6}

The following search criterion is used for bit rates at and above 16.4
kbps. It allows limiting the increase of complexity for high number of
pulses.

Let $N$ be the sub-frame length, and let matrices $H$ and $C$,
respectively, denote the $N \times N$ lower triangular Toeplitz
convolution matrix and the $\left( N + K - 1 \right) \times N$ full-size
convolution matrix, both defined for the filter $h(n)$. Here, $h(n)$ is
the length $K$ impulse response of the combination of the synthesis
filter, the weighting filter and the pre-filter $F(z)$which includes a
long-term filter. The target residual is $\text{q=H}^{- 1}x_{\text{11}}$
and $\Phi = C^{T}C$ is the autocorrelation matrix of filter $h(n)$.

The elements of the autocorrelation matrix can be calculated by

$\begin{matrix}
\Phi\left( k,h \right) = \varphi\left( k - h \right) = \underset{n = 0}{\overset{K}{\sum_{}^{}}}h\left( n \right)h\left( n - k + h \right)\text{,~~}n = 0,\ldots,N - 1 \\
\end{matrix}$ (532)

and the target residual by

$\begin{matrix}
q\left( n \right) = x_{\text{11}}\left( n \right) - \underset{k = 1}{\overset{n}{\sum_{}^{}}}h\left( k \right)q\left( n - k \right),n = 0,\ldots,N - 1\text{.} \\
\end{matrix}$ (533)

The final target is then $d = \text{Rq}$ which can be calculated by

$\begin{matrix}
d\left( n \right) = \underset{k = 0}{\overset{N - 1}{\sum_{}^{}}}\varphi\left( \left| n - k \right| \right)q\left( k \right),n = 0,\ldots,N - 1\text{.} \\
\end{matrix}$ (534)

Let $c_{k}$ be the *k*^th^ algebraic codevector. The algebraic codebook
is searched by maximizing the following criterion:

$Q_{k} = \frac{\left( q^{T}\text{Φc}_{k} \right)^{2}}{c_{k^{T}}\text{Φc}_{k}} = \frac{\left( d^{T}c_{k} \right)^{2}}{c_{k^{T}}\text{Φc}_{k}} = \frac{R_{k}^{2}}{E_{k}}$
(535)

##### 5.2.3.1.6 Combined algebraic codebook

In general the computational complexity of the algebraic codebook
increases with the codebook size. In order to keep the complexity
reasonable while providing better performance and scalability at high
EVS ACELP bit-rates, an efficient combined algebraic codebook structure
is employed. The combined algebraic codebook combines usually a
frequency-domain coding in a first stage followed by a time-domain ACELP
codebook in a second stage.

The frequency-domain coding of the first stage, denoted as a
pre-quantizer in figure 20, uses a Discrete Cosine Transform (DCT) as
the frequency representation and an Algebraic Vector Quantizer (AVQ)
(see subclause 5.2.3.1.6.9) to quantize the frequency-domain
coefficients of the DCT. The pre-quantizer parameters are set at the
encoder in such a way that the ACELP codebook (second stage of the
combined algebraic codebook) is applied to an excitation residual with
more regular spectral dynamics than the pitch residual.

Figure 21: Schematic diagram of the ACELP encoder using a combined
algebraic codebook in GC mode at high bit-rates

At the encoder, the first stage, or pre-quantizer, operates as follows.
In a given subframe (aligned to the subframe of the ACELP codebook in
the second stage) the excitation residual $q_{\text{in}}(n)$ after
applying the adaptive codebook is computed as

$q_{\text{in}}(n) = r(n) - g_{p} \cdot v(n)$ (536)

where *r*(*n*) is the target vector in residual domain. Further,
*v*(*n*) is the adaptive codevector and *g~p~* the adaptive codevector
gain.

The excitation residual $q_{\text{in}}(n)$ after applying the adaptive
codebook is de-emphasized with a filter $F_{p}(z)$. A difference
equation for such a de-emphasis filter $F_{p}(z)$ is given by

$q_{\text{in},d}(n) = q_{\text{in}}(n) + \alpha \cdot q_{\text{in},d}(n - 1)$
(537)

where $q_{\text{in},d}(n)$ is the de-emphasized residual and coefficient
$\alpha = 0\text{.}3$ controls the level of de-emphasis.

Further a DCT is applied to the de-emphasized excitation residual
$q_{\text{in},d}(n)$using a rectangular non-overlapping window.
Depending on the bit rate, all blocks or only some blocks of DCT
coefficients $Q_{\text{in},d}(k)$ usually corresponding to lower
frequencies are quantized using the AVQ encoder. The other (not
quantized) DCT coefficients $Q_{d}(k)$ are set to 0 (not quantized). To
obtain the excitation residual for the second (ACELP) stage of the
combined algebraic codebook, the quantized DCT coefficients $Q_{d}(k)$
are inverse transformed, and then a pre-emphasis filter
$\frac{1}{F_{p}}(z)$ is applied to obtain the time-domain contribution
from the pre-quantizer $q(n)$. The pre-emphasis filter has the inverse
transfer function of the de-emphasis filter $F_{p}(z)$.

##### 5.2.3.1.6.1 Quantization {#quantization .H6}

The AVQ encoder produces quantized transform-domain DCT coefficients
$Q_{d}(k)$. The indices of the quantized and coded DCT coefficients from
the AVQ encoder are transmitted as a pre-quantizer parameters to the
decoder.

In every sub-frame, a bit-budget allocated to the AVQ is composed as a
sum of a fixed bit-budget and a floating number of bits. Depending on
the used AVQ sub-quantizers of the encoder, the AVQ usually does not
consume all of the allocated bits, leaving a variable number of bits
available in each sub-frame. These bits are floating bits employed in
the following sub-frame. The floating number of bits is equal to 0 in
the first sub-frame and the floating bits resulting from the AVQ in the
last sub-frame in a given frame remain unused when coding WB signals or
are re-used in coding of upper band (see subclause 5.2.6.3).

##### 5.2.3.1.6.2 Computation of pre-quantizer gain {#computation-of-pre-quantizer-gain .H6}

Once the pre-quantizer contribution is computed, the pre-quantizer gain
is obtained as

$g_{q} = \frac{\sum_{k = 0}^{N - 1}{Q_{\text{in},d}(k)Q_{d}(k)}}{\sum_{k = 0}^{N - 1}{Q_{d}(k)Q_{d}(k)}}$
(538)

where $Q_{\text{in},d}(k)$ are the AVQ input frequency coefficients and
$Q_{d}(k)$ the AVQ output (quantized) frequency coefficients where
$k = 0,\text{.}\text{.}\text{.},K - 1$ is the transform-domain
coefficient index and $K = \text{64}$being the number of DCT transform
coefficients.

##### 5.2.3.1.6.3 Quantization of pre-quantizer gain {#quantization-of-pre-quantizer-gain .H6}

The pre-quantizer gain $g_{q}$ is quantized as follows. First, the gain
is normalized by the predicted innovation energy $E_{\text{pred}}$ as
follows:

![](media/image31.wmf){width="0.975in" height="0.5083333333333333in"}
(539)

where the predicted innovation energy $E_{\text{pred}}$ is obtained as
described in subclause 5.2.3.1.7.1.

Then the normalized gain $g_{q,\text{norm}}$ is quantized by a scalar
quantizer in a logarithmic domain and finally de-normalized resulting in
a quantized pre-quantizer gain. Specifically 6-bit scalar quantizer is
used whereby the quantization levels are uniformly distributed in the
log domain. The index of the quantized pre-quantizer gain is transmitted
as a pre-quantizer parameter to the decoder.

##### 5.2.3.1.6.4 Refinement of target vector {#refinement-of-target-vector .H6}

The pre-quantizer contribution $q(n)$ is used to refine the original
target vector for adaptive codebook search $x(n)$ as

$x_{\text{updt}}(n) = x(n) - g_{q} \cdot w(n)$, (537)

and to refine the adaptive codebook gain using equation (505) with
$x_{\text{updt}}(n)$ used instead of $x(n)$. When the pre-quantizer is
used, the computation of the target vector for algebraic codebook search
$x_{\text{11}}(n)$ is done using

$x_{\text{11}}(n) = x(n) - g_{q} \cdot w(n) - g_{p,\text{updt}} \cdot y(n)$
(538)

where $w(n)$ is the filtered pre-quantizer contribution, i.e. the
zero-state response of the weighted synthesis filter to the
pre-quantizer contribution $q(n)$, and $g_{p,\text{updt}}$ is the
refined adaptive codebook gain.

Similarly, the target vector in residual domain $r(n)$ is updated for
the algebraic codebook search (the second-stage of the combined
algebraic codebook) as

$r_{\text{updt}}(n) = r(n) - g_{q} \cdot q(n) - g_{p,\text{updt}} \cdot v(n)$.
(539)

##### 5.2.3.1.6.5 Combined algebraic codebook in GC mode {#combined-algebraic-codebook-in-gc-mode .H6}

In the EVS codec, the combined algebraic codebook structure as from
figure 21 is used at bit-rates of 32 kbps and 64 kbps. In both cases the
algebraic codebook search uses 36-bit codebooks and the rest of the
bit-budget is employed by the AVQ to quantize the pre-quantizer
coefficients.

At 32 kbps, the available fixed bit-budget for the AVQ (116, 115, 115,
115, 155 bits for every of five subframes) is sometimes too low to
properly encode all input signal frames. Consequently in GC mode at
32kbps, the DCT and iDCT stages of pre-quantizer computation are omitted
when the input signal is not classified as a harmonic one. The
classification is based on a harmonicity counter
$\text{harm}_{\text{acelp}}$ updated every frame in the pre-processing
module. If in a given frame the harmonicity counter
$\text{harm}_{\text{acelp}} \leq 2$ the frame is classified as
non-harmonic and the AVQ is applied directly on the time-domain signal
$q_{\text{in},d}(n)$ and similarly producing directly the time-domain
signal $q_{d}(n)$ in figure 21.

##### 5.2.3.1.6.6 Combined algebraic codebook in TC mode {#combined-algebraic-codebook-in-tc-mode .H6}

The combined algebraic codebook structure is used also in TC mode at
32kbps and 64kbps. In this mode the algebraic codebook from figure 21 is
replaced by glottal shape codebook but the structure of the
pre-quantizer remains the same as in the GC mode. In TC mode \@32kbps,
the DCT and iDCT stages of the pre-quantizer are always employed.

Figure 22: Schematic diagram of the ACELP encoder using a combined
algebraic codebook in IC mode at high bit-rates

##### 5.2.3.1.6.7 Combined algebraic codebook in IC mode {#combined-algebraic-codebook-in-ic-mode .H6}

Depending on the input signal characteristics, the ACELP encoder using a
combined algebraic codebook from figure 21 is further adaptively
changed. Specifically in coding of inactive speech segments, the order
of the combined algebraic codebook stages is changed. I.e. the modified
combined algebraic codebook combines a time-domain ACELP codebook in a
first stage followed by a frequency-domain de-quantizer coding in a
second stage as shown in figure 22. The first stage algebraic codebook
employs very small codebooks, specifically 12 bits per subframe.

At the encoder, the de-quantizer in IC mode operates as follows. In a
given subframe, the target signal $x_{3}(n)$ after subtracting the
scaled filtered adaptive excitation and the scaled filtered algebraic
excitation is computed as

$x_{3}(n) = x(n) - g_{p} \cdot y(n) - g_{c} \cdot z(n)$. (540)

The target signal in speech domain $x_{3}(n)$ is filtered through the
inverse of the weighted synthesis filter with zero states resulting in
the target in residual domain $u_{\text{in}}(n)$.

Similarly to the combined algebraic codebook in GC mode, the signal
$u_{\text{in}}(n)$ is first de-emphasized with a filter $F_{p}(z)$ to
enhance the low frequencies. A DCT is applied to the de-emphasized
signal $u_{\text{in},d}(n)$ using rectangular non-overlapping window.
Usually all blocks of DCT coefficients $U_{\text{in},d}(k)$ are
quantized using the AVQ encoder. The quantized DCT coefficients
$U_{d}(k)$ in some bands can be however set to zero.

The quantized DCT coefficients $U_{d}(k)$ are further inverse
transformed using iDCT, and then a pre-emphasis filter
$\frac{1}{F_{p}}(z)$ is applied to obtain the time-domain contribution
from the frequency-domain quantizer $u(n)$ where the pre-emphasis filter
has the inverse transfer function of the de-emphasis filter $F_{p}(z)$.

##### 5.2.3.1.6.8 Computation and quantization of de-quantizer gain {#computation-and-quantization-of-de-quantizer-gain .H6}

Once the de-quantizer contribution is computed, the de-quantizer gain is
obtained as

$g_{q} = \frac{\sum_{k = 0}^{N - 1}{U_{\text{in},d}(k)U_{d}(k)}}{\sum_{k = 0}^{N - 1}{U_{d}(k)U_{d}(k)}}$
(541)

where $U_{\text{in},d}(k)$ are the AVQ input transform-domain
coefficients and $U_{d}(k)$ are the AVQ output (quantized)
transform-domain coefficients.

The de-quantizer gain $g_{q}$ is quantized using the normalization by
the algebraic codebook gain $g_{c}$. Specifically a 6-bit scalar
quantizer is used whereby the quantization levels are uniformly
distributed in the linear domain. The indice of the quantized
de-quantizer gain $g_{q}$ is transmitted as a de-quantizer parameter to
the decoder.

When coding the inactive signal segments the adaptive codebook
excitation contribution is limited to avoid a strong periodicity in the
synthesis. In practice a limiter is applied in the adaptive codebook
search to constrain the adaptive codebook gain by
$0 \leq g_{p} \leq 0\text{.}\text{65}$.

##### 5.2.3.1.6.9 AVQ quantization with split multi-rate lattice VQ {#avq-quantization-with-split-multi-rate-lattice-vq .H6}

Prior to the AVQ quantization, the time domain or transform-domain 64
coefficients, here denoted as $S^{'}(k)$, are split into 8 consecutive
sub‑bands of 8 coefficients each. The sub-bands are quantized with an
8-dimensional multi-rate algebraic vector quantizer. The AVQ codebooks
are subsets of the Gosset lattice, referred to as the *RE~8~* lattice.

##### 5.2.3.1.6.9.1 Multi-rate AVQ with the Gosset Lattice *RE*~8~ {#multi-rate-avq-with-the-gosset-lattice-re8 .H6}

##### 5.2.3.1.6.9.1.1 Gosset Lattice *RE*~8~ {#gosset-lattice-re8 .H6}

*The Gosset lattice RE~8~* is defined as the following union:

$\text{RE}_{8} = 2D_{8} \cup \left\{ 2D_{8} + \left( 1,1,1,1,1,1,1,1 \right) \right\}$
(542)

where $D_{8}$ is the 8-dimensional lattice composed of all points with
integers components with the constraint that the sum of the 8 components
is even. The lattice $2D_{8}$ is simply the $D_{8}$ lattice scaled by 2.
This implies that the sum of the components of a lattice point in
$2D_{8}$ is an integer multiple of 4. Therefore, the 8 components of a
$\text{RE}_{8}$ lattice point have the same parity (either all even or
all odd) and their sum is a multiple of 4.

All points in the lattice *RE~8~* lie on concentric spheres of radius
$\sqrt{8n_{j}}$, $n_{j}$ being the codebook number in sub-band $j$. Each
lattice point on a given sphere can be generated by permuting the
coordinates of reference points called "*leaders".* There are very few
leaders on a sphere compared to the total number of lattice points which
lie on the sphere.

##### 5.2.3.1.6.9.1.2 Multi-rate codebooks in Gosset Lattice *RE8* {#multi-rate-codebooks-in-gosset-lattice-re8 .H6}

To form a vector codebook at a given rate, only lattice points inside a
sphere in 8 dimensions of a given radius are taken. Codebooks of
different bit rates can be constructed by including only spheres up to a
given radius. Multi-rate codebooks are formed by taking subsets of
lattice points inside spheres of different radii.

##### 5.2.3.1.6.9.1.2.1 Base codebooks {#base-codebooks .H6}

First, base codebooks are designed. A base codebook contains all lattice
points from a given set of spheres up to a number $n_{j}$. Four base
codebooks , noted $Q_{0}$, $Q_{2}$, $Q_{3}$, and $Q_{4}$, are used.
There are 36 non-null absolute leaders plus the zero leader (the
origin): Table 48 gives the list of these leaders and indicates to which
codebook a leader belongs. $Q_{0}$, $Q_{2}$, $Q_{3}$, and $Q_{4}$ are
constructed with respectively 0, 8, 12, and 16 bits. Hence codebook
$Q_{n_{j}}$ requires $4n_{j}$ bits to index any point in that codebook.

##### 5.2.3.1.6.9.1.2.2 Voronoi extensions {#voronoi-extensions .H6}

From a base codebook $C_{\text{AVQ}}$ (i.e. a codebook containing all
lattice points from a given set of spheres up to a number $n_{j}$), an
extended codebook can be generated by multiplying the elements of
$C_{\text{AVQ}}$ by a factor $M_{j}^{v}$, and adding a second-stage
codebook called the *Voronoi extension*. This construction is given by

$c_{j}\  = \ M_{j}^{v} \cdot \ z_{j} + v_{j}$ (543)

where $M_{j}^{v}$ is the scaling factor, $z_{j}$ is a point in a base
codebook $C_{\text{AVQ}}$ and $v_{j}$ is a point in the Voronoi
extension. The extension is computed in such a way that any point
$c_{j}$ from equation (544) is also a lattice point in $\text{RE}_{8}$.
The scaling factor $M_{j}^{v}$ is a power of 2
($M_{j}^{v} = 2^{r_{j}^{v}}$), where $r_{j}^{v}$ is called the Voronoi
extension order.

Such extended codebooks include lattice points that extend further out
from the origin than the base codebook. When a given lattice point
$c_{j}$ is not included in a base codebook $C_{\text{AVQ}}$ ($Q_{0}$,
$Q_{2}$, $Q_{3}$ or $Q_{4}$), the so-called Voronoi extension is
applied, using the $Q_{3}$ or $Q_{4}$ base codebook part.

Giving the available bit-budget in particular layers, the maximum
Voronoi extension order is $r_{j}^{v} = 2$. Therefore, for $Q_{3}$ or
$Q_{4}$, two extension orders are used: $r_{j}^{v} = 1\text{\ or\ }2$
($M_{j}^{v} = 2\text{\ or\ 4}$).

*When* $r_{j}^{v} = 0$, there is no Voronoi extension, and only a base
codebook is used.

##### 5.2.3.1.6.9.1.2.3 Codebook rates {#codebook-rates .H6}

There are 8 codebooks: the first 4 are base codebooks without Voronoi
extension and the last four with Voronoi extension. The codebook number
$n_{j}$ is encoded as a unary code with $n_{j}$ \"1\" bits and a
terminating \"0\". Table 49 gives for each of the 8 codebooks, its base
codebook, its Voronoi extension order ($r_{j}^{v} = 0$ indicates that
there is not Voronoi extension), and its unary code.

Table 50: Multi-rate *codebooks in RE~8~ lattice*

  ---------------- --------------- ------------------------------------- ------------------------
  Codebook\        Base Codebook   Voronoi extension order $r_{j}^{v}$   Unary code for $n_{j}$
  number $n_{j}$                                                         

  0                *Q*~0~          0                                     0

  2                *Q~2~*          0                                     10

  3                *Q*~3~          0                                     110

  4                *Q*~4~          0                                     1110

  5                *Q*~3~          1                                     11110

  6                *Q*~4~          1                                     111110

  7                *Q*~3~          2                                     1111110

  8                *Q*~4~          2                                     11111110
  ---------------- --------------- ------------------------------------- ------------------------

For the base codebook $Q_{0}$, ($n_{j} = 0$), there is only one point in
the codebook and 1 bit is used to transmit the unary code corresponding
to $n_{j}$.

For the other three base codebooks $Q_{n_{j}}$
($n_{j} = 2,3,\text{\ or\ }4$) without Voronoi extension:

-   $n_{j}$ bits are used to transmit the unary code corresponding to
    $n_{j}$*~,~*,

-   $4n_{j}$ bits are required to index a point in $Q_{n_{j}}$

-   thus $5n_{j}$ bits are used in total.

For codebooks with Voronoi extension ($n_{j} > 4$):

-   $n_{j}$ bits are used to transmit the unary code corresponding to
    the base codebook number $Q_{3}$ (respectively $Q_{4}$) if $n_{j}$
    is even (respectively odd) and the Voronoi extension order
    $r_{j}^{v}$ is 1 if $n_{j} < 7$, or 2 otherwise),

-   12 bits (respectively 16 bits) are required to index the point
    $z_{j}$ in the base codebook $Q_{3}$ (respectively $Q_{4}$)

-   $8r_{j}^{v}$ bits are required to index the 8-dimensional point
    $v_{j}$ in the Voronoi extension of order $r_{j}^{v}$

-   thus, $5n_{j}$ bits are used in total.

In the codebook number encoding, a simple bit overflow check is
performed: in case when the last AVQ coded sub-band of the spectrum
$S^{'}(k)$ is quantized, $n_{j} > 0$ and only $5n_{j} - 1$ bits are
available for the quantization, the terminating \"0\" in the codebook
number coding is not encoded. At the decoder, the same bit overflow
check enables the right decoding of the codebook number in this
sub-band.

##### 5.2.3.1.6.9.2 Quantization with *RE*~8~ lattice {#quantization-with-re8-lattice .H6}

In lattice quantization, the operation of finding the nearest neighbour
of the input spectrum $S^{'}(k)$ among all codebook points is reduced to
a few simple operations, involving rounding the components of spectrum
$S^{'}(k)$ and verifying a few constraints. Hence, no exhaustive search
is carried out as in stochastic quantization, which uses stored tables.
Once the best lattice codebook point is determined, further calculations
are also necessary to compute the index that will be sent to the
decoder. The larger the components of the input spectrum $S^{'}(k)$, the
more bits will be required to encode the index of its nearest neighbour
in the lattice codebook. Hence, to remain within a pre-defined
bit-budget, a gain-shape approach has to be used, where the input
spectrum is first scaled down by the AVQ gain, then each 8-dimensional
block of spectrum coefficients is quantized in the lattice and finally
scaled up again to produce the quantized spectrum.

5.2.3.1.6.9.2.1 AVQ gain estimation

Prior to the quantization (nearest neighbour search and indexation of
the nearest neighbour), the input spectrum has to be scaled down to
ensure that the total bit consumption will remain within the available
bit-budget.

A first estimation of the total bit-budget $\text{nbits}$ without
scaling (i.e. with an AVQ gain equals to 1) is performed:

$\text{nbits} = \sum_{j}^{}R_{j}$ (545)

where $R_{j}$ is a first estimate of the bit budget to encode the
sub-band $j$ given by:

$R_{j} = 5\text{log}_{2}\left( \frac{E_{j}}{2} \right)$ (546)

with $E_{i}$ being the energy (with a lower limit set to 2) of each
sub-band $S^{'}(8j)$:

$E_{j} = \text{max}\left( 2,\sum_{i = 0}^{7}\left\lbrack S^{'}(8j + i) \right\rbrack^{2} \right)$
(547)

This gain estimation is performed in an iterative procedure described
below.

Let NB\_BITS be the number of bits available for the quantization
process and NB\_SBANDS the number of 8-dimensional sub-bands to be
quantized:

+-----------------------------------------------------------------+---+
| *Initialization:*                                               |   |
|                                                                 |   |
| > *fac* = 128,                                                  |   |
| >                                                               |   |
| > *offset* = 0,                                                 |   |
| >                                                               |   |
| > *nbits~max~* = 0.95 (NB\_BITS -- *NB\_SBANDS*)                |   |
|                                                                 |   |
| for *i *= 1:10                                                  |   |
|                                                                 |   |
| *offset* = *offset* + *fac*                                     |   |
|                                                                 |   |
| $\text{nbits} = \sum_{j = 1}^{\text{                            |   |
| NB\_SBANDS}}{\text{max}\left( 0,R_{j} - \text{offset} \right)}$ |   |
|                                                                 |   |
| if *nbits* ≤ *nbits~max~*, then                                 |   |
|                                                                 |   |
| *offset* = *offset* -- *fac*                                    |   |
|                                                                 |   |
| *fac = fac / 2*                                                 |   |
+-----------------------------------------------------------------+---+

After the 10th iteration, the AVQ gain is equal to
$\text{10}\text{exp}\left( 0\text{.}1\text{offset} \cdot \text{log}_{\text{10}}\left( 2 \right) \right)$
and is used to obtain the scaled spectrum $S_{\text{norm}}^{'}(k)$:

$S_{\text{norm}}^{'}\left( k \right) = S^{'}\left( k \right)/\left\lbrack \text{10}\text{exp}\left( 0\text{.}1 \cdot \text{offset} \cdot \text{log}_{\text{10}}\left( 2 \right) \right) \right\rbrack$
(548)

##### 5.2.3.1.6.9.2.2 Nearest neighbour search {#nearest-neighbour-search .H6}

The search of the nearest neighbour in the lattice *RE~8~* is equivalent
to searching for the nearest neighbour in the lattice $2D_{8}$ and for
the nearest neighbour in the lattice $2D_{8} + (1,1,1,1,1,1,1,1)$, and
finally selecting among those two lattice points the closest to
$S_{\text{norm}}^{'}(8j)$) as its quantized version ${\hat{S}}^{'}(8j)$.

Based on the definition of $\text{RE}_{8}$*, the following fast
algorithm is used to search* the nearest neighbour of an 8-dimensional
sub-band $S_{\text{norm}}^{'}(8j)$ among all lattice points in
$\text{RE}_{8}$:

+-----------------------------------------------------------------+---+
| Search of the nearest neighbour **y**~1*j*~ in $2D_{8}$ of      |   |
| $S_{\text{norm}}^{'}(8j)$:                                      |   |
|                                                                 |   |
| Compute $z_{j} = 0\text{.}5 \cdot S_{\text{norm}}^{'}(8j)$.     |   |
|                                                                 |   |
| Round each component of $z_{j}$ to the nearest integer to       |   |
| generate $z_{j}^{'}$.                                           |   |
|                                                                 |   |
| Compute $y_{1j} = 2z_{j}^{'}$.                                  |   |
|                                                                 |   |
| Calculate the sum $S$ of the 8 components of $y_{1j}$.          |   |
|                                                                 |   |
| > if $S$ is not an integer multiple of 4, then modify its       |   |
| > $I^{\text{th}}$ component as follows:                         |   |
| >                                                               |   |
| > *where*                                                       |   |
| > $I = \text{arg}\left( \text{                                  |   |
| max}\left( \left| z_{j}(i) - y_{1j}(i) \right| \right) \right)$ |   |
|                                                                 |   |
| Search of the nearest neighbour $y_{2j}$ in                     |   |
| $2D_{8} + (1,1,1,1,1,1,1,1)$ of $S_{\text{norm}}^{'}(8j)$:      |   |
|                                                                 |   |
| Compute                                                         |   |
| $z_{j} = 0\text                                                 |   |
| {.}5 \cdot \left( S_{\text{norm}}^{'}(8j) - 1\text{.}0 \right)$ |   |
| where $1\text{.}0$ denotes an 8-dimensional vector with all     |   |
| ones.                                                           |   |
|                                                                 |   |
| Round each component of $z_{j}$ to the nearest integer to       |   |
| generate $z_{j}^{'}$.                                           |   |
|                                                                 |   |
| Compute $y_{2j} = 2z_{j}^{'}$.                                  |   |
|                                                                 |   |
| Calculate the sum *S* of the 8 components of $y_{2j}$.          |   |
|                                                                 |   |
| > if *S* is not an integer multiple of 4 then modify its        |   |
| > *I^th^* component as follows:                                 |   |
| >                                                               |   |
| > *where*                                                       |   |
| > $I = \text{arg}\left( \text{                                  |   |
| max}\left( \left| z_{j}(i) - y_{2j}(i) \right| \right) \right)$ |   |
|                                                                 |   |
| Compute $y_{2j} = y_{2j} + 1\text{.}0$.                         |   |
|                                                                 |   |
| Select between $y_{1j}$ and $y_{2j}$ as the closest point       |   |
| ${\hat{S}}^{'}(8j)$ in $\text{RE}_{8}$ to                       |   |
| $S_{\text{norm}}^{'}(8j)$:                                      |   |
|                                                                 |   |
| where                                                           |   |
| $e_{1j} = \left( S_{\text{norm}}^{'}(8j) - y_{1j} \right)^{2}$  |   |
| and                                                             |   |
| $e_{2j} = \left( S_{\text{norm}}^{'}(8j) - y_{2j} \right)^{2}$. |   |
+-----------------------------------------------------------------+---+

##### 5.2.3.1.6.9.3 Indexation {#indexation .H6}

The quantized scaled sub-band ${\hat{S}}^{'}(8j)$ of
$S_{\text{norm}}^{'}(8j)$) is a point $c_{j}$ in a *RE~8~* lattice
codebook, an index for each $c_{j}$ has to be computed and later
inserted into the bitstream.

This index is actually composed of three parts:

1)  a codebook number $n_{j}$;

2)  a vector index $I_{j}$, which uniquely identifies a lattice vector
    in a base codebook $C_{\text{AVQ}}$;

3)  and if $n_{j} > 4$, an 8-dimensional Voronoi extension index
    $I_{j}^{v}$ that is used to extend the base codebook when the
    selected point in the lattice is not in a base codebook
    $C_{\text{AVQ}}$.

The calculation of an index for a given point $c_{j}$ in the
$\text{RE}_{8}$ lattice is performed as follows:

**First, it is verified whether** $c_{j}$ **is in a base codebook**
$C_{\text{AVQ}}$ **by identifying its sphere and its leader:**

-   if $c_{j}$ **is in a base codebook,** the index used to encode
    $c_{j}$ is thus the codebook number $n_{j}$ plus the index $I_{j}$
    of the lattice point $c_{j}$ in $Q_{n_{j}}$.

Otherwise, the parameters of the Voronoi extension (see equation (549))
have to determined: the scaling factor *M~v~,* the base codebook
$C_{\text{AVQ}}$ ($Q_{3}$ or $Q_{4}$), the point $z_{j}$ in this base
codebook, and the point $v_{j}$ in the Voronoi extension. Then, the
index used to encode is composed of the codebook number $n_{j}$
($n_{j} > 4$) plus the index $I_{j}$ of the lattice point $z_{j}$ in the
base codebook $C_{\text{AVQ}}$ ($Q_{3}$ or $Q_{4}$), and the index
$I_{j}^{v}$ of $v_{j}$*~.~ in the Voronoi* extension.

##### 5.2.3.1.6.9.3.1 Indexing a codebook number {#indexing-a-codebook-number .H6}

As explained in subclause 5.2.3.1.6.9.1.2.3 -- Codebook rates, the
codebook index $n_{j}$ is unary encoded with $n_{j}$ bits except for
$n_{j} = 0$ that is coded with one bit (see table 51).

##### 5.2.3.1.6.9.3.2 Indexing of codevector in base codebook {#indexing-of-codevector-in-base-codebook .H6}

The index *I~j~* indicates the rank of codevector $z_{j}$ in *j*-th
sub-band, i.e., the permutation to be applied to a specific leader to
obtain $z_{j}$. The index computation is done in several steps, as
follows:

1\) The input codevector $z_{j}$ is decomposed into a sign vector
**s**~0~ and an absolute vector **y**~0~ following a two‑path procedure.

2\) The sign vector is encoded, the associated index
$\text{bits}_{\text{sign}}(s_{0})$ and the number of non-zero components
in $z_{j}$ are obtained. More details are given in subsequent
subclauses.

3\) The absolute vector is encoded using a multi-level permutation-based
index encoding method, and the associated index *rank(***y**~0~*)* is
obtained.

4\) The absolute vector index $\text{rank}(y_{0})$ and the sign index
$\text{bits}_{\text{sign}}(s_{0})$ are added together in order to obtain
the input vector rank: $\text{rank}(z_{j})$.

$\text{rank}(z_{j}) = \text{rank}(y_{0}) \cdot 2^{\text{sign}_{\text{nb}}(s_{0})} + \text{bits}_{\text{sign}}(s_{0})$
(550)

5\) Finally, the offset $\text{lead}_{\text{offset}}(z_{j})$ is added to
the rank. The index $I_{j}$ is obtained by

$I_{j} = \text{lead}_{\text{offset}}(z_{j}) + \text{rank}(z_{j})$ (551)

The indexing of codevector in base codebook is done in two steps. First
the sign vector is encoded.

The number of bits required for encoding the sign vector elements is
equal to the number of non‑zero elements in the codevector. \"1\"
represents a negative sign and \"0\" a positive sign. As lattice
$\text{RE}_{8}$ quantization is used, the sum of all the elements in a
codevector is an integer multiple of 4. If there is any change of sign
in the non-zero element, the sum may not be a multiple of 4 anymore, in
that case, the last element sign in the sign vector will be omitted. For
example, the sign vector of the input vector (--1, --1, 1, 1, 1, 1, --1,
--1) in leader 1 (see table 52) has seven bits and its value is
0x1100001.

In the second step the absolute vector and its position vector is
encoded

The encoding method for the absolute vector works as follows. The
absolute vector is first decomposed into *ML~max~* levels. The
highest-level vector $L_{0}$ is the original absolute vector. The $n$
value for $L_{n}$ is initialized to zero. Then:

1\) First the intermediate absolute value vector of $L_{1}$ is obtained
by removing the most frequent element as given in the decomposition
order column of table 56 from the original absolute vector of$L_{0}$.
Sequentially the remaining elements are built into a new absolute vector
for$L_{1}$; it has a position order related to the level $L_{0}$
original absolute vector. All position values$q_{i}$ of the remainder
elements are used to build a position vector
$(q_{0},q_{1},q_{2},\text{.}\text{.}\text{.},q_{m_{1} - 1})$of$L_{1}$.

The relationship between the original absolute vector of$L_{0}$and the
new absolute vector of$L_{1}$is that: the original absolute vector
of$L_{0}$is the upper-level vector of the new absolute vector of$L_{1}$,
and the new absolute vector of$L_{1}$is the lower-level vector of the
original absolute vector of$L_{0}$.The relationship between any two
neighbour level absolute vector is the same. The detail relationship is
described as following:

![](media/image33.wmf){width="5.010416666666667in"
height="2.3430555555555554in"}

Figure 23: Example processing of first level for$K_{a} = \text{20}$.

2\) Then the position vector
$(q_{0},q_{1},q_{2},\text{.}\text{.}\text{.},q_{m_{1} - 1})$of the new
absolute vector of$L_{1}$ related to the original absolute vector
of$L_{0}$ is indexed based on a permutation and combination function,
the indexing result being called the middle index $I_{\text{mid},1}$.
For the new absolute vector in$L_{1}$, the position vector indexing is
computed as follows:

$I_{\text{mid},1} = C_{m_{0}}^{m_{1}} - C_{m_{0} - q_{0}}^{m_{1}} + \sum_{i = 1}^{i < m_{1}}\left( C_{m_{0} - q_{i - 1}}^{m_{1} - i} - C_{m_{0} - q_{i}}^{m_{1} - i} \right)$
(552)

$I_{\text{final}} = I_{\text{final}} \cdot C_{m_{0}}^{m_{1}} + I_{\text{mid},1}$
(553)

where $I_{\text{final}}$ is initialized to zero before the first step at
the beginning of the procedure, $m_{0}$ is the dimension of the original
absolute vector of$L_{0}$, $m_{1}$ is the dimension of the new absolute
vector of$L_{1}$.

If there is more than one type of element in the new absolute vector,
the new absolute vector, named the upper-level vector, will be encoded
using the multi-level permutation-based index encoding method as
following step:

3\) Increment the *n* value. At level $L_{n}$,
$0 < L_{n} < \text{ML}_{\text{max}}$, the intermediate absolute value
vector is obtained by removing the most frequent element as given in the
decomposition order column of table 53 from the upper-level vector.
Sequentially the remaining elements are built into a new absolute vector
for the current level; it has a position order related to the level
$L_{n - 1}$ absolute vector. All position values of the remainder
elements are used to build a position vector.

4\) The position vector of the current lower-level vector related to its
upper-level vector is indexed based on a permutation and combination
function, the indexing result being called the middle index
$I_{\text{mid},n}$. For the absolute vector in the current lower level,
the position vector indexing is computed as follows:

$I_{\text{mid},n} = C_{m_{n - 1}}^{m_{n}} - C_{m_{n - 1} - q_{0}}^{m_{n}} + \sum_{i = 1}^{i < m_{n}}\left( C_{m_{n - 1} - q_{i - 1}}^{m_{n} - i} - C_{m_{n - 1} - q_{i}}^{m_{n} - i} \right)$
(554)

$I_{\text{final}} = I_{\text{final}} \cdot C_{m_{n - 1}}^{m_{n}} + I_{\text{mid},n}$
(555)

The elements $q_{0},q_{1},q_{2}\text{.}\text{.}\text{.}$ are the element
values in the $L_{n}$ level position vector ranged from left to right
according to their level, $m_{n - 1}$ is the dimension of the
upper-level absolute vector, $m_{n}$ is the dimension of the
current-level absolute vector, $C_{q}^{m}$ represents the permutation
and combination formula $C_{q}^{m} = \frac{q!}{m!(p - q)!}$,
$q,m = \left\{ 1,2,3,4,8 \right\}$, and $q > m$. All the values
for$C_{q}^{m}$can be stored in a simple table in order to avoid
calculation of factorials. The $L_{n - 1}$ level final-index,
$I_{\text{final}}$, is multiplied by the possible index value number,
$C_{m_{n - 1}}^{m_{n}}$, in the current level and is added to the index,
$I_{\text{mid},n}$, in the current level, to obtain the final index,
$I_{\text{final}}$, of the current level.

5\) Repeat steps 3 and 4 until there is only one type of element left in
the current absolute vector. The $I_{\text{final}}$ for the lowest level
is the rank of the absolute vector called $\text{rank}(y_{0})$. Table 54
is a sample extracted from the 36 leader table case. The leaders are
indexed by $K_{a}$. The decomposition order corresponds to the level
order. The decomposition order column gives the order in which the
element will be removed from the higher level. The last column gives the
three class parameters, the first one is the number of sign bits,
$S_{n}$, the second one is the number of decomposition levels and equals
the number of element types in the leader, $V_{c}$, from the third one
to the last one they represent the absolute vector dimension in each
lower level except the highest level, $m_{1},m_{2},m_{3}$ (note that the
dimension for the highest level is eight, but is not listed in table
55).

Table 56: List of leaders in base codebooks $Q_{n_{j}}$ with their
decomposition order and set parameter of multi-level permutation-based
encoding

  ---- --------------------- --------------------- --------------------------------------- ------ ------ ------ ------
       Leader                Decomposition order   $S_{n}$, $V_{c}$, $m_{1},m_{2},m_{3}$   Q~0~   Q~2~   Q~3~   Q~4~
       {0,0,0,0,0,0,0,0}                                                                   X                    
  0    {1,1,1,1,1,1,1,1}     {1}                   {7,1}                                          X      X      
  1    {2,2,0,0,0,0,0,0}     {0,2}                 {2,2,2}                                        X      X      
  2    {2,2,2,2,0,0,0,0}     {0,2}                 {4,2,4}                                               X      
  3    {3,1,1,1,1,1,1,1}     {1,3}                 {7,2,1}                                               X      
  4    {4,0,0,0,0,0,0,0}     {0,4}                 {1,2,1}                                        X      X      
  5    {2,2,2,2,2,2,0,0}     {2,0}                 {6,2,2}                                                      X
  6    {3,3,1,1,1,1,1,1}     {1,3}                 {7,2,2}                                                      X
  7    {4,2,2,0,0,0,0,0}     {0,2,4}               {3,3,3,1}                                             X      
  8    {2,2,2,2,2,2,2,2}     {2}                   {8,1}                                                        X
  9    {3,3,3,1,1,1,1,1}     {1,3}                 {7,2,3}                                                      X
  10   {4,2,2,2,2,0,0,0}     {2,0,4}               {5,3,4,1}                                                    X
  11   {4,4,0,0,0,0,0,0}     {0,4}                 {2,2,2}                                               X      
  12   {5,1,1,1,1,1,1,1}     {1,5}                 {7,2,1}                                                      X
  13   {3,3,3,3,1,1,1,1}     {1,3}                 {7,2,4}                                                      X
  14   {4,2,2,2,2,2,2,0}     {2,0,4}               {7,3,2,1}                                                    X
  15   {4,4,2,2,0,0,0,0}     {0,2,4}               {4,3,4,2}                                                    X
  16   {5,3,1,1,1,1,1,1}     {1,3,5}               {7,3,2,1}                                                    X
  17   { 6,2,0,0,0,0,0,0}    {0,2,6}               {2,3,2,1}                                             X      
  18   { 4,4,4,0,0,0,0,0}    {0,4}                 {3,2,3}                                                      X
  19   { 6,2,2,2,0,0,0,0}    {0,2,6}               {4,3,4,1}                                                    X
  20   { 6,4,2,0,0,0,0,0}    {0,2,4,6}             {3,4,3,2,1}                                                  X
  21   { 7,1,1,1,1,1,1,1}    {1,7}                 {7,2,1}                                                      X
  22   { 8,0,0,0,0,0,0,0}    {0,8}                 {1,2,1}                                                      X
  23   {6,6,0,0,0,0,0,0}     {0,6}                 {2,2,2}                                                      X
  24   {8,2,2,0,0,0,0,0}     {0,2,8}               {3,3,3,1}                                                    X
  25   {8,4,0,0,0,0,0,0}     {0,4, 8}              {2,3,2,1}                                                    X
  26   {9,1,1,1,1,1,1,1}     {1,9}                 {7,2,1}                                                      X
  27   {10,2,0,0,0,0,0,0}    {0,2,10}              {2,3,2,1}                                                    X
  28   {8,8,0,0,0,0,0,0}     {0,8}                 {2,2,2}                                                      X
  29   {10,6,0,0,0,0,0,0}    {0,6,10}              {2,3,2,1}                                                    X
  30   {12,0,0,0,0,0,0,0}    {0,12}                {1,2,1}                                                      X
  31   {12,4,0,0,0,0,0,0}    {0,4,12}              {2,3,2,1}                                                    X
  32   {10,10,0,0,0,0,0,0}   {0,10}                {2,2,2}                                                      X
  33   {14,2,0,0,0,0,0,0}    {0,2,14}              {2,3,2,1}                                                    X
  34   {12,8,0,0,0,0,0,0}    {0,8,12}              {2,3,2,1}                                                    X
  35   {16,0,0,0,0,0,0,0}    {0,16}                {1,2,1}                                                      X
  ---- --------------------- --------------------- --------------------------------------- ------ ------ ------ ------

The last value of the decomposition order for the leader
$K_{a} = \text{20}$ is stored separately because this leader is the only
one with 4 different values, the second dimension of the decomposition
order being thus reduced from 4 to 3.

Figure 24 gives an encoding example for the leader $K_{a} = \text{20}$.

  --
  --

Figure 32: Example processing for
![](media/image35.wmf){width="0.5138888888888888in"
height="0.20833333333333334in"}.

For example, in case the input vector is {0,--2,0,0,4,0,6,0}, the
absolute input vector will be {0,2,0,0,4,0,6,0}, its associated leader
can be found for $K_{a} = \text{20}$*K~a~*. The set of decomposition
order is {0,2,4,6}. For the highest level *L~0~*, element \"0\" is
removed first from the absolute vector. The first level $L_{1}$ absolute
vector is {2,4,6}, its position vector is {1,4,6}. The second element
which will be removed is \"2\", the second level $L_{2}$ absolute vector
is {4,6}, its position vector is {1,2}. The third element which will be
removed is \"4\", the third level $L_{3}$ absolute vector is {6}, its
position vector is {1}.

The absolute vectors that have only two different values, out of which
the most frequent is zero, are treated separately in a less complex
procedure combining the encoding of the position vector with the sign
encoding. These vectors have generally higher probability of occurrence.
Example of such vectors are those derived for instance from the leaders:
(2,2,0,0,0,0,0,0), (2,2,2,2,0,0,0,0). For these vectors there is a
single level for the creation of the index and the first level remaining
elements are the non-null components which are the significant elements
for the sign encoding. The determination of the remaining elements and
the creation of the sign index can be done thus in a single loop.

##### 5.2.3.1.6.9.4 Voronoi extension determination and indexing {#voronoi-extension-determination-and-indexing .H6}

If the nearest neighbour $c_{j}$ is not in the base codebook, then the
Voronoi extension has to be determined through the following steps.

\(a\) Set the Voronoi extension order $r_{j}^{v} = 1$ and the scaling
factor $M_{j}^{v} = 2^{r_{j}^{v}}$.

\(b\) Compute the Voronoi index $I_{j}^{v}$ of the lattice point $c_{j}$
that depends on the extension order $r_{j}^{v}$ and the scaling factor
$M_{j}^{v}$. The Voronoi index is computed via component-wise modulo
operations such that $I_{j}^{v}$ depends only on the relative position
of $c_{j}$ in a scaled and translated Voronoi region:

$I_{j}^{v} = \text{mod}_{M_{j}^{v}}(c_{j}G^{- 1})$ (556)

> where $G$ is the $\text{RE}_{8}$ generator matrix. Hence, the Voronoi
> index $I_{j}^{v}$ is a vector of integers with each component in
> $\lbrack 0,M_{j}^{v} - 1\rbrack$.

\(c\) Compute the Voronoi codevector $v_{j}$ from the Voronoi index
$I_{j}^{v}$. The Voronoi codevector is obtained as

$v_{j} = y_{1j} - M_{j}^{v} \cdot {\hat{y}}_{2j}$ (557)

where ${\hat{y}}_{2j}$ is the nearest neighbour of $y_{2j}$ in infinite
$\text{RE}_{8}$ (see subclause 5.2.3.1.6.9.2.2for search details) and
$y_{1j}$ and $y_{2j}$ are defined as

$y_{1j} = I_{j}^{v}\begin{bmatrix}
4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
2 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
2 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\
2 & 0 & 0 & 2 & 0 & 0 & 0 & 0 \\
2 & 0 & 0 & 0 & 2 & 0 & 0 & 0 \\
2 & 0 & 0 & 0 & 0 & 2 & 0 & 0 \\
2 & 0 & 0 & 0 & 0 & 0 & 2 & 0 \\
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
\end{bmatrix}$ (558)

and

$y_{2j} = \left( y_{1j} - \lbrack 2,0,0,0,0,0,0,0\rbrack \right)/M_{j}^{v}$
(559)

\(d\) Compute the difference vector $w_{j} = c_{j} - v_{j}$. This
difference vector $w_{j}$ always belongs to the scaled lattice
$M_{j}^{v} \cdot \text{RE}_{8}$. Compute $z_{j} = w_{j}/M_{j}^{v}$, i.e.
apply the inverse scaling to the difference vector $w_{j}$. The
codevector $z_{j}$ belongs to the lattice $\text{RE}_{8}$ since $w_{j}$
belongs to $M_{j}^{v} \cdot \text{RE}_{8}$ lattice.

\(e\) Verify whether $z_{j}$ is in the base codebook $C_{\text{AVQ}}$
*(i.e. in* $Q_{3}$ or $Q_{4}$*).*

> If $z_{j}$ is not in *C*, increment the extension order $r_{j}^{v}$ by
> 1, multiply the scaling factor $M_{j}^{v}$ by 2, and go back to
> sub-step (b).

Otherwise, if $z_{j}$ is in *C*, then the Voronoi extension order
$r_{j}^{v}$ has been found and the scaling factor
$M_{j}^{v} = 2^{r_{j}^{v}}$ is sufficiently large to encode the index of
$c_{j}$.

##### 5.2.3.1.6.9.3 Insertion of AVQ parameters into the bitstream {#insertion-of-avq-parameters-into-the-bitstream .H6}

The parameters of the AVQ in each sub-band *j* consist of the codebook
number $n_{j}$, the vector index in base codebook $I_{j}$ and the
8-dimensional Voronoi index $I_{j}^{v}$. The codebook numbers $n_{j}$
are in the set of integers {0, 2, 3, 4, 5, 6, 7, 8} and the size of its
unary code representation is $n_{j}$ bits with the exception of $Q_{0}$
that requires 1 bit and a possible overflow in the last AVQ coded
sub-band. The size of each index $I_{j}$ and $I_{j}^{v}$ is given by
4*n~j~* bits and $8r_{j}^{v}$ *bits, respectively*.

The AVQ parameters $n_{j}$, $I_{j}$, $I_{j}^{v}$, are written
sequentially in groups corresponding to the same sub‑band into the
corresponding bitstream as

$\lbrack(n_{0}I_{0}I_{0}^{v})(n_{1}I_{1}I_{1}^{v})\text{.}\text{.}\text{.}\rbrack$.
(560)

Note that if the lattice point in the block $j$ is in the base codebook
$C$, the Voronoi extension is not searched and consequently the index
$I_{j}^{v}$ is not written into the bitstream in this group.

The actual bit-budget needed to encode AVQ parameters in current frame
varies from sub-frame to sub-frame. The difference of bits between the
allocated bits and actually spent bits are unused bits that can be
employed in the subsequent sub-frame or high-rate higher band coding.

##### 5.2.3.1.7 Gain quantization

##### 5.2.3.1.7.1 Memory-less quantization of the gains {#memory-less-quantization-of-the-gains .H6}

The adaptive codebook gain (pitch gain) and the algebraic codebook gain
are quantized jointly in each subframe, using a 5-bit vector quantizer.
While the adaptive codebook gain is quantized directly, the algebraic
codebook gain is quantized indirectly, using a predicted energy of
algebraic codevector. Note that, in this case, the prediction does not
use any past information which limits the effect of frame-erasure
propagation.

First, energy of residual signal in dB is calculated in each subframe as

$E_{r}^{\left\lbrack i \right\rbrack} = \text{10}\text{log}\left( \frac{1}{\text{64}}\sum_{i = 0}^{\text{63}}{r^{2}\left( i \right)} \right),\ \text{for}\ i = 0,\ 1,\ 2,\ 3,$
(559)

where $i$denotes the subframe and $r\left( n \right)$is the residual
signal, defined in subclause 5.2.3.1.1. Then, average residual signal
energy is calculated for the whole frame as

${\overline{E}}_{r} = 0\text{.}\text{25}\sum_{i = 0}^{3}E_{r}^{\left\lbrack i \right\rbrack}$
(560)

which is further modified by subtracting an estimate of the adaptive
codebook contribution. That is

$E_{i} = {\overline{E}}_{r} - 0\text{.}5\left( C_{\text{norm}}^{\left\lbrack 0 \right\rbrack} + C_{\text{norm}}^{\left\lbrack 1 \right\rbrack} \right)$
(561)

where$C_{\text{norm}}^{\left\lbrack 0 \right\rbrack}$ and
$C_{\text{norm}}^{\left\lbrack 1 \right\rbrack}$, are as defined in
subclause 5.1.10.4, are the normalized correlations of the first and the
second half-frames, respectively. The result of equation (562), $E_{i}$,
serves as a prediction of the algebraic codevector energy and is
quantized with 3 bits once per frame. The quantized value of the
predicted algebraic codevector energy is defined as

$\begin{matrix}
k_{\text{ind}} = \underset{k = 0}{\overset{7}{\text{min}}}\left| E_{i} - E_{\text{book}}\left( k \right) \right| \\
{\hat{E}}_{i} = E_{\text{book}}\left( k_{\text{ind}} \right) \\
\end{matrix}$$$ (563)

where $E_{\text{book}}\left( k \right),\ k = 0,\ldots,2^{n}$is the n-bit
codebook for the predicted algebraic codevector energy and
$k_{\text{ind}}$is the index minimizing the criterion above. The bit
allocation $n$ is bit-rate and mode dependant and is given in Table 57

Table 47: Predictor energy codebook bit allocation

  ------------- ------ ------ ------ -------
  Rate (kbps)   VC     GC     TC     IC/UC
  7.2           n.a.   n.a.   4      n.a.
  8             n.a.   n.a.   4      n.a.
  9.6           3      3      n.a.   n.a.
  13.2          5      4      4      n.a.
  16.4          3      3      n.a.   3
  24.4          3      3      n.a.   3
  32            n.a.   5      5      5
  64            n.a.   5      5      5
  ------------- ------ ------ ------ -------

Now, let $E_{c}$denote the algebraic codebook excitation energy in dB in
a given subframe, which is given by

$E_{c} = \text{10}\text{log}\left( \frac{1}{\text{64}}\sum_{n = 0}^{\text{63}}{c^{2}\left( n \right)} \right)$
(564)

In the equation above, $c\left( n \right)$ is the filtered algebraic
codevector, found in subclause 5.2.3.1.5.

Using the predicted algebraic codevector energy and the calculated
algebraic codebook excitation energy, we may estimate the algebraic
codebook gain as

$g_{c}^{'} = \text{10}^{0\text{.}\text{05}\left( {\hat{E}}_{i} - E_{c} \right)}$
(565)

A correction factor between the true algebraic codebook gain, $g_{c}$,
and the estimated one, $g_{c}^{'}$, is given by

$\gamma = g_{c}g_{c}^{'}$ (566)

The pitch gain,$g_{p}$, and correction factor $\gamma$are jointly
vector-quantized using a n-bit codebook, where n is dependent on the
bit-rate and coding mode as shown in Table 48

Table 49: Gain codebook bit allocation per subframe

  ------------- ---------------- ---------------- -----------
  Rate (kbps)   VC               GC               UC/IC
  7.2           7/6/6/6          6/6/6/6          n.a.
  8             8/7/6/6          8/7/6/6          n.a.
  9.6           5/5/5/5          5/5/5/5          n.a.
  13.2          6/6/6/6          6/6/6/6          n.a.
  16.4          7/7/7/7/7        7/7/7/7/7        6/6/6/6/6
  24.4          7/7/7/7/7        7/7/7/7/7        6/6/6/6/6
  32            6/6/6/6/6        6/6/6/6/6        6/6/6/6/6
  64            12/12/12/12/12   12/12/12/12/12   6/6/6/6/6
  ------------- ---------------- ---------------- -----------

The gain codebook search is performed by minimizing a mean-squared
weighted error between the original and the reconstructed signal, which
is given by

$E = x^{T}x + g_{p}^{2}y^{T}y + g_{c}^{2}z^{T}z - 2g_{p}x^{T}y - 2g_{c}x^{T}z + 2g_{p}g_{c}y^{T}z$
(567)

where $x$is the target vector, $y$is the filtered adaptive codevector,
and $z$is the filtered algebraic codevector. The quantized value of the
pitch gain is denoted as${\hat{g}}_{p}$and the quantized value of the
algebraic codebook gain is denoted as
${\hat{g}}_{c} = \hat{\gamma}\ g_{c}^{'}$ , where $\hat{\gamma}$ is the
quantized value of the factor $\gamma$.

Furthermore, if pitch gain clipping is detected (as described in
subclause 5.2.3.1.4.2), the last 13 entries in the codebook are skipped
in the quantization procedure since the pitch gain in these entries is
higher than 1.

##### 5.2.3.1.7.2 Memory-less joint gain coding at lowest bit-rates {#memory-less-joint-gain-coding-at-lowest-bit-rates .H6}

For the lowest bitrates of 7.2 and 8.0 kbps, slightly different
memory-less joint gain coding scheme is used. This is due to the fact
that there are not enough bits to cover the dynamic range of the target
vector for algebraic search.

In the first subframe of the current frame, the estimated (predicted)
gain of algebraic codebook is given by

$g_{c0}^{\lbrack 0\rbrack} = \text{10}^{a_{0} + a_{1}\text{CT} - \text{log}_{\text{10}}(\sqrt{E_{c}})}$
(568)

where *CT* is a signal classification parameter (the coding mode),
selected for the current frame in the pre-processing part, and $E_{c}$
is the energy of the filtered algebraic codevector, calculated in
equation (569). The inner term inside the logarithm corresponds to the
gain of innovation vector. The constants *a~0~* and *a~1~* are found by
means of MSE minimization on a large signal database. The only parameter
in the equation above is the coding mode *CT* which is constant for all
subframes of the current frame. The superscript \[0\] denotes the first
subframe of the current frame. The estimation process for the first
subframe is schematically depicted in the figure below.

Figure 33: Schematic description of the calculation process of algebraic
gain in the first subframe

All subframes following the first subframes use slightly different
estimation scheme. The difference is in the fact that in these
subframes, the quantized gains of both the adaptive and the algebraic
codebook from previous subframe(s) are used as auxiliary estimation
parameters to increase the efficiency. The estimated value of the
algebraic codebook gain in *k*th subframe, *k*\>0 is given by

$g_{c0}^{\lbrack k\rbrack} = \text{10}^{b_{0} + b_{1}\text{CT} + \sum_{i = 1}^{k}{b_{i + 1}\text{log}_{\text{10}}(g_{c}^{\lbrack i - 1\rbrack})} + \sum_{i = 1}^{k}{b_{k + 1 + i}g_{p}^{\lbrack i - 1\rbrack}}}$
(570)

where *k*=1,2,3. Note, that the terms in the first and in the second sum
of the exponent, there are quantized gains of algebraic and adaptive
excitation of previous subframes, respectively. Note that the term
including the gain of innovation vector
$\text{log}_{\text{10}}(\sqrt{E_{c}})$ is not subtracted. The reason is
in the use of the quantized values of past algebraic codebook gains
which are already close enough to the optimal gain and thus it is not
necessary to subtract this gain again. The estimation constants
*b*~0~,...,*b*~2*k+*1~ are found again through MSE minimization on a
large signal database. The gain estimation process for the second and
the following subframes is schematically depicted in the figure below.

Figure 34: Schematic description of the calculation process of algebraic
gain in the following subframes

The gain quantization is done both at the encoder and at the decoder by
searching the gain codebook and evaluating the MMSE between the target
signal and the filtered adaptive codeword. In each subframe, the
codebook is searched completely, i.e. for *q*=0,..,*Q*-1 where *Q* is
the number of codebook entries. It is possible to limit the searching
range in case *ĝ~p~* is mandated to lie below certain threshold. To
allow reducing the search range, the codebook entries are sorted in
ascending order according to the value of *ĝ~p~*.

The gain quantization is performed by calculating the following MMSE
criterion for each codebook entry

$E = c_{0}{\hat{g}}_{p}^{2} + c_{1}{\hat{g}}_{p} + c_{2}\lbrack\hat{\gamma}g_{c0}\rbrack^{2} + c_{3}\hat{\gamma}g_{c0} + c_{4}{\hat{g}}_{p}\hat{\gamma}g_{c0} + c_{5}$
(571)

where the constants *c*~0~, *c*~1~, *c*~2,~ *c*~3,~ *c*~4~ and *c*~5~
are calculated as

$c_{0} = y^{T}y,\ c_{1} = x^{T}y,\ c_{2} = z^{T}z,\ c_{3} = x^{T}z,\ c_{4} = y^{T}z,\ c_{5} = x^{T}x$
(572)

in which *x*(*i*) is the target signal, *y*(*i*) is the filtered
adaptive excitation signal and *z*(*i*) is the filtered algebraic
excitation signal. The codevector leading to the lowest energy is chosen
as the winning codevector and its entries correspond to the quantized
values of *g~p~* and **.

Before the gain quantization process it is assumed that both the
filtered adaptive and innovation codewords are already known. The gain
quantization at the encoder is performed by searching the designed gain
codebook in the MMSE sense. Each entry in the gain codebook consists of
two values: the quantized gain of the adaptive part and the correction
factor $\gamma$ for the algebraic part of the excitation. The estimation
of the algebraic gain excitation is done beforehand and the resulting
*g~c~*~0~ is used to multiply the correction factor selected from the
codebook. In each subframe the gain codebook is searched completely,
i.e. for *q*=0,..,*Q*-1. It is possible to limit the search range if the
quantized gain of the adaptive part of the excitation is mandated to be
below certain threshold. To allow for reducing the search range, the
codebook entries are sorted in ascending order according to the value of
*g~p~*. The gain quantization process is schematically depicted in the
figure below.

Figure 35: Schematic diagram of the gain quantization process in the
encoder

The gain quantization is performed by minimizing the energy of the error
signal *e*(*i*) The error energy is given by

$E = e^{T}e = (x - g_{p}y - g_{c}z)^{T}(x - g_{p}y - g_{c}z)$ (573)

By replacing $g_{c}$ by $\text{γg}_{c0}$ we obtain

$E = c_{5} + g_{p}^{2}c_{0} - 2g_{p}c_{1} + \gamma^{2}g_{c0}^{2}c_{2} - 2\text{γg}_{c0}c_{3} + 2g_{p}\text{γg}_{c0}c_{4}$
(574)

The constants *c*~0~, *c*~1~, *c*~2,~ *c*~3,~ *c*~4~ and *c*~5~ and the
estimated gain $g_{c0}$ are computed before the search of the gain
codebook. The error energy *E* is calculated for each codebook entry.
The codevector \[$g_{p}$;$\gamma$\] leading to the lowest error energy
is selected as the winning codevector and its entries correspond to the
quantized values of *g~p~* and **. The quantized value of the fixed
codebook gain is then calculated as

$g_{c} = g_{c0}\text{.}\gamma$ (575)

In the decoder, the received index is used to retrieve the values of the
quantized gain of the adaptive excitation and the quantized correction
factor of the estimated gain of the algebraic excitation. The estimated
gain for the algebraic part of the excitation is done in the same way as
in the encoder.

##### 5.2.3.1.7.3 Scalar gain coding at highest bit-rates {#scalar-gain-coding-at-highest-bit-rates .H6}

At the bit-rate of 64kbps and at the last subframe of TC7 and TC~16~5
(see later in subclause 5.2.3.2.2), the adaptive codebook gain (pitch
gain) and the algebraic codebook gain are quantized using a scalar
quantizers. The adaptive codebook gain is quantized using a uniform
scalar quantizer according to MMSE criterion in the range between
\[0; 1.22\]. In contrast the quantized algebraic codebook gain is
obtained as a product of a correction factor $\gamma$ and the estimated
algebraic codebook gain $g_{c}^{'}$, see equation 566, where the
correction factor is quantized in log domain in the range between
\[0.02; 5.0\].

At 64 kbps, both the adaptive codebook gain and the algebraic codebook
gain are quantized by means of 6 bits each. In the last subframe of TC
configurations TC7 and TC~16~5, they are quantized by means of 6-8 bits
depending on the bit-rate.

5.2.3.1.8 Update of filter memories

An update of the states of the synthesis and weighting filters is needed
in order to compute the target signal in the next subframe.

After the two gains have been quantized, the excitation
signal,$u^{'}\left( n \right)$, in the present subframe is found by

$u^{'}\left( n \right) = {\hat{g}}_{p}\ v\left( n \right) + {\hat{g}}_{c}\ c\left( n \right),\ n = 0,\ldots,\text{63}$
(576)

where ${\hat{g}}_{p}$and${\hat{g}}_{c}$are the quantized adaptive and
algebraic codebook gains, respectively, $v\left( n \right)$is the
adaptive codevector (interpolated, low-pass filtered past excitation),
and $c\left( n \right)$is the algebraic codevector (including
pre-filtering). The states of the filters can be updated by filtering
the signal $r\left( n \right) - u^{'}\left( n \right)$ (difference
between the residual signal and the excitation signal) through the
filters $1/\hat{A}\left( z \right)$and
$A\left( z/\gamma_{1} \right)H_{\text{de} - \text{emph}}\left( z \right)$and
saving the states of the filters. This would require 3 stages of
filtering. A simpler approach, which requires only one filtering, is as
follows. The local synthesis signal (without excitation post-processing)
from layer $1,\ {\hat{s}}_{1}\left( n \right)$is computed by filtering
the excitation signal through$1/\hat{A}\left( z \right)$. The output of
the filter due to the input
$r\left( n \right) - u^{'}\left( n \right)$is equivalent
to$x_{1}\left( n \right) = s\left( n \right) - {\hat{s}}_{1}\left( n \right)$.
So, the states of the synthesis filter$1/\hat{A}\left( z \right)$are
given by $x_{1}\left( n \right),\ n = \text{48},\ldots,\text{63}$.

The updating of the states of the
filter$A\left( z/\gamma_{1} \right)H_{\text{de} - \text{emph}}\left( z \right)$can
be done by filtering the error signal $x_{1}\left( n \right)$through
this filter to find the perceptually weighted error
$x_{1,w}\left( n \right)$. However, the signal
$x_{1,w}\left( n \right)$can be equivalently found by

$x_{1,w}\left( n \right) = x\left( n \right) - {\hat{g}}_{p}\ y\left( n \right) - {\hat{g}}_{c}\ z\left( n \right)$
(577)

where $x\left( n \right)$is the adaptive codebook search target signal,
$y\left( n \right)$is the filtered adaptive codebook vector, and
$z\left( n \right)$is the filtered algebraic codebook vector. Since the
signals $x\left( n \right)$, $y\left( n \right)$, and
$z\left( n \right)$are available, the states of the weighting filter are
updated by computing $x_{1,w}\left( n \right)$as in equation 578 for
$n = \text{48},\ldots,\text{63}$. This saves two stages of filtering.

#### 5.2.3.2 Excitation coding in TC mode

The principle of excitation coding in TC mode is shown on a schematic
diagram in Figure 36. The individual blocks and operations are described
in detail in the following clauses.

##### 5.2.3.2.1 Glottal pulse codebook search

The TC mode improves the robustness of the codec to frame erasures. It
also encodes frames with an outdated past excitation buffer, e.g. after
switching from HQ core frame.

The TC mode in the current frame is selected based on the classification
algorithm described in subclause 5.1.13. The increased robustness, or
the excitation building when the past excitation is outdated, is
achieved by replacing the adaptive codebook (inter-frame long-term
prediction) with a codebook of glottal impulse shapes (glottal-shape
codebook) \[19\], which is independent from past excitation. The
glottal-shape codebook consists of quantized normalized shapes of the
truncated glottal impulses placed at specific positions. The codebook
search consists of both the selection of the best shape and the best
position.

To select the best codevector, the mean-squared error between the target
signal, $x\left( n \right)$ (the same target signal as used for the
adaptive codebook search described in subclause 5.2.3.1.2), and the
contribution signal, $y\left( n \right)$, is minimized for all candidate
glottal-shape codevectors. The glottal-shape codebook search has been
designed in a similar way as the algebraic codebook search, described in
subclause 5.2.3.1.5.9. In this approach, each glottal shape is
represented as an impulse response of a shaping
filter$G\left( z \right)$. This impulse response can be integrated in
the impulse response of the weighted synthesis filter
$W\left( z \right)H\left( z \right)$prior to the search of the optimum
impulse position. The searched codevectors can then be represented by
vectors containing only one non-zero element corresponding to candidate
impulse positions, and they can be searched very efficiently. Once
selected, the position codevector is convolved with the impulse response
of the shaping filter. This procedure needs to be repeated for all the
candidate shapes and the best shape-position combination will form the
excitation signal.

![](media/image39.wmf){width="6.440277777777778in"
height="5.639583333333333in"}

Figure 37: Schematic diagram of the excitation coding in TC mode

In the following, all vectors are supposed to be column vectors. Let
$p_{k^{'}}$ be a position codevector with one non-zero element at a
position$k^{'}$, and $q_{k^{'}}$ the corresponding glottal-shape
codevector with index $k^{'}$representing the centre of the glottal
shape. Index $k^{'}$is chosen from the range \[0, 63\], where 64 is the
subframe length. Note that, due to the non-causal nature of the shaping
filter, its impulse response is truncated for positions in the beginning
and at the end of the subframe. The glottal shape codevector
$q_{k^{'}}$can be expressed in a matrix form as
$q_{k^{'}} = \text{Gp}_{k^{'}}$, where $G$is a Toeplitz matrix
representing the glottal impulse shape. Similarly to the algebraic
codebook search, we can write

$\Im_{k^{'}} = \frac{\left( x^{T}y \right)^{2}}{y^{T}y} = \frac{\left( x^{T}\text{Hq}_{k^{'}} \right)^{2}}{q_{k^{'}}^{T}H^{T}\text{Hq}_{k^{'}}} = \frac{\left( x^{T}\text{HGp}_{k^{'}} \right)^{2}}{p_{k^{'}}^{T}G^{T}H^{T}\text{HGp}_{k^{'}}} = \frac{\left( x^{T}\text{Zp}_{k^{'}} \right)^{2}}{p_{k^{'}}^{T}Z^{T}\text{Zp}_{k^{'}}} = \frac{\left( d_{g}^{T}p_{k^{'}} \right)^{2}}{p_{k^{'}}^{T}\Phi_{g}p_{k^{'}}}$
(579)

where $H$is a lower triangular Toeplitz convolution matrix of the
weighted synthesis filter. The rows of a convolution matrix
$Z^{T}$correspond to the filtered shifted version of the glottal impulse
shape or its truncated representation.

Because of the fact that the position codevector $p_{k^{'}}$has only one
non-zero sample, the computation of the criterion (580) is very simple
and can be expressed as

$\Im_{k^{'}} = \frac{\left( d_{g}\left( k^{'} \right) \right)^{2}}{\Phi_{g}\left( k^{'},k^{'} \right)}$
(581)

As it can be seen from criterion (582), only the diagonal of the
correlation matrix $\Phi_{g}$from criterion (583) needs to be computed.

The codebook consists of 8 prototype glottal impulse shapes of length
$L = \text{17}$samples placed at all subframe positions. Note that,
since $L$is shorter than the subframe length, the remaining samples in
the subframe are set to zero.

In general, the coding efficiency of the glottal-shape codebook is lower
than the efficiency of the long-term prediction, and more bits are
generally needed to assure good synthesized speech quality.

However, the glottal-shape codebook does not need to be used in all
subframes. First, there is no reason to use this codebook in subframes
that do not contain any significant glottal impulse in the residual
signal. Second, the glottal-shape codebook search is important only in
the first pitch period in a frame. The following pitch periods can be
encoded using the more efficient standard adaptive codebook search as it
does not use the excitation of the past frame anymore. To satisfy the
constant bit-rate requirement, the glottal-shape codebook is used in the
EVS codec only in one of the four subframes in a frame. This leads to a
highly structured coding mode where the bit allocation is dependent on
the position of the first glottal impulse and the pitch period. The
subframe where the glottal-shape codebook is used is chosen as the
subframe with the maximum sample in the residual signal in the range
$\left\lbrack 0,T_{\text{op}} + 2 \right\rbrack$, where $T_{\text{op}}$
is the open-loop pitch period estimated over the first half of the
frame. The other subframes are processed as described in subclause
5.2.3.2.2.

Criterion (584) is typically used in the algebraic codebook search by
pre-computing the backward filtered target vector $d_{g}$ and the
correlation matrix $\Phi_{g}$. Given the non-causal nature of the filter
$G\left( z \right)$, the matrix $Z$is not triangular and Toeplitz
anymore, and this approach cannot be efficiently applied for the first
$L_{1/2} = 8$positions in the glottal-shape codebook search.

Let $z_{k^{'}}$ be the $\left( k^{'} + 1 \right)$th row of the matrix
$Z^{T}$, where $Z^{T} = G^{T}H^{T}$is computed in two steps to minimize
the computational complexity. In the first step, the first
$L_{1/2} + 1$rows of this matrix $Z^{T}$are calculated that correspond
to the positions $k^{'}$from the range
$\left\lbrack 0,L_{1/2} \right\rbrack$. In the second step, the
criterion (585) is used in a similar way as in the algebraic codebook
search for the remaining part of $Z^{T}$ (the last
$\text{64} - L_{1/2} - 1$rows of the matrix $Z^{T}$).

In the first step, the convolution between the glottal-shape codebook
entry for position $k^{'} = 0$and the impulse response
$h\left( n \right)$is first computed using

$z_{0}\left( n \right) = \sum_{i = 0}^{n}{g\left( n - i \right)h\left( i \right)}\ \text{for}\ n = 0,\ldots,\text{63}$
(586)

where we take advantage of the fact that the filter
$G\left( z \right)$has only $L_{1/2} + 1$non-zero coefficients.

Next, the convolution $z_{1}\left( n \right)$between the glottal-shape
codebook entry for the position $k^{'} = 1$and the impulse response
$h\left( n \right)$is computed, reusing the values of
$z_{0}\left( n \right)$. For the following rows, the recursion is
reused, resulting in

$\begin{matrix}
z_{k^{'}}\left( 0 \right) = g\left( - k^{'} \right)h\left( 0 \right), \\
z_{k^{'}}\left( n \right) = z_{k^{'} - 1}\left( n - 1 \right) + g\left( - k^{'} \right)h\left( n \right)\ \text{for}\ n = 1,\ldots,\text{63} \\
\end{matrix}$ (587)

The recursion (588) is repeated for all$k^{'} \leq L_{1/2}$.

Now, the criterion (589) can be computed for all positions $k^{'}$from
the range $\left\lbrack 0,L_{1/2} \right\rbrack$in the form

$\Im_{k^{'}} = \frac{\left( \sum_{i = 0}^{\text{63}}{z_{k^{'}}\left( i \right)\text{.}x_{1}\left( i \right)} \right)^{2}}{\sum_{i = 0}^{\text{63}}{z_{k^{'}}\left( i \right)\text{.}z_{k^{'}}\left( i \right)}}$
(590)

In the second step, we take advantage of the fact that rows
$L_{1/2} + 1,\ldots,\text{63}$of the matrix $Z^{T}$are built using the
coefficients of the convolution that are already computed as described
by recursion (591) for $k^{'} = L_{1/2}$. That is, each row corresponds
to the previous row shifted to the right by 1 with a zero added at the
beginning

$\begin{matrix}
z_{k^{'}}\left( 0 \right) = 0, \\
z_{k^{'}}\left( n \right) = z_{k^{'} - 1}\left( n - 1 \right)\ \text{for}\ n = 1,\ldots,\text{63} \\
\end{matrix}$ (592)

and this is repeated for $k^{'}$ from the range
$\left\lbrack L_{1/2} + 1,\ldots,\text{63} \right\rbrack$.

Next, the target vector $d_{g}$and the diagonal of the matrix
$\Phi_{g}$need to be computed. First, we evaluate the numerator and the
denominator of the criterion (593) for the last position
$k^{'} = \text{63}$

$d_{g}\left( \text{63} \right) = \sum_{i = 0}^{L_{1/2}}{x\left( \text{63} - L_{1/2} + i \right)\ z_{L_{1/2}}\left( i \right)}$
(594)

and

$\Phi_{g}\left( \text{63},\text{63} \right) = \sum_{i = 0}^{L_{1/2}}{z_{L_{1/2}}\left( i \right)}\ z_{L_{1/2}}\left( i \right)$
(595)

For the remaining positions, the numerator is computed using equation
(596), but with the summation index changed. In the computation of the
denominator, some of the previously computed values can be reused. For
example, for the position $k^{'} = \text{62}$, the denominator of
criterion (597) is computed using

$\Phi_{g}\left( \text{62},\text{62} \right) = \Phi_{g}\left( \text{63},\text{63} \right) + z_{L_{1/2}}\left( L_{1/2} + 1 \right)\ z_{L_{1/2}}\left( L_{1/2} + 1 \right)$
(598)

Similarly, we can continue to compute the numerator and the denominator
of criterion (599) for all positions $k^{'} > L_{1/2}$.

The search continues using the previously described procedure for all
other glottal impulse shapes and the codevector corresponding to the
best combination of glottal-shape and position is selected. To maintain
the complexity low, the computation described above is further reduced
by limiting the position search to ±4 samples around the maximum
absolute value of the residual signal.

The last parameter to be determined is the gain of the glottal-shape
codebook excitation. The gain is quantized in two steps. First, a
roughly quantized gain of the glottal-shape codevector, $g_{m}$, is
found. Then, after both the first-stage contribution (glottal-shape
codevector) and the second-stage contribution (algebraic codevector) of
the excitation signal are found, the gain $g_{p}$of the first-stage
contribution signal is jointly quantized with the second-stage
contribution gain, $g_{c}$. This is done using the memory-less gain
vector quantization, as described in subclause 5.2.3.1.7.1. The found
glottal shape codevector $q_{k^{'}}\left( n \right)$ is thus the
position codevector $p_{k^{'}}\left( n \right)$ filtered through the
shaping filter $G\left( z \right)$that represents the best found glottal
shape. When scaling the glottal-shape codevector
$q_{k^{'}}\left( n \right)$with the signed quantized
gain${\hat{g}}_{m}$, we finally obtain the first stage excitation
codevector, $v\left( n \right)$.

The glottal-shape gain $g_{m}$is quantized using a quantization table as
follows. First, an unquantized gain in the current glottal-shape
subframe is found as

$g_{m} = \frac{\sum_{n = 0}^{N - 1}{x\left( n \right)\text{.}y\left( n \right)}}{0\text{.}\text{01} + \sum_{n = 0}^{N - 1}{y\left( n \right)\text{.}y\left( n \right)}}$
(600)

where $N$ is the subframe length, $x\left( n \right)$is the target
signal and $y\left( n \right)$is the glottal-shape codevector
$q_{k^{'}}\left( n \right)$filtered through the weighted synthesis
filter$W\left( z \right)/\hat{A}\left( z \right)$. Further, the sign of
the glottal-shape gain is set to 0 if $g_{m} < 0$and 1 otherwise, and
written to the bitstream. Finally, the glottal-shape gain quantization
index $I_{\text{gm}}$is found as the maximum value of
$I_{\text{gm}}$that satisfies
$t\left( I_{\text{gm}} \right) < \left| g_{m} \right|$, where $t$is the
glottal-shape gain quantization table of dimension 8. The signed
quantized glottal-shape gain${\hat{g}}_{m}$is thus found as
${\hat{g}}_{m} = t\left( I_{\text{gm}} \right)$and its value is
quantized using 4 bits (1 bit for sign, 3 bits for the value).

It should be noted that the closed-loop pitch period, $T_{0}$, does not
need to be transmitted anymore in a subframe which uses the
glottal-shape codebook search with the exception of subframes containing
more than one glottal impulse, i.e.,
when$\left( k^{'} + T_{0} \right) < \text{64} + L_{1/2}$. There are
situations where the pitch period of the input signal is shorter than
the subframe length and, in this case, we have to transmit its value.
Given the pitch period length limitations and the subframe length, a
subframe cannot contain more than two impulses. In the situation that
the glottal-shape codevector contains two impulses, an adaptive codebook
search is used in a part of the subframe. The first $T_{0}$samples of
the glottal-shape codevector $q_{k^{'}}\left( n \right)$are built using
the glottal-shape codebook search and then the other samples in the
subframe are built using the adaptive search as shown in Figure 38.

![](media/image40.wmf){width="3.439583333333333in"
height="1.8430555555555554in"}

Figure 39: Glottal-shape codevector with two impulses construction

The described procedure is used even if the second glottal impulse
appears in one of the first $L_{1/2}$positions of the next subframe. In
this situation, only a few samples (less than $L_{1/2} + 1$) of the
glottal shape are used at the end of the current subframe. This approach
has a limitation because the pitch period value transmitted in these
situations is limited to $T_{0} < N$, if it is bigger, it is not
transmitted.

In order to enhance the coding performance, a low-pass filter is applied
to the first stage excitation signal $v\left( n \right)$. In all
subframes after the glottal-shape codebook subframe, the low-pass
filtered first stage excitation is found as described in subclause
5.2.3.1.4.2.

##### 5.2.3.2.2 TC frame configurations

##### 5.2.3.2.2.1 TC frame configurations at 12.8 kHz internal sampling {#tc-frame-configurations-at-12.8-khz-internal-sampling .H6}

At bit-rates with 12.8 kHz internal sampling rate the glottal-shape
codebook is used in one out of four subframes. The other subframes in a
TC frame (not encoded with the use of the glottal-shape codebook) are
processed as follows. If the subframe with glottal-shape codebook search
is not the first subframe in the frame, the excitation signal in
preceding subframes is encoded using the algebraic CELP codebook only,
this means that the first stage contribution signal is zero. If the
glottal-shape codebook subframe is not the last subframe in the frame,
the following subframes are processed by the standard CELP coding (i.e.,
using the adaptive and the algebraic codebook search). Thus, the first
stage excitation signal is the scaled glottal-shape codevector, the
adaptive codevector or the zero codevector.

In order to further increase encoding efficiency and to optimize bit
allocation, different processing is used in particular subframes of a TC
frame dependent on the pitch period. When the first subframe is chosen
as a TC subframe, the subframe with the 2nd glottal impulse in the LP
residual signal is determined. This determination is based on the pitch
period value and the following four situations then can appear. In the
first situation, the 2nd glottal impulse is in the 1st subframe, and the
2nd, 3rd and 4th subframes are processed using the standard CELP coding
(adaptive and algebraic codebook search). In the second situation, the
2nd glottal impulse is in the 2nd subframe, and the 2nd, 3rd and 4th
subframes are processed using the standard CELP coding again. In the
third case, the 2nd glottal impulse is in the 3rd subframe. The 2nd
subframe is processed using algebraic codebook search only as there is
no glottal impulse in the 2nd subframe of the LP residual signal to be
searched for using the adaptive codebook. The 3rd and 4th subframes are
processed using the standard CELP coding. In the last (fourth) case, the
2nd glottal impulse is in the 4th subframe (or in the next frame), the
2nd and 3rd subframes are processed using the algebraic codebook search
only, and the 4th subframe is processed using the standard CELP coding.
Table 50 shows all possible coding configurations in the EVS codec at
12.8 kHz internal sampling rate.

The TC configuration is transmitted in the bit-stream using a
Huffman-style coding and its bit sequence is show in the Table 50 in the
column bitstream.

Table 50: TC configurations used in the EVS codec at 12.8 kHz internal
sampling rate

+---------+---------+---------+---------+---------+---------+---------+
| Coding  | Bi      | Po      | Type of |         |         |         |
| config  | tstream | sitions | c       |         |         |         |
| uration |         | of the  | odebook |         |         |         |
|         |         | first   | used    |         |         |         |
|         |         | (and    | (GS =   |         |         |         |
|         |         | the     | glottal |         |         |         |
|         |         | second, | -shape, |         |         |         |
|         |         | if      |         |         |         |         |
|         |         | re      | Ada =   |         |         |         |
|         |         | levant) | ad      |         |         |         |
|         |         | glottal | aptive, |         |         |         |
|         |         | imp     | Alg =   |         |         |         |
|         |         | ulse(s) | alg     |         |         |         |
|         |         | in the  | ebraic) |         |         |         |
|         |         | frame   |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
|         |         |         | 1st     | 2nd     | 3rd     | 4th     |
|         |         |         | subfr.  | subfr.  | subfr.  | subfr.  |
+---------+---------+---------+---------+---------+---------+---------+
| TC1     | 1       | ![      | GS +    | Ada +   | Ada +   | Ada +   |
|         |         | ](media | Alg     | Alg     | Alg     | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 1.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| TC2     | 0101    | ![      | GS +    | Ada +   | Ada +   | Ada +   |
|         |         | ](media | Alg     | Alg     | Alg     | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 2.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| TC3     | 0100    | ![      | GS +    | Alg     | Ada +   | Ada +   |
|         |         | ](media | Alg     |         | Alg     | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 3.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| TC4     | 011     | ![      | GS +    | Alg     | Alg     | Ada +   |
|         |         | ](media | Alg     |         |         | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 4.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| TC5     | 001     | ![      | Alg     | GS +    | Ada +   | Ada +   |
|         |         | ](media |         | Alg     | Alg     | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 5.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| TC6     | 0001    | ![      | Alg     | Alg     | GS +    | Ada +   |
|         |         | ](media |         |         | Alg     | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 6.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+
| TC7     | 0000    | ![      | Alg     | Alg     | Alg     | GS +    |
|         |         | ](media |         |         |         | Alg     |
|         |         | /image4 |         |         |         |         |
|         |         | 7.wmf){ |         |         |         |         |
|         |         | width=" |         |         |         |         |
|         |         | 2.02291 |         |         |         |         |
|         |         | 6666666 |         |         |         |         |
|         |         | 6667in" |         |         |         |         |
|         |         | hei     |         |         |         |         |
|         |         | ght="0. |         |         |         |         |
|         |         | 3805555 |         |         |         |         |
|         |         | 5555555 |         |         |         |         |
|         |         | 554in"} |         |         |         |         |
+---------+---------+---------+---------+---------+---------+---------+

##### 5.2.3.2.2.2 TC frame configurations at 16 kHz internal sampling {#tc-frame-configurations-at-16-khz-internal-sampling .H6}

At bit-rates with 16 kHz internal sampling rate the glottal-shape
codebook is used in one out of five subframes. If the subframe with
glottal-shape codebook search is not the first subframe in the frame,
the excitation signal in preceding subframes is encoded using the
algebraic CELP codebook only. If the glottal-shape codebook subframe is
not the last subframe in the frame, the following subframes are
processed by the standard CELP coding.

As the bit-rates with 16 kHz internal sampling rate are with high
bit-budget, the number of TC configurations is reduced compared to the
12.8 kHz internal sampling rate. Table 51 shows all possible coding
configurations in the EVS codec at 16 kHz internal sampling rate

Table 51: TC configurations used in the EVS codec at 16 kHz internal
sampling rate

+-------+-------+-------+-------+-------+-------+-------+-------+
| C     | Bits  | Posi  | Type  |       |       |       |       |
| oding | tream | tions | of    |       |       |       |       |
| con   |       | of    | cod   |       |       |       |       |
| figur |       | the   | ebook |       |       |       |       |
| ation |       | first | used  |       |       |       |       |
|       |       | gl    |       |       |       |       |       |
|       |       | ottal | (GS = |       |       |       |       |
|       |       | im    | glot  |       |       |       |       |
|       |       | pulse | tal-s |       |       |       |       |
|       |       | in    | hape, |       |       |       |       |
|       |       | the   |       |       |       |       |       |
|       |       | frame | Ada = |       |       |       |       |
|       |       |       | adap  |       |       |       |       |
|       |       |       | tive, |       |       |       |       |
|       |       |       | Alg = |       |       |       |       |
|       |       |       | algeb |       |       |       |       |
|       |       |       | raic) |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
|       |       |       | 1st   | 2nd   | 3rd   | 4th   | 5th   |
|       |       |       | s     | s     | s     | s     | s     |
|       |       |       | ubfr. | ubfr. | ubfr. | ubfr. | ubfr. |
+-------+-------+-------+-------+-------+-------+-------+-------+
| TC    | 00    |       | GS +  | Ada + | Ada + | Ada + | Ada + |
| ~16~1 |       |       | Alg   | Alg   | Alg   | Alg   | Alg   |
+-------+-------+-------+-------+-------+-------+-------+-------+
| TC    | 01    |       | Alg   | GS +  | Ada + | Ada + | Ada + |
| ~16~2 |       |       |       | Alg   | Alg   | Alg   | Alg   |
+-------+-------+-------+-------+-------+-------+-------+-------+
| TC    | 10    |       | Alg   | Alg   | GS +  | Ada + | Ada + |
| ~16~3 |       |       |       |       | Alg   | Alg   | Alg   |
+-------+-------+-------+-------+-------+-------+-------+-------+
| TC    | 110   |       | Alg   | Alg   | Alg   | GS +  | Ada + |
| ~16~4 |       |       |       |       |       | Alg   | Alg   |
+-------+-------+-------+-------+-------+-------+-------+-------+
| TC    | 111   |       | Alg   | Alg   | Alg   | Alg   | GS +  |
| ~16~5 |       |       |       |       |       |       | Alg   |
+-------+-------+-------+-------+-------+-------+-------+-------+

##### 5.2.3.2.2.3 Pitch period and gain coding in the TC mode {#pitch-period-and-gain-coding-in-the-tc-mode .H6}

When using the TC, it is not necessary to transmit the pitch period for
certain subframes. Further, it is not necessary to transmit both pitch
gain, $g_{p}$, and the algebraic codebook gain, $g_{c}$, for subframes
where there is no important glottal impulse, and only the algebraic
codebook contribution is computed (the first stage excitation is the
zero vector).

In subframes, where the glottal-shape, or adaptive, search is used, the
first stage excitation gain (pitch gain), $g_{p}$, and the second stage
excitation gain (algebraic gain), $g_{c}$, are quantized at bit-rates ≤
32 kbps using the memory-less vector gain quantization described in
subclause 5.2.3.1.7.1. At 64 kbps bit-rate, gains are scalar quantized
as described in subclause 5.2.3.1.7.3. In glottal-shape subframes, the
first stage gain, $g_{p}$, is found in the same manner as described in
subclause 5.2.3.1.4.2.

When only an algebraic gain is quantized in the current frame (the first
stage excitation is the zero vector), the following scalar quantization
process is used. First, an optimal algebraic gain in the current
subframe is found as

$g_{c} = \frac{\sum_{n = 0}^{N - 1}{x\left( n \right)\text{.}z\left( n \right)}}{0\text{.}\text{01} + \sum_{n = 0}^{N - 1}{z\left( n \right)\text{.}z\left( n \right)}}$
(601)

where $N$is the subframe length, $x\left( n \right)$is the target signal
and $z\left( n \right)$is the algebraic codevector
$c\left( n \right)$filtered through the weighted synthesis
filter$W\left( z \right)/\hat{A}\left( z \right)$with the pre-filter
$F\left( z \right)$. The predictive algebraic energy calculated once per
frame is employed as described in subclause 5.2.3.1.7. Further the
algebraic codebook gain, $g_{c}^{'}$, and the correction factor,
$\gamma$, are given by equations (602) and (603), respectively. Finally,
the correction factor quantization index, $I_{\gamma}$, is found as the
maximum value of $I_{\gamma}$that satisfies

${\hat{g}}_{c} < \left( \frac{t\left( I_{\gamma} + 1 \right) + t\left( I_{\gamma} \right)}{2} \right)\text{.}g_{c}^{'}$
(604)

where $t$is the algebraic gain quantization table of dimension 8. The
correction factor $\gamma$ is quantized with 3 bits using the
quantization table$t$and the quantized algebraic gain is obtained by

${\hat{g}}_{c} = t\left( I_{\gamma} \right)\ \text{.}\ g_{c}^{'}$ (605)

The following is a list of all TC configurations corresponding to Table
52 and Table 53.

Configuration TC1

> In this configuration, two first glottal impulses appear in the first
> subframe that is processed using the glottal-shape codebook search.
> This means that the pitch period value in the 1st subframe can have
> the maximum value less than the subframe length, i.e.,
> $\text{34} < T_{0} < N$. Here, $T_{0}$is the closed-loop pitch period
> and $N$ the subframe length. With the ½ sample resolution it can be
> coded with 6 bits. The pitch periods in the next subframes are found
> using -- depending on the bit-rate -- a 5- or 6-bit delta search with
> a fractional resolution.

Configuration TC2

> When configuration TC2 is used, the first subframe is processed using
> the glottal-shape codebook search. The pitch period is not needed and
> all following subframes are processed using the adaptive codebook
> search. Because we know that the 2nd subframe contains the second
> glottal impulse, the pitch period maximum value holds
> $T_{0} < 2\text{.}N - 1$. This maximum value can be further reduced
> thanks to the knowledge of the glottal impulse position value $k^{'}$.
> The pitch period value in the 2nd subframe is then coded using 7 bits
> with a fractional resolution in the whole range of
> $\left\lbrack \text{min}\left( N - k^{'},\text{34} \right),2\text{.}N - 1 - k^{'} \right\rbrack$.
> In the 3rd and 4th subframes, a delta search using 6 bits is used with
> a fractional resolution.

Configuration TC3

> When configuration TC3 is used, the first subframe is processed using
> the glottal-shape codebook search with no use of the pitch value
> again. But because the 2nd subframe of the LP residual signal contains
> no glottal impulse and the adaptive search is useless, the first stage
> contribution signal is replaced by zeros in the 2nd subframe. The
> adaptive codebook parameters ($T_{0}$ and $g_{p}$) are not transmitted
> in the 2nd subframe. The first stage contribution signal in the 3rd
> subframe is constructed using the adaptive codebook search with the
> pitch period maximum value $\left( 3\text{.}N - 1 - k^{'} \right)$and
> the minimum value $\left( 2\text{.}N - 1 - k^{'} \right)$, thus only a
> 7-bit coding of the pitch value with fractional resolution in all
> range is needed. The 4th subframe is processed using the adaptive
> search with -- depending on the bit-rate -- a 5- or 6-bit delta search
> coding of the pitch period value.
>
> In the 2nd subframe, only the algebraic codebook gain, gc, is
> transmitted. Consequently, only 3 bits are needed for gain
> quantization in this subframe as described at the beginning of this
> subclause.

Configuration TC4

> When configuration TC4 is used, the first subframe is processed using
> the glottal-shape codebook search. Again, the pitch period does not
> need to be transmitted. But because the LP residual signal contains no
> glottal impulse in the 2nd and also in the 3rd subframe, the adaptive
> search is useless for both these subframes. Again, the first stage
> excitation signal in these subframes is replaced by zeros. The pitch
> period value is transmitted only in the 4th subframe by means of 7
> bits and its minimum value is $\left( 3\text{.}N - k^{'} \right)$.The
> maximum value of the pitch period is limited by the
> $T_{\text{max}} = \text{231}$value only. It does not matter if the
> second glottal impulse will appear in the 4th subframe or not (the
> second glottal impulse can be present in the next frame if
> $k^{'} + T_{\text{max}} \geq N$).
>
> Note that the absolute value of the pitch period is necessary at the
> decoder for the frame concealment; therefore, it is transmitted also
> in the situation when the second glottal impulse appears in the next
> frame. When a frame $m$preceding the TC frame $m + 1$is missing, the
> correct knowledge of the pitch period value from the frames $m - 1$
> and $m + 1$helps to reconstruct the missing part of the synthesis
> signal in the frame $m$successfully.
>
> The algebraic codebook gain,$g_{c}$, is quantized with 3 bits in the
> 2nd subframe and 3rd subframe.

Configuration TC5

> When the first glottal impulse appears in the 2nd subframe, the pitch
> period is transmitted only for the 3rd and 4th subframe. 3^rd^
> subframe, the pitch value is coded using 9-bit absolute search while
> in the 4^th^ subframe using -- depending on the bit-rate -- 5- or 6-
> bits delta search. In this case, only algebraic codebook parameters
> are transmitted in the 1st subframe (with the algebraic codebook gain,
> $g_{c}$, quantized with 3 bits).

Configuration TC6

> When the first glottal impulse appears in the 3rd subframe, the pitch
> period does not need to be transmitted for the TC technique. In this
> case, only algebraic codebook parameters are transmitted in the 1st
> and 2nd subframe with the algebraic codebook gain,$g_{c}$, quantized
> with 3 bits in both subframes. Nevertheless, the pitch period is
> transmitted in the 4th subframe by means of 9 bit absolute search
> coding for the reason of better frame erasure concealment in the frame
> after the TC frame. Also, the pitch period is transmitted for the 3rd
> subframe by means of 5 bit absolute search coding although it is not
> usually necessary.

Configuration TC7

> When the first glottal impulse appears in the 4th subframe, the pitch
> period value information is not usually used in this subframe.
> However, its value is necessary for the frame concealment at the
> decoder (this value is used for the missing frame reconstruction when
> the frame preceding or following the TC frame is missing) or in case
> of strong onsets at the frame-end and very short pitch period. Thus,
> the pitch value is transmitted only in the 4th subframe by means of
> 9-bit absolute search coding and only algebraic codebook parameters
> are transmitted in the first three subframes (the gain pitch, $g_{p}$,
> is not essential). The algebraic codebook gain, $g_{c}$, is quantized
> with 3 bits in the 1^st^, 2^nd^ and 3rd subframes. The scalar gain
> quantization is employed only at the 4th subframe in this
> configuration to encode the gain pitch and the algebraic codebook
> gain.

Configuration TC~16~1

> In this configuration, one or two first glottal impulses appear in the
> first subframe that is processed using the glottal-shape codebook
> search. This means that the pitch period value in the 1st subframe can
> have the maximum value less than the subframe length $N$, i.e.,
> $\text{42} < T_{0} < N$ and it is coded with 6 bits. Then the pitch
> period in the 2nd subframe is found using 8-bit absolute search on the
> interval $\text{42} < T_{0} < 2 \ast N$. Finally the pitch period in
> the 3^rd^ subframe is coded using 10-bit absolute search and in the
> 4^th^ and 5^th^ subframe using 6-bit delta search.
>
> The gain pitch and the algebraic codebook gain are coded in all
> subframes using 6-bit VQ at 32 kbps resp. 12-bit SQ at 64 kbps.

Configuration TC~16~2

> The first glottal impulse appears in the 2nd subframe and the pitch
> period is transmitted for the 3^rd^, 4^th^ and 5th subframe. In the
> 3^rd^ subframe, the pitch value is coded using 10-bit absolute search
> while in the 4^th^ and 5^th^ subframe using 6- bits delta search. The
> pitch period is transmitted also in the 2^nd^ subframe by means of 6
> bits and serves in case when two first glottal impulses appears in the
> second subframe.
>
> The gain pitch and the algebraic codebook gain are coded in the 2^nd^,
> 3^rd^, 4^th^ and 5th subframe using 6-bit VQ at 32 kbps resp. 12-bit
> SQ at 64 kbps. The algebraic codebook gain,$g_{c}$, is quantized in
> the 1st subframe with 3 bits at 32 kbps resp. 6 bits at 64 kbps

Configuration TC~16~3

> The first glottal impulse appears in the 3rd subframe. In this case,
> only algebraic codebook parameters are transmitted in the 1st and 2nd
> subframe with the algebraic codebook gain,$g_{c}$, quantized in both
> subframes with 3 bits at 32 kbps resp. 6 bits at 64 kbps. Then the
> pitch period is coded by means of 10-bit absolute search in the 3^rd^
> subframe and by means of 6-bit delta search in the 4^th^ and 5^th^
> subframe.
>
> The gain pitch and the algebraic codebook gain are coded in the 3^rd^,
> 4^th^ and 5th subframe using 6-bit VQ at 32 kbps resp. 12-bit SQ at 64
> kbps.

Configuration TC~16~4

> The first glottal impulse appears in the 4th subframe. In this case,
> only algebraic codebook parameters are transmitted in the 1^st^, 2nd
> and 3^rd^ subframe with the algebraic codebook gain,$g_{c}$, quantized
> in all these subframes with 3 bits at 32kbps resp. 6 bits at 64kbps.
> Then the pitch period is coded by means of 10-bit absolute search in
> the 4^th^ subframe and by means of 6-bit delta search in the 5^th^
> subframe.
>
> The gain pitch and the algebraic codebook gain are coded in the 4^th^
> and 5th subframe using 6-bit VQ at 32 kbps resp. 12-bit SQ at 64 kbps.

Configuration TC~16~5

When the first glottal impulse appears in the 5th subframe, the pitch
period value information is not usually used in this subframe. However,
its value is necessary for the frame concealment at the decoder (this
value is used for the missing frame reconstruction when the frame
preceding or following the TC frame is missing) or in case of strong
onsets at the frame-end and very short pitch period. Thus, the pitch
value is transmitted only in the 5th subframe by means of 10-bit
absolute search coding and only algebraic codebook parameters are
transmitted in the first four subframes. The algebraic codebook gain,
$g_{c}$, is quantized in the 1^st^, 2^nd^ and 3rd subframes with 3 bits
at 32kbps and 6 bits at 64 kbps. The gain pitch and the algebraic
codebook gain are coded only in 5th subframe using a scalar gain
quantizer.

##### 5.2.3.2.2.4 Update of filter memories {#update-of-filter-memories .H6}

In TC mode, the memories of the synthesis and weighting filter are
updated as described in subclause 5.2.3.1.8. Note that signals in
equation (606) are the first stage excitation signal $v\left( n \right)$
(i.e., the glottal-shape codevector, the low-pass filtered adaptive
codevector, or the zero codevector) and the algebraic codevector
$c\left( n \right)$ (including pre-filtering).

#### 5.2.3.3 Excitation coding in UC mode at low rates

The principle of excitation coding in UC mode is shown in a schematic
diagram in Figure 40. The individual operations are described in detail
in the following clauses.

![](media/image53.wmf){width="6.558333333333334in"
height="3.0305555555555554in"}

Figure 38: Schematic diagram of the excitation coding in UC mode

##### 5.2.3.3.1 Structure of the Gaussian codebook

In UC mode, a Gaussian codebook is used for representing the excitation.
To simplify the search and reduce the codebook memory requirement, an
efficient structure is used whereby the excitation codevector is derived
by the addition of 2 signed vectors taken from a table containing 64
Gaussian vectors of dimension 64 (the subframe size). Let $t_{i}$ denote
the $i$th 64-dimensional Gaussian vector in the table. Then, a
codevector is constructed by

$c = s_{1}t_{p_{1}} + s_{2}t_{p_{2}}$ (607)

where $s_{1}$and $s_{2}$are the signs, equal to --1 or 1, and $p_{1}$and
$p_{2}$are the indices of the Gaussian vectors from the table. In order
to reduce the table memory, a shift-by-2 table is used, thus only 64 +
63 × 2 = 190 values are needed to represent the 64 vectors of dimension
64.

To encode the codebook index, one has to encode 2 signs, $s_{1}$and
$s_{2}$, and two indices, $p_{1}$and $p_{2}$. The values of $p_{1}$and
$p_{2}$are in the range $\left\lbrack 0,\text{63} \right\rbrack$, so
they need 6 bits each, and the signs need 1 bit each. However, 1 bit can
be saved since the order of the vectors $v_{i}$and $v_{j}$is not
important. For example, choosing $v_{\text{16}}$as the first vector and
$v_{\text{25}}$as the second vector is equivalent to choosing
$v_{\text{25}}$as the first vector and $v_{\text{16}}$as the second
vector. Thus, similar to the case of encoding two pulses in a track,
only one bit is needed for both signs. The ordering of the vector
indices is such that the other sign information can be easily deduced.
This gives a total of 13 bits. To better explain this procedure, let us
assume that the two vectors have the indices $p_{1}$and $p_{2}$with sign
indices $\sigma_{1}$and $\sigma_{2}$, respectively ($\sigma = 0$if the
sign is positive and $\sigma = 1$if the sign is negative). The
codevector index is given by

$I = \sigma_{1} + 2 \times \left( p_{1} \times 6 + p_{2} \right)$ (608)

If $p_{1} \leq p_{2}$then $\sigma_{1} = \sigma_{2}$; otherwise
$\sigma_{2}$ is different from $\sigma_{1}$. Thus, when constructing the
codeword (index of codevector), if the two signs are equal then the
smaller index is assigned to $p_{1}$and the larger index to $p_{2}$,
otherwise the larger index is assigned to $p_{1}$and the smaller index
to $p_{2}$.

##### 5.2.3.3.2 Correction of the Gaussian codebook spectral tilt

In UC mode, the Gaussian codebook spectral tilt is corrected by a
modification factor, which is encoded using 3 bits per subframe. First,
the tilt of the target vector $x\left( n \right)$is computed as

$e_{\text{tilt}}^{\text{xx}} = \frac{\sum_{i = 1}^{\text{63}}{x\left( i \right)x\left( i - 1 \right)}}{\sum_{i = 1}^{\text{63}}\left\lbrack x\left( i \right) \right\rbrack^{2}}$
(609)

and the tilt of the filtered Gaussian codebook $y\left( n \right)$is
computed as

$e_{\text{tilt}}^{\text{yy}} = \frac{\sum_{i = 1}^{\text{189}}{y\left( i \right)y\left( i - 1 \right)}}{\sum_{i = 1}^{\text{189}}\left\lbrack y\left( i \right) \right\rbrack^{2}}$
(610)

The filtered Gaussian codebook, $y\left( n \right)$, is the initial
Gaussian codebook, $t\left( n \right)$, convolved with the weighted
filter, $h\left( n \right)$. Note that vector
$t\left( n \right)$represents the whole codebook, i.e.,
$n = 0,\ldots,\text{189}$.

The spectral tilt modification factor is found by

$\delta = \frac{1 - e_{\text{tilt}}^{\text{yy}}\ \text{.}\ e_{\text{tilt}}^{\text{xx}}}{2\ \text{.}\ e_{\text{tilt}}^{\text{yy}}\  + \ e_{\text{tilt}}^{\text{xx}}}$
(611)

and the integer quantization index is found by

$I_{\delta} = \left\lfloor \text{16}\ \delta \right\rfloor$ (612)

where the operator $\left\lfloor \ \text{.}\  \right\rfloor$ returns the
integer part of a floating point number. The integer quantization index
is limited to \[0, 7\].

Finally, the quantized spectral tilt modification factor is used to
adapt the tilt of the initial Gaussian codebook. That is

$\begin{matrix}
t^{'}\left( n \right) = t\left( n \right)\ \text{for}\ n = 0, \\
t^{'}\left( n \right) = \frac{t\left( n \right) - \hat{\delta}\text{.}t\left( n - 1 \right)}{1 + {\hat{\delta}}^{2}}\ \text{for}\ n = 1,\ldots,\text{189} \\
\end{matrix}$ (613)

where the quantized spectral tilt modification factor is found as

$\hat{\delta} = \frac{I_{\delta}}{\text{16}}$ (614)

In the following, the adapted Gaussian codebook
$t^{'}\left( n \right),\ n = 0,\ldots,\text{189}$, is searched to obtain
the best two codevectors and signs which form the final codevector,
$c\left( n \right)$, of dimension 64. In the following, we assume
$t\left( n \right) \leftarrow t^{'}\left( n \right)$.

##### 5.2.3.3.3 Search of the Gaussian codebook

The goal of the search procedure is to find the indices $p_{1}$and
$p_{2}$of the two best random vectors and their corresponding signs,
$s_{1}$and $s_{2}$. This is achieved by maximizing the following search
criterion

$Q = \frac{\left( x^{T}z \right)^{2}}{z^{T}z} = \frac{\left( x^{T}H\ c \right)^{2}}{z^{T}z} = \frac{\left( d^{T}c \right)^{2}}{z^{T}z}$
(615)

where $x\left( n \right)$is the target vector and $z = \text{Hc}$is the
filtered final codevector. Note that in the numerator of the search
criterion, the dot product between $x\left( n \right)$and
$z\left( n \right)$, $n = 0,\ldots,\text{63}$, is equivalent to the dot
product between $d\left( n \right)$and $c\left( n \right)$, where
$d = H^{T}x$is the backward filtered target vector which is also the
correlation between $d\left( n \right)$and the impulse response
$h\left( n \right)$. The elements of the vector $d\left( n \right)$are
found by

$d\left( n \right) = x\left( n \right) \ast h\left( - n \right) = \sum_{i = n}^{\text{63}}{x\left( i \right)}\ h\left( i - n \right)\ \text{for}\ n = 1,\ldots,\text{63}$
(616)

Since $d\left( n \right)$is independent of the codevector
$c\left( n \right)$, it is computed only once, which simplifies the
computation of the numerator for different codevectors.

After computing the vector $d\left( n \right)$, a predetermination
process is used to identify $K$out of the 64 random vectors in the
random table, so that the search process is then confined to those
$K$vectors. The predetermination is performed by testing the numerator
of the search criterion $Q_{k}$for the $K$vectors which have the largest
absolute dot product (or squared dot product) between
$d\left( n \right)$and $v_{i}\left( n \right)$,
$i = 0,\ldots,\text{63}$. That is, the dot products$\chi_{i}$that are
given by

$\chi_{i} = \sum_{n = 0}^{\text{63}}{d\left( n \right)}\ v_{i}\left( n \right)$
(617)

are computed for all random vectors $v_{i}\left( n \right)$and the
indices of the $K$vectors which result in the $K$largest values of
$\left| \chi_{i} \right|$are retained. These indices are stored in the
index vector $m_{i}$, $i = 0,\ldots,K - 1$. To further simplify the
search, the sign information corresponding to each predetermined vector
is also preset. The sign corresponding to each predetermined vector is
given by the sign of $\chi_{i}$for that vector. These preset signs are
stored in the sign vector $s_{i}$, $i = 0,\ldots,K - 1$.

The codebook search is now confined to the pre-determined $K$vectors
with their corresponding signs. Here, the value $K = 8$is used, thus the
search is reduced to finding the best 2 vectors among 8 random vectors
instead of finding them among 64 random vectors. This reduces the number
of tested vector combinations from
$\frac{\text{64} \times \text{65}}{2} = \text{2080}$to
$\frac{8 \times 9}{2} = \text{36}$.

Once the most promising $K$vectors and their corresponding signs are
predetermined, the search proceeds with the selection of 2 vectors among
those $K$vectors which maximize the search criterion $Q$.

We first start by computing and storing the filtered vectors $w_{j}$,
$j = 0,\ldots,K - 1$corresponding to the $K$predetermined vectors. This
can be performed by convolving the predetermined vectors with the
impulse response of the weighted synthesis filter, $h\left( n \right)$.
The sign information is also included in the filtered vectors. That is

$w_{j}\left( n \right) = s_{j}\sum_{i = 0}^{n}{t_{m_{j}}\left( i \right)}\ h\left( n - i \right)\ \text{for}\ n = 0,\ldots,\text{63},\ j = 0,\ldots,K - 1$
(618)

We then compute the energy of each filtered pre-determined vector as

$\varepsilon_{j} = w_{j}^{T}w_{j} = \sum_{n = 0}^{\text{63}}{w_{j}^{2}\left( n \right)}\ \text{for}\ j = 0,\ldots,K - 1$
(619)

and its dot product with the target vector

$\rho_{j} = x^{T}w_{j} = \sum_{n = 0}^{\text{63}}{w_{j}\left( n \right)\ x\left( n \right)}\ \text{for}\ j = 0,\ldots,K - 1$
(620)

Note that $\rho_{j}$and $\varepsilon_{j}$correspond to the numerator and
denominator of the search criterion due to each predetermined vector.
The search proceeds now with the selection of 2 vectors among the
$K$predetermined vectors by maximizing the search criterion $Q$. Note
that the final codevector is given in equation (621).

The filtered codevector $z\left( n \right)$is given by

$z = \text{Hc} = s_{1}\text{Ht}_{p_{1}} + s_{2}\text{Ht}_{p_{2}} = w_{p_{1}} + w_{p_{2}}$
(622)

Note that the predetermined signs are included in the filtered
predetermined vectors $w_{j}$. The search criterion in equation (623)
can be expressed as

$Q = \frac{\left( x^{T}z \right)^{2}}{z^{T}z} = \frac{\left( x^{T}w_{p_{1}} + x^{T}w_{p_{2}} \right)^{2}}{\left( w_{p_{1}} + w_{p_{2}} \right)^{T}\left( w_{p_{1}} + w_{p_{2}} \right)} = \frac{\left( \rho_{p_{1}} + \rho_{p_{2}} \right)^{2}}{\varepsilon_{p_{1}} + \varepsilon_{p_{2}} + 2w_{p_{1}}^{T}w_{p_{2}}}$
(624)

The vectors $w_{j}$and the values of$\rho_{j}$and $\varepsilon_{j}$are
computed before starting the codebook search. The search is performed in
two nested loops for all possible positions $p_{1}$and $p_{2}$that
maximize the search criterion $Q$. Only the dot products between the
different vectors $w_{j}$need to be computed inside the loop.

At the end of the two nested loops, the optimum vector indices
$p_{1}$and $p_{2}$will be known. The two indices and the corresponding
signs are then encoded as described above. The gain of the final
Gaussian codevector is computed based on a combination of waveform
matching and energy matching. The gain is given by

$g_{c} = 0\text{.}6g_{w} + 0\text{.}4g_{e}$ (625)

where $g_{w}$is the gain that matches the waveforms of the vectors
$x\left( n \right)$and $z\left( n \right)$and is given
by$g_{w} = x^{T}z/z^{T}z$and $g_{e}$is the gain that matches the
energies of the vectors $x\left( n \right)$and $z\left( n \right)$and is
given by $g_{e} = \sqrt{x^{T}z/z^{T}z}$ . Here, $x\left( n \right)$is
the target vector and $z\left( n \right)$is the filtered codevector
$c\left( n \right)$, $n = 0,\ldots,\text{63}$.

##### 5.2.3.3.4 Quantization of the Gaussian codevector gain

In UC mode, the adaptive codebook is not used and only the Gaussian
codevector gain needs to be quantized. The Gaussian codevector gain in
dB is given by

$g_{c}^{\text{dB}} = \text{20}\text{log}\left( g_{c} \right)$ (626)

is uniformly quantized
between$\Gamma_{\text{min}}$and$\Gamma_{\text{max}}$with the step size
given by

$\delta = \left( \Gamma_{\text{max}} - \Gamma_{\text{min}} \right)L$
(627)

where $L$is the number of quantization levels. The quantization index
$k$is given by the integer part of

$k = \left\lfloor \frac{g_{c}^{\text{dB}} - \Gamma_{\text{min}}}{\delta} + 0\text{.}5 \right\rfloor$
(628)

Finally, the quantized gain in dB is given by

${\hat{g}}_{c}^{\text{dB}} = k \times \delta + \Gamma_{\text{min}}$
(629)

and the quantized gain is given by

${\hat{g}}_{c} = \text{10}^{{\hat{g}}_{c}^{\text{dB}}/\text{20}}$ (630)

In every subframe, 7 bits are used to quantize the gain. Thus,
$L = \text{128}$and the quantization step is
$\delta = 1\text{.}\text{71875}$dB with the quantization
boundaries$\Gamma_{\text{min}} = - \text{30}$and$\Gamma_{\text{max}} = \text{190}$.
The quantized gain, ${\hat{g}}_{c}$, is finally used to form the total
excitation in the UC mode by multiplying each sample of the codevector,
$c\left( n \right)$, by${\hat{g}}_{c}$.

##### 5.2.3.3.5 Other parameters in UC mode

In UC mode, the SAD and noisiness parameters are encoded to modify the
excitation vector in stationary inactive segments. The noisiness
parameter is required for an anti-swirling technique used in the decoder
for enhancing the background noise representation during inactive
speech.

The noisiness parameter is defined as the ratio between low- and
high-order LP residual variances:

$\nu = \frac{\sigma_{e,2}^{2}}{\sigma_{e,\text{16}}^{2}}$ (631)

where $\sigma_{e,2}^{2}$ and $\sigma_{e,\text{16}}^{2}$denote the LP
residual variances for second-order and 16th-order LP filters,
respectively. The LP residual variances are readily obtained as a
by-product of the Levinson-Durbin procedure, described in subclause
5.1.9.4.

The noisiness parameter is normalized to the interval \[0, 1\] within
which it is linearly quantized with 32 levels. That is

$\hat{\nu} = \left\lfloor \mu\text{.}\left( \nu - 1 \right)\text{32} \right\rfloor$
(632)

where $\mu$is a normalization factor, which is different for WB and NB
signals. For WB signals, $\mu = 2$, otherwise $\mu = 0\text{.}5$.

##### 5.2.3.3.6 Update of filter memories

In UC mode, the memories of the synthesis and weighting filter are
updated as described in subclause 5.2.3.1.8. Note that the excitation
component $v\left( n \right)$is missing in equation (633) for UC mode.

#### 5.2.3.4 Excitation coding in IC and UC modes at 9.6 kbps

At 9.6 kbps, the IC and UC modes are coded with a hybrid coding
embedding two stages of innovative codebooks, the algebraic pulse
codebook and a Gaussian noise-like excitation. Since the long term
prediction gain is expected to be very low for such frames, the adaptive
codebook is not used. The principle is depicted in figure 41.

![](media/image54.png){width="6.689583333333333in"
height="4.253472222222222in"}

Figure 39: Schematic diagram of the excitation coding in UC and IC modes
at 9.6 kbps

##### 5.2.3.4.1 Algebraic codebook

##### 5.2.3.4.1.1 Adaptive pre-filter {#adaptive-pre-filter-1 .H6}

For UC mode, the adaptive pre-filter is performed similarly as in
subclause 5.2.3.1.5.1. Additional the pre-filter $F\left( z \right)$ is
amended with a phase scrambling filter as follows:

$F\left( z \right) = F\left( z \right) \cdot (\frac{0\text{.}7 + z^{- 1}}{1 + 0\text{.}7z^{- 1}})$
(613)

For NB IC mode, the filter is designed as follows:

$F\left( z \right) = F^{\left( 0 \right)}\left( z \right)\frac{\hat{A}\left( z/\eta_{1} \right)}{\hat{A}\left( z/\eta_{2} \right)}$
(614)

where $F^{\left( 0 \right)}$is defined in subclause 5.2.3.1.5.1 with
$T = \text{64}$, $\beta_{1}$ is also defined in subclause 5.2.3.1.5.1,
and $\eta_{1} = 0\text{.}\text{75}$and $\eta_{2} = 0\text{.}9$.

For WB IC mode, $F\left( z \right)$ is defined as:

$F\left( z \right) = \frac{(1 - (a \cdot \beta_{1} + b \cdot \gamma)z^{- 1}}{1 - 0\text{.}\text{85}z^{- T}}$
(615)

where $T = \text{64}$, $a = 0\text{.}5$, $b = 0\text{.}\text{25}$and
$\gamma$ represents the tilt of following filter:

$F^{(1)}\left( z \right) = \frac{\hat{A}\left( z/\eta_{1} \right)}{\hat{A}\left( z/\eta_{2} \right)}$
(616)

The tilt is computed as:

$\gamma = - \sum_{n = 1}^{\text{63}}\frac{f^{(1)}(n - 1)f^{(1)}(n)}{(f^{(1)})^{2}(n)}$
(617)

$\beta_{1}$ is bounded by \[0.25 0.5\] and given is by:

$\beta_{1} = 0\text{.}\text{25} + \frac{0\text{.}\text{25}E_{v}^{\left\lbrack - 1 \right\rbrack}}{E_{v}^{\left\lbrack - 1 \right\rbrack} + E_{c}^{\left\lbrack - 1 \right\rbrack}}$
(618)

where$E_{v}^{\left\lbrack - 1 \right\rbrack}$and
$E_{c}^{\left\lbrack - 1 \right\rbrack}$are the energies of the scaled
pitch codevector and the scaled algebraic codevector of the previous
subframe, respectively.

##### 5.2.3.4.2 Gaussian noise generation

The Gaussian noise excitation is a second excitation added to the first
innovative excitation from the algebraic codebook. This second
contribution is only computed and added in WB.

The Gaussian noise excitation is produced by calling three times a
random generator with a uniform distribution between -1 and +1. It
follows the Central Limit Theorem.

$c2(n) = \sum_{i = 0}^{2}{\text{rand}(n)}$ (619)

The Gaussian noisy excitation $c2(n)$is spectrally shaped by applying
the pre-filter $F\left( z \right)$ defined in subclause 5.2.3.4.1.1.

##### 5.2.3.4.3 Gain coding

For NB only one gain has to be quantized, the gain of the algebraic
codebook $g_{c}$. It is quantized using a 6-bit quantizer.

For WB, the two gains $g_{c}$ and $g_{c2}$ are quantized jointly in each
subframe, using a 7-bit vector quantizer.

In both cases the optimal algebraic codeword gain is computed as
follows:

$g_{c} = \frac{\sum_{n = 0}^{N - 1}{x\left( n \right)\text{.}z\left( n \right)}}{\sum_{n = 0}^{\text{63}}{z\left( n \right)\text{.}z\left( n \right)}}$
(620)

In the equation above, $z\left( n \right)$ is the algebraic codevector
$c\left( n \right)$ filtered through the weighted synthesis filter
$W\left( z \right)/\hat{A}\left( z \right)$ with the pre-filter
$F\left( z \right)$.is the filtered algebraic codevector.

The algebraic codebook excitation energy in dB, $E_{c}$, is also
computed as follows:

$E_{c} = \text{10}\text{log}\left( \frac{1}{\text{64}}\sum_{n = 0}^{\text{63}}{c^{2}\left( n \right)} \right)$
(621)

##### 5.2.3.4.3.1 Innovative codebook gain coding (NB) {#innovative-codebook-gain-coding-nb .H6}

The algebraic codevector gain in dB is given by

$g_{c}^{\text{dB}} = \text{20}\text{log}\left( g_{c} \right)$ (622)

is uniformly quantized between -30 dB and 90dB with the step size of
1.9dB. The quantization index $k$ is given by the integer part of

$k = \left\lfloor \frac{g_{c}^{\text{dB}} + E_{c} + \text{30}}{1\text{.}9} + 0\text{.}5 \right\rfloor$
(623)

Finally, the quantized gain in dB is given by

${\hat{g}}_{c}^{\text{dB}} = k \times 1\text{.}9 - \text{30}$ (624)

and the quantized gain is given by

${\hat{g}}_{c} = \text{10}^{({\hat{g}}_{c}^{\text{dB}} - E_{c})/\text{20}}$
(625)

##### 5.2.3.4.3.2 Joint gain coding (WB) {#joint-gain-coding-wb .H6}

The algebraic codebook gain is quantized indirectly, using a predicted
energy of algebraic codevector. The energy of residual signal in dB is
calculated.

Then, average residual signal energy is calculated for the whole frame
and serves as a prediction of the algebraic codevector energy. It is
quantized on 4 bits once per frame. The quantized value of the predicted
algebraic codevector energy is defined as

$\begin{matrix}
k_{\text{ind}} = \underset{k = 0}{\overset{\text{15}}{\text{min}}}\left| {\overline{E}}_{r} - E_{\text{book}}\left( k \right) \right| \\
{\hat{E}}_{i} = E_{\text{book}}\left( k_{\text{ind}} \right) \\
\end{matrix}$ (626)

where $E_{\text{book}}\left( k \right),\ k = 0,\ldots,\text{15}$is the
4-bit codebook for the predicted algebraic codevector energy and
$k_{\text{ind}}$ is the index minimizing the criterion above.

Using the predicted algebraic codevector energy, we may estimate the
algebraic codebook gain as

$g_{c}^{'} = \text{10}^{0\text{.}\text{05}\left( {\hat{E}}_{i} - E_{c} \right)}$
(627)

A correction factor between the true algebraic codebook gain, $g_{c}$,
and the estimated one, $g_{c}^{'}$, is given by

$\gamma = g_{c}g_{c}^{'}$ (628)

The correction factor $\gamma$ is uniformly quantized on 5 bits between
-20 dB and 20dB with the step size of 1.25dB. The quantization index
$k$is given by the integer part of

$k = \left\lfloor \frac{\gamma + \text{20}}{1\text{.}\text{25}} + 0\text{.}5 \right\rfloor$
(629)

Finally, the quantized gain in dB is given by

${\hat{g}}_{c}^{\text{dB}} = k \times 1\text{.}\text{25} - \text{20} - E_{c} + {\hat{E}}_{i}$
(630)

and the quantized gain is given by

${\hat{g}}_{c} = \text{10}^{{\hat{g}}_{c}^{\text{dB}}/\text{20}}$ (631)

The gain of Gaussian noise excitation is quantized on 2 bits. Unlike the
algebraic codeword gain, the Gaussian noise excitation gain is optimized
in order to minimize the energy mismatch between the target signal and
reconstructed signal. The following criterion is minimized:

$\underset{{\hat{g}}_{c2}}{\text{min}}\left| C \cdot \sum_{n = 0}^{\text{63}}{x^{2}(n)} - \sum_{n = 0}^{\text{63}}{({\hat{g}}_{c}z(n) + {\hat{g}}_{c2}z2(n))^{2}} \right|$
(632)

where $C$ is an attenuation factor set to 1 for clean speech, where high
dynamic of energy is perceptually important and set to 0.8 for noisy
speech where the noise excitation is made more conservative for avoiding
fluctuation in the output energy between unvoiced and non-unvoiced
frames

The quantized gain is expressed as follows where the index $k2$ of the
optimal gain is sent on 2 bits:

${\hat{g}}_{c2} = (0\text{.}\text{25}k2 + 0\text{.}\text{25}) \cdot {\hat{g}}_{c} \cdot \frac{\sum_{n = 0}^{\text{63}}\sqrt{c^{2}(n)}}{\sum_{n = 0}^{\text{63}}\sqrt{c2^{2}(n)}}$
(633)

##### 5.2.3.4.4 Memory update

The update of the filter memories is performed as described in subclause
5.2.3.1.8 except that there is no adaptive codebook contribution. The
Gaussian noise excitation is not taken into account for the update and
for computing the next subframe signal target.

#### 5.2.3.5 Excitation coding in GSC mode

In the GSC mode, the excitation is encoded using mixed
time-domain/frequency-domain coding technique. This mode is aimed at
encoding generic audio signals at low bit rates without introducing more
delay than the ACELP structure requires. The GSC mode is used only at
12.8 kHz internal sampling rate, and the excitation could be encoded
with 4 subframes, 2 subframes, or 1 subframe per frame depending on the
bit rate or the signal type.

Figure 40 is a schematic block diagram showing the general concept of
coding the excitation in the GSC mode. The speech/music selector is used
to choose between coding the excitation signal in the GSC mode or the
other ACELP modes described above. The selector mainly consists of the
speech music classification (as described in subclause 5.1.13.5), where
GSC is used in case music signals are detected. A further detector (as
described in subclause 5.1.13.5.3) is used to verify if a detected music
contains a temporal attack. In such a case the time domain transient
coding mode is used to code only the attack.

When encoding in the GSC mode, the time-domain excitation contribution
is first computed. In case of 4 subframes, the time-domain excitation
consists of both adaptive codebook and fixed codebook as in ordinary
ACELP. In case 1 or 2 subframes are used, the time-domain excitation
consists only of the adaptive codebook contribution. Then the
time-domain contribution and residual signal are both converted to the
transform domain (using DCT). The transform-domain signals are used to
determine a cut-off frequency (the upper band still containing
significant pitch contribution). The time-domain excitation contribution
is then filtered by removing the frequency content above the cut-off
frequency. The filtered time-domain contribution in the frequency domain
is subtracted from the frequency-domain residual signal, and difference
signal is quantized in the frequency domain using PVQ. The quantized
difference signal is then added to the filtered transformed time-domain
excitation contribution, and the resulting signal is converted back to
the time domain to obtain the total excitation signal.

The GSC mode is used for encoding audio signals at 7.2, and 13.2 kbit/s
for WB inputs. It is also used to encode unvoiced active speech and some
audio signal at 13.2 kbit/s in case of SWB inputs. Further, the GSC mode
is used to encode inactive signals in case of NB, WB, SWB, and FB
signals (in case DTX is off) at 7.2, 8 and 13.2kbit/s.

![](media/image55.wmf){width="6.498611111111111in"
height="4.290972222222222in"}

Figure 41: GSC encoder overview

##### 5.2.3.5.1 Determining the subframe length

The subframe length, or number of subframes per frame, is determined
depending on the bit rate and nature of encoded signal. In case of SWB
unvoiced mode at 13.2 kbit.s, 4 subframes are used. For NB and WB
signals at 13.2 kbit/s, 2 subframes are used in case of inactive signals
or when the high frequency dynamic range flag $F_{\text{hf}}$ is 0,
where $F_{\text{hf}}$is an indicator when set to 1 is indicates the
presence of high frequency spectral correlation and is computed in
subclause 5.1.11.2.6. Otherwise 1 subframe is used (audio signal where
long-time support is needed to get better frequency resolution).

For the bit rates of 7.2 and 8 kbit/s, 1 subframe is always used (NB,
WB, and SWB audio signals and inactive speech signals). The number of
subframe information is encoded at 13.2 kbps with 1 bit.

##### 5.2.3.5.2 Computing time-domain excitation contribution

For the SWB unvoiced mode at 13.2 kbit/s, 4 subframes are used and the
excitation is computed similar to ACELP Generic coding mode using both
adaptive and fixed codebooks (see subclause 5.2.3.1). The signal is
encoded using GENERIC coding type at 7.2 kbit/s, and the remaining bits
are used to encode the frequency domain contribution. The target signal
for FCB search is computed without low-pass filtering of ACB excitation.

In other modes where 1 or 2 subframes are used the time-domain
excitation contributions consists only of the adaptive codebook
contribution. This is determined using ordinary closed-loop pitch search
as in subclause 5.2.3.1.4.1.

When only 1 or 2 subframe are used, for example at 7.2 and 8 kbit/s
rates, the adaptive codebook excitation and pitch gain quantization use
the AUDIO coding type. The pitch found is quantized using 10 bits for
the first subframe and 6 bit for the following subframe, if any. In
these case, he pitch gain is quantized using a 4 bits vector quantizer.

The total excitation is finally constructed based on both ACB and FCB at
13.2 UC mode or only ACB contribution for other modes using a total
between 14 and 24 bits in case of 1 or 2 subframe up to 106 bits for the
4 subframe case..

##### 5.2.3.5.3 Frequency transform of residual and time-domain excitation contribution

In the frequency-domain coding of the mixed time-domain /
frequency-domain GSC mode, the residual signal and the time-domain
excitation contribution are transformed to frequency domain. The
time-to-frequency transform is performed using a 256-point Type IV
discrete cosine transform (DCT~IV~) giving a resolution of 25 Hz at the
internal sampling frequency of 12.8 kHz.

The DCT~IV~, $y(k)$, of a signal $x(n)$of length $L$is defined by the
following equation:

$y(k) = \sum_{n = 0}^{L - 1}{x(n)\text{cos}\left\lbrack \left( n + \frac{1}{2} \right)\left( k + \frac{1}{2} \right)\frac{\pi}{L} \right\rbrack},\ k = 0,\text{.}\text{.}\text{.},L - 1$
(634)

Here $L = \text{256}$ and $x(n)$ refers to either residual signal $r(n)$
or time-domain excitation contribution $u(n)$ with DCT output $y(k)$
corresponding to frequency transformed signals $f_{r}(k)$ and
$f_{u}(k)$, respectively.

##### 5.2.3.5.3.1 eDCT for DCT~IV~ {#edct-for-dctiv .H6}

The efficient eDCT is built upon a discrete cosine transform type IV
(DCT~IV~) but the eDCT requires less storage and has lower complexity.

The DCT~IV~ formula in above subclause can be rewritten as:

$\begin{matrix}
\left\{ y(2q) = \text{Re}\left\{ \overline{Z}(q) \right\} \middle| \right.\  \\
\end{matrix}$ (635)

where the values $\overline{Z}\left( q \right)$are given by

$\overline{Z}(q) = \overset{}{W_{8L}^{- 3}} \cdot \overset{}{W_{4L}^{2q + 1}}\sum_{p = 0}^{\frac{L}{2} - 1}{\left\{ \overset{}{z(p) \cdot W_{4L}^{2p + 1}} \right\}\overset{}{W_{\frac{L}{2}}^{\text{pq}}}},q = 0,1,\ldots,\frac{L}{2} - 1$
(636)

**and** $z(p) = x(2p) + \text{jx}(L - 1 - 2p)$**,**
$p = 0,1,\ldots,\frac{L}{2} - 1$**, and**
$W_{N} = e^{- j\frac{2\pi}{N}} = \text{cos}(\frac{2\pi}{N}) - j\text{sin}(\frac{2\pi}{N})$.

Hence, the eDCT is computed using a Fast Fourier Transform (FFT) of
$\frac{L}{2}$ points on the pre-rotated data $z(p)$:

A complex DFT with length $\frac{L}{2}$ is applied to the rotated data
$z(p)$:

$r\left( k \right) = \sum_{}^{}{z(p)W_{\frac{L}{2}}^{\text{pk}}}\ k = 0,\cdots,\frac{L}{2} - 1$
(637)

Here, when $\frac{L}{2} = \text{320}\ \text{or}\ \text{160}$, a simple
power-2 DFT is not suitable, so it is implemented with the following low
complexity 2-dimensional ($L2 = P \times Q$) DFT, where
$P = \text{64}\ (\frac{L}{2} = \text{320})\ \text{or}\ \text{32}\ (\frac{L}{2} = \text{160})$
and $Q = 5$ are coprime factors.

To reduce complexity, an address table is introduced. It can be
calculated by:

$I(n_{1},n_{2}) = \left( K_{1} \cdot n_{1} + K_{2} \cdot n_{2} \right)\ \text{mod}\ \frac{L}{2}\ n_{1} = 0,\cdots,P - 1,\ n_{2} = 0,\cdots,Q - 1$
(638)

where $K_{1}$, $K_{2}$ are coprime and satisfy the condition
$(K_{1} \cdot K_{2})\ \text{mod}\ (\frac{L}{2}) = 0$.

Here,
$K_{1} = \text{65},K_{2} = \text{256}\ (P = \text{64})\ \text{or}\ \text{96}\ (P = \text{32})$.
The address table $I$ is stored for low complexity 2-dimentional DFT,
and is used to indicate which samples are used for $P$-point DFT or
$Q$-point DFT following:

a\) Applying $P$-point DFT to $z(p)$ for $Q$ times based on the address
table $I$.

The input data to the *i-*th
($i = 0,\text{.}\text{.}\text{.},\ Q - 1$)$P$-point DFT is found by
seeking their addresses stored in the address table $I$. For the *i-*th
$P$-point DFT, the addresses of the input data are the $P$ continuous
elements starting from the $i \cdot P\text{'s}$ element in table $I$.
For every time of $P$-point DFT, the resulting data need to be applied a
circular shift with a step of $c_{1}$, where $c_{1}$ is the re‑ordered
index, which satisfies
$(c_{1} \cdot ((\frac{K_{1}^{2}}{Q})\text{mod}\ P))\text{mod}\ P = 1$.

The output of step a) is:

$w\left( k \right) = \text{DFT}_{P}(z(I + \text{iP}))_{c_{1}}\ i = 0,\cdots,Q - 1$
(639)

For the *i-*th ($i = 0,\text{.}\text{.}\text{.},\ Q - 1$)$P$-point DFT,
the addresses of the input data are the $P$ continuous elements starting
from $I\lbrack\text{Pi}\rbrack$, and the results are circular shifted
with $c_{1} = 5$. Here is an example of circular shift, the original
vector is $Z = \lbrack z_{0}\ z_{1}\ z_{2}\ z_{3}\ z_{4}\rbrack$, the
new vector with 2 circular shift is
$Z = \lbrack z_{0}\ z_{2}\ z_{4}\ z_{1}\ z_{3}\rbrack$.

b\) Applying $Q$-point DFT to $w\left( k \right)$ for $P$ times based on
the address table $I$*.*

The input data to the *i-*th
($i = 0,\text{.}\text{.}\text{.},\ P - 1$)$Q$-point DFT is found by
seeking their addresses stored in the address table $I$. For the *i-*th
$Q$-point DFT, the addresses of the input data are the $Q$ elements
starting from the *i-*th element in table $I$, each of which separated
by a step of $P$. For every time of $Q$-point DFT, the resulting data
need to be applied a circular shift with a step of $c_{2}$. $c_{2}$ is
the re-ordered index, which satisfies
$(c_{2} \cdot ((\frac{K_{2}^{2}}{P})\text{mod}\ Q))\text{mod}\ Q = 1$.

The output of step b) is:

$r\left( k \right) = \text{DFT}_{Q}(w(I + i))_{c_{2}}\ i = 0,\cdots,P - 1$
(640)

For the *i-*th ($i = 0,\text{.}\text{.}\text{.},\ P - 1$)$Q$-point DFT,
the addresses of the input data are the $Q$ continuous elements starting
from $I\lbrack i\rbrack$, each of which is separated by a step of $P$,
and the results are circular shifted with
$c_{2} = 4\ (P = \text{64})\ \text{or}\ 2\ (P = \text{32})$. Finally,
the coefficients are output according to the stored address
corresponding to the address table $I$.

##### 5.2.3.5.4 Computing energy dynamics of transformed residual and quantization of noise level

The DCT of the residual is divided into 16 bands (0 to 15) of length 16
bins. For bands 7 to 14, the energy dynamic per band is computed as the
square of the maximum value divided by the average value per band,
scaled by a factor 10. Then the average value $D_{\text{av}}$ over the 8
band (from 7 to 14) is computed.

A long-term dynamic $D_{\text{lt}}$ is updated as

$D_{\text{lt}} = \begin{matrix}
\left\{ 0\text{.}2D_{\text{lt}} + 0\text{.}8D_{\text{av}},\text{when}\ D_{\text{av}} > D_{\text{lt}} \middle| \right.\  \\
\end{matrix}$ (641)

$D_{\text{lt}}$ is quantized with 8 levels in the rage 50-82 (50, 54,
58, 62, 66, 70, 74, 78) with quantization index $i_{D}$ from 0 to 7.

The noise level is computed as $N_{\text{lev}} = \text{15} - i_{D}$

For the bit rates of 7.2 and 8 kbit/s, the noise level is low limited to
12. Thus only values 12, 13, 14, 15 are permitted and $N_{\text{lev}}$
is quantized with 2 bits). For UC SWB mode at 13.2 kbit/s
$N_{\text{lev}}$ is set to 15, otherwise $N_{\text{lev}}$ is quantized
with 3 bits (values 8 to 15).

##### 5.2.3.5.6 Find and encode the cut-off frequency

The cut-off frequency consists of the last band with significant pitch
contribution (the frequency after which coding improvement brought by
the time-domain excitation contribution becomes too low to be valuable).
Finding the cut-off frequency starts by computing the normalized
cross-correlation for each frequency band between the
frequency-transformed LP residual $f_{r}(k)$and the
frequency-transformed time-domain excitation contribution $f_{u}(k)$.
The 256-sample DCT spectrum is divided into the 16 bands with the
following number of frequency bins per band

$B_{b}(i) = \left\{ 8,8,\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{16},\text{32} \right\}$
(642)

with cumulative frequency bins per band

$C_{\text{Bb}}(i) = \left\{ 0,8,\text{16},\text{32},\text{48},\text{64},\text{80},\text{96},\text{112},\text{128},\text{144},\text{160},\text{176},\text{192},\text{208},\text{224} \right\}$
(643)

The last frequency $L_{f}$ included in each of the 16 frequency bands
are defined in Hz as:

$L_{f}(i) = \left\{ \text{175},\ \text{375},\ \text{775},\ \text{1175},\ \text{1575},\ \text{1975},\ \text{2375},\ \text{2775},\ \text{3175},\ \text{3575},\ \text{3975},\ \text{4375},\ \text{4775},\ \text{5175},\ \text{5575},\ \text{6375} \right\}$
(644)

The normalized correlation per band is defined as

$C_{c}(i) = \frac{\sum_{j = C_{\text{Bb}}(i)}^{C_{\text{Bb}}(i) + B_{b}(i)}{f_{r}(i)f_{u}(i)}}{\sqrt{S_{f_{r}}^{'}(i)S_{f_{u}}^{'}(i)}}$
(645)

Where $S_{f_{r}}^{'}(i) = \sum_{}^{}{f_{r}(i)f_{r}(i)}$ and
$S_{f_{u}}^{'}(i) = \sum_{}^{}{f_{u}(i)f_{u}(i)}$.

The cross-correlation vector is then smoothed between the different
frequency bands using the following relation

${\overline{C}}_{c2}\left( i \right) = \begin{matrix}
\left\{ 2 \cdot (\text{min}(0\text{.}5,\alpha \cdot C_{c}(0) + \delta \cdot C_{c}(1)) - 0\text{.}5),\text{for}\ i = 0 \middle| \right.\  \\
\end{matrix}$ (646)

where $\alpha = 0\text{.}\text{95}$,
$\delta = \left( 1 - \alpha \right)$ and $\beta = \frac{\delta}{2}$

The average of the smoothed cross-correlation vector is computed over
the first 13 bands (representing 5575 Hz). It is then limited to a
minimum value of 0.5 normalised between 0 and 1.

A first estimate of the cut-off frequency is obtained by finding the
last frequency of a frequency band $L_{f}$ which minimizes the
difference between the last frequency of a frequency band $L_{f}$ and
the normalized average ${\overline{C}}_{c2}$ of the smoothed
cross-correlation vector multiplied by the width of the spectrum of the
input sound signal. That is

$i_{\text{min}} = \text{min}\left( L_{f}(i) - {\overline{C}}_{c2}(\frac{F_{s}}{2}) \right),\text{for}\ 0 \leq i < \text{12}$
(647)

and the first estimate of the cut-off frequency is given by

$f_{\text{tc}1} = L_{f}(i_{\text{min}})$ (648)

where $F_{s} = \text{12800}$ Hz.

At 7.2 and 8 kbit/s, where the normalized average ${\overline{C}}_{c2}$
is never really high, or to artificially increase the value of
$f_{\text{tc}_{1}}$ to give a little more weight to the time domain
contribution, the value of ${\overline{C}}_{c_{2}}$ is upscaled with a
factor of 2.

The 8^th^ pitch harmonic is computed from the minimum or lowest pitch
lag value of the time-domain excitation contribution of all sub-frames,
and the frequency band containing the 8^th^ harmonic is determined. The
final cut-off frequency is given by the higher value between the first
estimate of the cut-off frequency $f_{\text{tc}1}$ and the last
frequency of the frequency band in which the 8^th^ harmonic is located
$L_{f}(i_{8})$.

The cut-off frequency is quantized with a maximum of 4 bits using the
values {0, 1175, 1575, 1975, 2775, 3175, 3575, 3975, 4375, 4775, 5175,
5575, 6375}.

Some hangover is added to stabilize the decision and prevent the cut-off
frequency to switch between 0 (meaning no temporal contribution) and
something else too often. First for the temporal contribution to be
allowed, the average normalized correlation $C_{\text{norm}2}$and the
long-term correlation ${\overset{\sim}{C}}_{\text{norm}2}$as computed in
subclause 5.1.13.5.3, the long term average pitch gain of the GSC
temporal contribution $G_{\text{plt}}$and the last value of the cut-off
frequency are compared to some threshold to decide if it is allowed to
remove all the temporal contribution (cut-off frequency would be 0). In
addition a hangover logic is used to diminish any undesired switching to
a complete frequency model where the cut-off frequency would be 0.

For the lowest bitrate, 7.2 and 8.0 kbit/s, only 1 bit is used to send
the cut-off frequency information when the coding mode is INACTIVE
otherwise, the cut-off frequency is considered as greater than 0
(meaning the temporal contribution is used) and the length of the
contribution is deduced from the pitch information. At 13 kbps, 4 bits
are used to send the cut-off frequency allowing all the possible cut-off
frequency values.

Once the cut-off frequency is determined, the transform of the
time-domain excitation contribution is filtered in the frequency domain
by zeroing the frequency bins situated above the cut-off frequency
supplemented with a smooth transition region. The transition region is
situated above the cut-off frequency and below the zeroed bins, and it
allows for a smooth spectral transition between the unchanged spectrum
below $f_{\text{tc}}$ and the zeroed bins in higher frequencies.

##### 5.2.3.5.7 Band energy computation and quantization

The filtered time-domain contribution in the frequency domain
$f_{u}'(k)$ is subtracted from the frequency-domain residual signal
$f_{r}(k)$, and the resulting difference signal in the frequency
domain$f_{d}(k)$is quantized with the PVQ. Before the quantization is
done, some gains per frequency band $B_{b}$, as defined above, are
computed and quantized using a split VQ. First the gain per band on the
difference signal $G_{\text{bd}}$ is computed as :

$G_{\text{bd}}\left( i \right) = \begin{matrix}
\left\{ \text{log}_{\text{10}}\left( \sqrt{2\text{.}0 \cdot \left( \sum_{k = C_{\text{Bb}}\left( i \right)}^{k = C_{\text{Bb}}\left( i \right) + B_{b}\left( i \right)}{f_{d}\left( k \right)} \right)} \right),\text{for}\ 0 \leq i < 2 \middle| \right.\ \left\{ \text{log}_{\text{10}}\left( \sqrt{\sum_{k = C_{\text{Bb}}\left( i \right)}^{k = C_{\text{Bb}}\left( i \right) + B_{b}\left( i \right)}{f_{d}\left( k \right)}} \right),\text{for}\ 2 \leq i < \text{15} \middle| \right.\  \\
\end{matrix}$ (649)

where $C_{\text{Bb}}$and $B_{b}$are defined in subclause 5.2.3.5.6.

In case of NB content, only the first 10 bands are quantized using a
split VQ. For other bandwidth, the number of band quantized depends on
the bitrate. At low bit rate only 12 bands are quantized, being the band
0 to 8 plus the bands 10, 12 and 14. The band 9, 11, 13 and 15 being
interpolated based on the quantized bands 8, 10, 12, and 14. The
codebook used for the vector quantization are different depending of the
bitrate and the bandwidth of the input signal giving a total 4 different
set of codebooks.

In all cases, prior to the vector quantitation of the bands, the average
gain of all the bands is subtract from the bands and vector quantized as
well using 6 bits. In total between 21 and 26 bits are used to get the
gain per band quantized ${\hat{G}}_{\text{bd}}$ depending of the
bitrate.

##### 5.2.3.5.8 PVQ Bit allocation

The PVQ is a coding technic that is flexible in its bit allocation. To
decide where bits should be allocated inside the difference spectrum to
quantize, some parameters are analyse as the bitrate, the cut-off
frequency$f_{\text{tc}1}$, the noise level$N_{\text{lev}}$, the coding
mode (INACTIVE, AUDIO or active UC), the bit budget available and the
bandwidth.

First, only a subset of bands will be sent to the PVQ for quantization.
The minimum number of band is 5 out of 16. To determine the number of
band, a first criteria is the bit rate, a second criteria is the cut-off
frequency and another criteria is noise level. When the number of band
is decided, a minimum amount of bit is spread over the number of band
decided with an emphasis on the low frequencies. If some bits remain
after the minimum bit allocation, then the remaining bits are split
among the bands. When the number of bands and its bit allocation are
found, the bands are picked from the initial spectrum of the difference
signal $f_{d}(k)$based on the quantized gain of this band. The 5 first
bands are always sent to the PVQ, the choice of the other bands on the
energy associated to that band and the high frequency flag
indicator$F_{\text{hf}}$.

##### 5.2.3.5.9 Quantization of difference signal

Once the bit allocation the number of band to quantize and their
position in the spectrum is defined, a new vector is concatenated
containing all the chosen bands. The values are then passed to the PVQ
for quantization to obtain the quantized difference
spectrum${\hat{f}}_{d}(k)$. The PVQ quantization scheme is described in
subclause 5.3.4.2.7.

##### 5.2.3.5.10 Spectral dynamic and noise filling

After the quantization by the PVQ, some band are empty and many more
bins are zeroed due to the low inherent to the GSC technology available.
To make the frequency model as robust as possible on speech like
content, the spectral dynamic is revised and some noise filling is added
to the difference spectrum.

For INACTIVE content below 13.2 kbit/s, the quantized spectrum above
1.6kHz is multiplied by a factor of 0.15. For INACTIVE content at 13.2
kHz, the quantized spectrum above 2.0kHz is multiplied by a factor of
0.25. Otherwise the scaling factor for the spectral
dynamic$S_{\text{fsd}}$ and the frequency bin where the scaling of the
spectral dynamic $F_{\text{fsd}}$is applied is computed as follow:

$S_{\text{fsd}} = \frac{\left( \text{14} - N_{\text{lev}} \right)}{\text{10}} - 0\text{.}4$
(650)

and

$F_{\text{fsd}} = \text{112} + \left( \text{14} - N_{\text{lev}} \right) \ast \text{16}$
(651)

Furthermore, for frequencies above 3.2 kHz, the spectral dynamic is
limited to an amplitude of ±1 for bitrate below 13.2 kbit/s and to ± 1.5
otherwise.

This scaling is then applied to the quantized difference
spectrum${\hat{f}}_{d}(k)$, to obtain its scaled
version${\hat{f}}_{\text{ds}}(k)$.

A noise filling is then applied to the whole difference spectrum. The
noise level added is based on the bitrate, the coding mode and the
spectral dynamic$N_{\text{lev}}$ to obtain the scaled difference
spectrum with noise${\hat{f}}_{\text{dsn}}(k)$on which the gain will be
applied on.

##### 5.2.3.5.11 Quantized gain addition, temporal and frequency contributions combination

Once dynamic of the quantized difference spectrum has been scaled and
the noise fill has be performed, the gain of each bands is computed
exactly as in subclause 5.2.3.5.7 to get gain of the quantized
spectrum$G_{\text{bd}2}$. The gain per band to apply
$G_{a}\left( i \right)$ consists as:

$G_{a}\left( i \right) = \text{10}^{\left( {\hat{G}}_{\text{db}}\left( i \right) - G_{\text{db}2}\left( i \right) \right)}$
(652)

This gain is applied to both the scaled difference spectrum with
noise${\hat{f}}_{\text{dsn}}(k)$ and the scaled difference
spectrum${\hat{f}}_{\text{ds}}(k)$and both vectors are added to the
temporal contribution to get two different spectral representation of
the quantized excitation in the frequency domain, one with noise
fill$f_{u}\text{\ left\ (i\ right\ )\}\ \{}$ and the other
without$f_{u2}\text{\ left\ (i\ right\ )\}\ \{}$ as shown below.

$f_{u}\text{\ left\ (i\ right\ )=f\ rSub\ \{\ size\ 8\{u\}\ \}\ '\ left\ (i\ right\ )+\ \{\ hat\ \ \{f\}\}\ rSub\ \{\ size\ 8\{\ ital\ }\text{dsn}\text{\}\ \}\ \ \textbackslash(\ i\ \textbackslash)\ \ cdot\ G\ rSub\ \{\ size\ 8\{a\}\ \}\ \ left\ (k\ right\ )\ \textbackslash rline\ \ rSub\ \{\ size\ 8\{i=C\ rSub\ \{\ size\ 6\{\ ital\ }\text{Bb}\text{\}\ \}\ \ \textbackslash(\ k\ \textbackslash)\ +B\ rSub\ \{\ size\ 6\{b\}\ \}\ \ \textbackslash(\ k\ \textbackslash)\ ,``0\ <=\ k<}16\text{\}\ \}\ \}\ \{}$
(653)

and

$f_{u2}\text{\ left\ (i\ right\ )=f\ rSub\ \{\ size\ 8\{u\}\ \}\ '\ left\ (i\ right\ )+\ \{\ hat\ \ \{f\}\}\ rSub\ \{\ size\ 8\{\ ital\ }\text{ds}\text{\}\ \}\ \ \textbackslash(\ i\ \textbackslash)\ \ cdot\ G\ rSub\ \{\ size\ 8\{a\}\ \}\ \ left\ (k\ right\ )\ \textbackslash rline\ \ rSub\ \{\ size\ 8\{i=C\ rSub\ \{\ size\ 6\{\ ital\ }\text{Bb}\text{\}\ \}\ \ \textbackslash(\ k\ \textbackslash)\ +B\ rSub\ \{\ size\ 6\{b\}\ \}\ \ \textbackslash(\ k\ \textbackslash)\ ,``0\ <=\ k<}16\text{\}\ \}\ \}\ \{}$
(654)

##### 5.2.3.5.12 Specifics for wideband 8kbps

The available bits are allocated to the bands of the frequency
excitation signal according to the bit allocation algorithm as described
in subclause 5.2.3.5.8, where the frequency excitation signal is the
output of DCT~IV~ as described in subclause 5.2.3.5.3. If the index
$I_{\text{high}_{\text{bit}}}$ of the highest frequency band with bit
allocation is more than a given threshold, the bit allocation for the
frequency excitation bands will be adjusted: Decrease the number of the
allocated bits of the bands with more bits, and increase the number of
the allocated bits of the band $I_{\text{high}_{\text{bit}}}$ and the
bands near to $I_{\text{high}_{\text{bit}}}$. And then, encode the
frequency excitation signal with the allocated bits, where the given
threshold is determined by the available bits and the resolution of the
frequency excitation signal.

The details are described as follows:

1)  Allocate most of the available bits to the 5 lower frequency bands
    by the pre-determined bit allocation table;

2)  Allocate the remaining bits to those bands excluding the lower
    frequency 5 bands which have the largest band energy, if there are
    remaining bits after the first step;

3)  Search the index $I_{\text{high}_{\text{bit}}}$of the highest
    frequency band with bit allocation.

If the index $I_{\text{high}_{\text{bit}}}$ of the highest frequency
band with bit allocation is more than a given
threshold$\text{Tr}_{1} = 7$, the bit allocation for the frequency
excitation bands will be adjusted:

1)  Allocate bits to some more bands whose index is above
    $I_{\text{high}_{\text{bit}}}$. The number of the newly bit
    allocated bands$N_{\text{extr}}$ is determined by the noise
    level$N_{\text{lev}}$and the coding mode.

2)  For the newly bit allocated bands, allocate 5 bits to each band. If
    the number of the newly bit allocated bands$N_{\text{extr}} \leq 2$,
    allocate 1 more bit to each band whose index starts from 4 to
    $I_{\text{high}_{\text{bit}}} + N_{\text{extr}}$.

3)  The total number of newly allocated bits$R_{\text{extr}}$ is
    $5 \cdot N_{\text{extr}} + I_{\text{high}_{\text{bit}}} + N_{\text{extr}} - 4$.
    $R_{\text{extr}}$is obtained by decreasing the number of the bits
    allocated to the 4 lower frequency bands.

Otherwise, the original number of allocated bits to each band is not
changed.

Finally, quantize and encode the frequency excitation signal according
to the allocated bits.

Then, reconstruct the frequency excitation signal based on the quantized
parameters. The reconstructed frequency excitation signal is
corresponding to the decoded frequency excitation signal in decoder.

For the reconstructed frequency excitation signal, if the index
$I_{\text{last}_{\text{bin}}}$ of the highest frequency band with bit
allocation is more than a given threshold, or there is the temporal
contribution in the reconstructed frequency excitation signal, the
frequency excitation signal above
$C_{\text{Bb}}(I_{\text{high}_{\text{bit}}}^{'})$ will be reconstructed
by the reconstructed frequency excitation signal; otherwise, the
frequency excitation signal above
$C_{\text{Bb}}(I_{\text{high}_{\text{bit}}}^{'})$will be reconstructed
by noise filling.

The detailed descriptions are as follows:

When the coding mode of previous frame is AC mode, if the last sub-band
index of bit allocation $I_{\text{last}_{\text{bin}}}$is larger than
$\text{Tr}_{2} = 8$ or there is the temporal contribution in the
reconstructed frequency excitation signal, the BWE flag
$F_{\text{BWE}}$is set to 1. It should be noted that the BWE flag
$F_{\text{BWE}}$ is initialized to 0 and calculated for every frame.
$I_{\text{last}_{\text{bin}}}$is then refined by:

$I_{\text{last}_{\text{bin}}} = \begin{matrix}
\left\{ \text{max}(I_{\text{last}_{\text{bin}}},\text{10})\text{if}F_{\text{BWE}} = 1 \middle| \right.\  \\
\end{matrix}$ (655)

If $F_{\text{BWE}} = 1$, the frequency excitation signal below
$C_{\text{Bb}}(I_{\text{last}_{\text{bin}}})$ will be reconstructed as
described in subclause 5.2.3.5.11, and the frequency excitation signal
above $C_{\text{Bb}}(I_{\text{last}_{\text{bin}}})$ will be
reconstructed as follows:

${\hat{f}}_{d}(\text{255} - k) = f_{u2}\text{\ left\ (}159\text{\ -\ k\ right\ )0\ <=\ k<}255\text{\ -\ \ left\ (}16\text{\ cdot\ I\ rSub\ \{\ size\ 8\{\ ital\ }\text{last}\text{\_\ ital\ }\text{bin}\text{\}\ \}\ \ -\ 1\ right\ )\}\ \{}$
(656)

And then the frequency excitation signal
${\hat{f}}_{d}(\text{255} - k)0 \leq k < \text{255} - \left( \text{16} \cdot I_{\text{last}_{\text{bin}}} - 1 \right)$
is scaled by the quantized gains to obtain the scaled frequency
excitation signal
$f_{u2}\text{\ \textbackslash(\ }255\text{\ -\ k\ \textbackslash)\ 0\ <=\ k<}255\text{\ -\ \ left\ (}16\text{\ cdot\ I\ rSub\ \{\ size\ 8\{\ ital\ }\text{last}\text{\_\ ital\ }\text{bin}\text{\}\ \}\ \ -\ 1\ right\ )\}\ \{}$.

When the energy ratio between the current frame and the previous frame
is in the range (0.5, 2), for any band with index range is \[4, 9\], if
the band is bit allocated in the current frame or in the previous frame,
the coefficients in the band are smoothed by weighting the coefficients
of the current frame and the previous frame.

For the scaled frequency excitation signal above
$C_{\text{Bb}}(I_{\text{last}_{\text{bin}}})$, estimate the position of
the formant by LSF parameters. If the magnitudes of the coefficients
near to the formant are larger than a threshold, the magnitudes are
decreased to improve the perceptual quality.

Otherwise, If $F_{\text{BWE}} = 0$, the un-quantized coefficients, i.e.
un-decoded coefficients at the decoder side will be reconstructed by
noise filling as described in subclause 5.2.3.5.10.

##### 5.2.3.5.13 Inverse DCT

After the gain has been applied and the combination in the frequency
domain done, both frequency representations of the coded excitation are
convert back to time domain using the exact same DCT as in subclause
5.2.3.5.3. The inverse transform is performed to get the quantized
excitation $\hat{u}\left( n \right)$ which is the temporal
representation of $f_{u}\text{\ left\ (i\ right\ )\}\ \{}$ and
${\hat{u}}_{2}\left( n \right)$which is the temporal representation
of$f_{u2}\text{\ left\ (i\ right\ )\}\ \{}$. $\hat{u}\left( n \right)$
will be used to update the TDBWE while ${\hat{u}}_{2}\left( n \right)$
is used to update the internal CELP state as the adaptive codebook
memory.

##### 5.2.3.5.14 Remove pre-echo in case of onset detection

Compute the energy of the excitation $\hat{u}\left( n \right)$over each
4 samples using a 4-sample sliding window, and find the more energetic
section to determine a possible attack (onset). If the attack is larger
than the previous frame energy plus 6 dB, the algorithm finds the energy
before the attack (excluding the section where the attack has been
detected) and it scales it to the level of the previous frame energy
plus 6dB.
